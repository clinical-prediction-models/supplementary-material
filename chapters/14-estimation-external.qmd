# Estimation with External Information {#estimation-external}

```{r setup-ch14, include=FALSE}
knitr::opts_knit$set(
  echo = TRUE,
  root.dir = here::here()
)

knitr::opts_chunk$set(
  fig.path = "fig/"
)

library(rms)
library(knitr)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(error = FALSE)
library(kableExtra)
library(metamisc)
library(lme4)
library(DescTools)
library(outliers)

source("R/bootcor.uni.mult.R") 
options(digits=3)
```
## Learning from external information {-}

### A local model with external information: Table 14.1

We can use external information to optimize the development of a prediction model with local or global applicability.

Table 14.1 provides the basic idea of model development with a focus on a locally applicable model.

```{r Table14.1, echo=F}
tab14 <- rbind(
  c("Model specification", "Mixture of IPD and literature", "Focus on consensus in literature"),
  c("Model coefficients", "IPD with literature as background", "Meta-analysis of literature"),
  c("Baseline risk", "IPD", "Literature"))
colnames(tab14) <- c("Modeling aspect", "Local model", "Global model")
kable(tab14, caption = "Table 14.1: local vs global models")
```

## The data: TBI (n=11022) and AAA (n=238) data sets {-}

The impact data set includes patients with moderate / severe TBI for 15 studies ([Steyerberg 2019](https://pubmed.ncbi.nlm.nih.gov/31373722/ "Assessment of heterogeneity in an individual participant data meta-analysis of prediction models: An overview and illustration")). We can learn from the combined information in an Individual Patient Data Meta-Analysis (IPD MA).

The AAA data set includes a small set of patients undergoing elective surgery for Abdominal Aortic Aneurysm ([Steyerberg 1995](https://pubmed.ncbi.nlm.nih.gov/7575054/ "Perioperative mortality of elective abdominal aortic aneurysm surgery. A clinical prediction rule based on literature and individual patient data")). We can learn from univariate information in the literature where many studies are available ([Debray 2012](https://pubmed.ncbi.nlm.nih.gov/22883206/ "Incorporating published univariable associations in diagnostic and prognostic modeling")). Specifically, we borrow information from the univariate coefficients in other studies and perform an adaptation of the multivariable coefficients in the AAA data set. The resulting model is stabilized such that better local and global performance is expected.

```{r data}
# impact data come with metamisc package Debray; read here
data("impact", package = "metamisc")
impact$name         <- as.factor(impact$name)
impact$ct           <- as.factor(impact$ct)
impact$age10        <- impact$age/10 - 3.5 # reference 35-year-old patient, close to mean age
impact$motor_score  <- as.factor(impact$motor_score)
impact$pupil        <- as.factor(impact$pupil)
levels(impact$ct) <- c("I/II", "III/IV", "V/VI") # correct an error in Debray labels

impact$motor.lin    <- as.numeric(impact$motor_score) # 1/2, 3, 4, 5/6 linear
impact$pupil.lin    <- as.numeric(impact$pupil) # 1, 2, 3 linear
impact$study <- as.factor(ifelse(impact$name=="TINT", 1,
                ifelse(impact$name=="TIUS", 2,
                  ifelse(impact$name=="SLIN", 3,
                    ifelse(impact$name=="SAP", 4,
                      ifelse(impact$name=="PEG", 5,
                        ifelse(impact$name=="HIT I", 6,
                          ifelse(impact$name=="UK4", 7,
                            ifelse(impact$name=="TCDB", 8,
                              ifelse(impact$name=="SKB", 9,
                                ifelse(impact$name=="EBIC", 10,
                                  ifelse(impact$name=="HIT II", 11,
                                    ifelse(impact$name=="NABIS", 12,
                                      ifelse(impact$name=="CSTAT", 13,
                                      ifelse(impact$name=="PHARMOS", 14,
                                      ifelse(impact$name=="APOE", 15,NA))))))))))))))))
names <- levels(impact[,1])

AAA  <- read.csv("data/AAA.csv", row.names = 1)
```

## The impact study {-}

### Descriptives

Overall results are presented below. We note that missing values were imputed (using mice, a single imputation for simple illustrations).

```{r impact.data.describe}
html(describe(impact), scroll=TRUE) 
kable(table(impact$name, impact$mort), row.names = T, caption = "Mortality by study")
```

### Analyses for Table 14.2

We analyze various model variants for the `impact` data, with 3 key predictors:

-   `age10`: age per decade, continuous; centered at age 40

-   `motor.lin`: the Motor score component from the Glasgow Coma Scale, continuous for codes 1/2, 3, 4, 5/6

-   `pupil.lin`: pupillary reactivity, continuous for codes 1, 2, 3 relating to both, one, or no reacting pupils

The models are as follows:

1.  A naive analyses of the merged data, ignoring the clustering nature of the data (`rms::lrm`)
2.  Per study analyses for each of the 15 studies (`rms::lrm`)
3.  Stratified analysis, with study as a factor variable to estimate common predictor effects (`rms::lrm`)
4.  One-stage meta-analysis, with study as a random effect to estimate common predictor effects (`lme4::glmer`)
5.  Two-stage univariate meta-analysis, with pooling of the per study estimates obtained in step 2, to obtain random effect estimates for the predictor effects (`metamisc::uvmeta`)

The output includes estimates for the model intercept and the predictor effects. Standard errors (SE) are obtained for each estimate. Moreover, the 1-step and 2-step meta-analyses estimate the heterogeneity parameter tau, which reflects between study differences in the estimates. This heterogeneity is considered in the random effect 95% confidence intervals, and in 95% prediction intervals.

```{r impact.lrm }
# store the results in 2 matrices to produce something close to Table 14.2
coef.matrix <- matrix(nrow=23, ncol=4)
se.matrix <- matrix(nrow=23, ncol=4)
dimnames(coef.matrix) <- dimnames(se.matrix) <- 
  list(c(levels(impact$study), "naive",
      "stratified", "one-stage", "tau-1", "two-stage", "tau-2", "Low pred", "High pred")
                      ,Cs(Intercept,age,motor,pupils))

# naive merged model
fit0 <- lrm(mort~age10+motor.lin+pupil.lin, data=impact)
coef.matrix[16,] <- coef(fit0)  
se.matrix[16,] <- diag(se(fit0))

for (i in 1:15) {
# fit stratified models
fit.lin <-lrm(mort~age10+motor.lin+pupil.lin, data=impact, subset=impact$study==i)
coef.matrix[i,] <- coef(fit.lin)  
se.matrix[i,] <- diag(se(fit.lin)) }

# global model with stratification; intercept is for Study==1
fit.stratified <- lrm(mort~age10+motor.lin+pupil.lin +name, data=impact)
coef.matrix[17,2:4] <- coef(fit.stratified)[2:4]  
se.matrix[17,2:4] <- diag(se(fit.stratified))[2:4] 

# Estimate heterogeneity in 1 step, global model for covariate effects
fit.lin.meta <- glmer(mort~ (1 | name) + age10  + motor.lin+pupil.lin, 
                      family =binomial(), data = impact)
coef.matrix[18,]  <- fit.lin.meta@beta
se.matrix[18,]    <- sqrt(diag(vcov(fit.lin.meta)))
coef.matrix[19,1] <- fit.lin.meta@theta

# Estimate heterogeneity in 2 step model
for (i in 1:4) {
impact.meta <- uvmeta( r=coef.matrix[1:15,i], r.se=se.matrix[1:15,i])
coef.matrix[20,i] <- impact.meta$est
se.matrix[20,i]   <- impact.meta$se
coef.matrix[21,i] <- sqrt(impact.meta$tau)
coef.matrix[22,i] <- impact.meta$pi.lb
coef.matrix[23,i] <- impact.meta$pi.ub
} # end loop over 4 columns of per study estimates

# Made Tabe 14.2; SE estimates separate
kable(coef.matrix, row.names = T, col.names = NA, caption = "Coefficients and heterogeneity by study")
```

### Estimation of coefficients: naive, stratified and IPD-MA

We consider the estimates for the multivariable coefficients of 3 predictors: age, motor score, and pupils. Per study differences are modest; a forest plot might visualize the patterns. The model is:

`lrm(mort~age10+motor.lin+pupil.lin, data=impact, subset=impact$study==i)`

A naive summary estimate ignores the pooling:

`lrm(mort~age10+motor.lin+pupil.lin, data=impact)`

It produces overall estimates that are only slightly different than the estimates from a stratified model (with study as fixed effect). Also, similar estimates come for a one-step random effect analysis (with `lrm4::glmer`), or a two-step random effect analysis (with `metamisc::uvmeta`). Overall, the summary effect estimates for the predictors are quite similar.

The intercept differences between the studies are more substantial, but the summary estimates of the overall baseline risks are similar, all close to the naive estimate of -0.55. For the stratified analysis, a specific study intercept is taken as the reference, so the overall estimate is not available directly from `lrm`.

The between study variability is quantified with the parameter tau: slot `@beta` in glmer, and `$tau` in `metamisc::uvmeta`. The tau estimates were 0.356 with a one-step random effect analysis (with `lrm4::glmer`); and 0.438 with a two-step random effect analysis (with `metamisc::uvmeta`). Note that the latter tau estimate for the intercept depends on the coding of the predictors; it is the heterogeneity for a reference patient with zero values for the covariates, in our case; age 35, poor motor score and both pupils reacting.\
The prediction interval (from `metamisc::uvmeta`) was quite wide: \[-1.7 - .2\], or odds ranging from exp(-1.73)=0.177 to exp(0.26)=1.3, or mortality risks between `plogis(-1.73)`=15% to `plogis(0.26)`=56% for a reference patient with all covariate values set to zero (age 35, poor motor score and both pupils reacting). We can also obtain prediction intervals for the predictor effects with the two-stage approach, which shows wider intervals than the standard 95% confidence intervals (either from fixed or random effect meta-analysis).

### Estimation of standard errors

The SE estimates (see below) for the predictor effects are very similar with a naive, stratified, or one-stage approach: 0.015 for age, around 0.02 for motor, and 0.03 for pupils. The pooled SEs are substantially smaller than the SE estimates per study. Even for the larger studies, the SEs are substantially wider: around 0.05 for age, around 0.07 for motor, and 0.09 for pupils. These SEs are fixed effect estimates, assuming a single, uniform association of each predictor with the outcome, 6-month mortality.

The random effect estimates allow for between study heterogeneity, and are wider: 0.024 for age, around 0.04 for motor, and 0.05 for pupils. For the model intercept, the naive approach underestimates the uncertainty by ignoring the clustering of the data. The one-stage and two-stage meta-analysis approach largely agree (SE = 0.12 - 0.14).

```{r impact.se.estimates }
kable(se.matrix, row.names = T, col.names = NA, caption = "SE estimates")
```

### Estimation for a specific study, assuming a global model holds

We can estimate the intercept for study 14 with fixed estimates from the stratified model:

`Mortality | Study 14 ~ intercept14 + offset(global linear predictor)`.

```{r impact.study14}
# Stratified model, fixed effects
fit.stratified <- lrm(mort~age10+motor.lin+pupil.lin +name, data=impact)
study14 <- impact[impact$study==14, ] # selected reference set
fit.14 <- lrm.fit(y=study14$mort, 
                  offset=as.matrix(study14[,c("age10",  "motor.lin", "pupil.lin")]) %*%
                    fit.stratified$coefficients[2:4] )

cat("Fixed effect estimates for study 14, Tirilazad US (TIUS)\n")
print(c(coef(fit.14), fit.stratified$coefficients[2:4]) )

```

------------------------------------------------------------------------

## The AAA study {-}

### Descriptives

The AAA data set is rather small, with n=238 patients undergoing elective surgery for an Abdominal Aortic Aneurysm, and only 18 events (`STATUS==1`). The data set consists of patients operated on at the University Hospital Leiden (the Netherlands) between 1977 and 1988 ([Steyerberg 1995](https://pubmed.ncbi.nlm.nih.gov/7575054/ "Perioperative mortality of elective abdominal aortic aneurysm surgery. A clinical prediction rule based on literature and individual patient data")).

```{r AAA.data.describe}
html(describe(AAA), scroll=TRUE) 
```

### Literature, univariate estimates: Table 14.4

We consider the development of a prediction model for in-hospital mortality after elective surgery for AAA. We consider predictors that are identified as relevant in the literature and that are available in the local data. We performed a literature review to obtain univariate estimates of regression coefficients from publications in PubMed between 1980 and 1994. The selection was limited to English-language studies, which had to contain frequency data on the association of a potential predictor and surgical mortality, either in tables or mentioned in the text.

The pooled log odds ratio estimates (=logistic regression coefficients) from a fixed effect or random effect procedure. The estimates agreed up to 2 decimals usually. We compare the univariate coefficients from the literature (fixed or random pooled estimates) to the estimates from the local data.

-   the effect for `SEX` was small, a slightly stronger in the literature (0.36 vs 0.28)

-   the age effect (per decade) was strong. A literature coefficient of 0.79 (slightly smaller than the local 0.98 estimate) implies a exp(0.79)=2.2 times higher odds of mortality per 10 years older. So, as often in surgical procedures, age was a strong predictor ([Finlayson 2001](https://pubmed.ncbi.nlm.nih.gov/11525104/ "Operative mortality with elective surgery in older adults"))

-   the univariate estimates for cardiac comorbidity (a history of `MI`, presence of `CHF`, presence of `ISCHEMIA`) were all a bit smaller in the literature than the local data

-   `LUNG` or `RENAL` comorbidity showed quite similar univariate associations in the literature and the IPD, so using or ignoring the literature would result in similar coefficient estimates

The standard errors (SE) were smaller for the literature estimates, as expected. The pooled estimates of the predictors `SEX` and `AGE10` were based on large numbers and many studies, especially for age. This leads to a relatively small fixed effect SE. For other predictors, the number of studies was small leading to a modestly smaller SE, e.g. for cardiac, lung and renal comorbidity. The random effect SE was larger for `SEX` and `AGE10`, including the between study heterogeneity in effect estimates. We used the random effect estimates of the coefficients with the random effect SE for the adaptation procedures.

```{r AAA.lit.estimates}
# The following estimates come from a univariable logistic regression analysis
# Fixed effects, better random effect estimates
lit.coefficients.f	<- c(log(1.44),log(2.20),log(2.80),log(4.89),log(4.58),
                        log(3.75),log(2.43))
lit.se.f			<- c(0.08,0.06,0.27,0.33,0.31,0.25,0.23)
# Random effects
lit.coefficients	<- c(0.3606,0.788,1.034,1.590,1.514,1.302,0.8502)
lit.se			<- c(0.176,0.112,0.317,0.4109,0.378,0.2595,0.2367)

# Local data multivariable estimates
full <- lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA,x=T,y=T)

# Local data univariate estimates
local.coef <- local.se <- rep(NA, 7)
for (i in 1:7) {
fit.ind.x		<- lrm.fit(y=full$y,x=full$x[,i],maxit=25)
local.coef[i] <- fit.ind.x$coef[2]
local.se[i]   <- sqrt(fit.ind.x$var[2,2]) }

# Table 14.4
tab14 <- cbind(local.coef, local.se, 
            lit.coefficients.f, lit.se.f, 
            lit.coefficients, lit.se)
rownames(tab14) <- names(full$coefficients[-1])
colnames(tab14) <- c("local.coef", "local.se", 
                     "fixed.coef", "fixed.se", 
                     "random.coef", "random.se")
kable(tab14, digits=2, caption = "Univariate coefficients")
```

### A simple prediction model: Table 14.5

We may fit a simple model for in-hospital mortality based on predictors that are known to be relevant from the literature, and that are available in the local data. The standard logistic model with maximum likelihood is obtained from: `lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA)`

We may apply a shrinkage factor, estimated by bootstrapping using the `rms::validate` function. And finally we explore penalized logistic regression, using `rms::pentrace`. We find that

-   the estimated coefficients are shrunken towards zero with both the bootstrap shrinkage approach and the penalized maximum likelihood approach.

-   the estimated shrinkage factor is 0.66, the reduction in c statistic is from 0.83 to 0.76, the reduction in R\^2 is from 24 to 10%; this is all in agreement with the small effective sample size (18 events).

```{r AAA.models, message=F }
full <- lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA,x=T,y=T)

# Estimate shrinkage factor
set.seed(1)
full.validate <- validate(full, B=500, maxit=10)
# Apply shrinkage factor on model coefficients
full.shrunk.coefficients	<- full.validate["Slope","index.corrected"] * full$coefficients
# Adjust intercept with lp as offset variable
lp.AAA	<- full$x %*% full.shrunk.coefficients[2:(ncol(full$x)+1)]
fit.offset  <- lrm.fit(y=full$y, offset=lp.AAA)
full.shrunk.coefficients[1]	<- fit.offset$coef[1]

# Full model, penalized estimation
penalty	<- pentrace(full,penalty=c(0.5,1,2,3,4,6,8,12,16,24,36,48),maxit=25)
plot(penalty)
cat("The optimal penalty was", penalty$penalty, "for effective df around 3.5 rather than 7 df\n")

full.penalized <- update(full, penalty=penalty$penalty)

# the 3 sets of coefs
coef.mat <- cbind(full$coef, full.shrunk.coefficients, full.penalized$coef )
colnames(coef.mat) <- c("Full, ML", "Shrinkage", "Penalized")

kable(coef.mat, digits=2, 
      caption = "Estimated local multivariable regression coefficients in IPD")
# shrinkage factor: slope of lp
cstat <- full.validate[1,]
cstat[c(1:3,5)] <- full.validate[1,c(1:3,5)] * 0.5 + 0.5 # c = D/2 + 0.5
cstat[4] <- full.validate[1,4] * 0.5 # optimism in c

# show key performance measures, including c statistic, R2, and estimated shrinkage factor
kable(rbind(cstat, full.validate[1:4,]), digits = 2, 
      caption = "Internally validated performance" )
```

### Adaptation of univariate coefficients: Table 14.6

We implement two variants of an "adaptation method" that takes advantage of univariate literature data in the estimation of the multivariable regression coefficients in a local prediction model.

Adaptation method 1 is simple: beta m\|(I+L) = beta u\|L + (beta m\|I - beta u\|I), where beta m\|(I+L) is the set of multivariable coefficients ("m") for the predictors considering both individual patient data ("I") and literature data ("L"). The univariate coefficients are denoted as "beta u".

The adaptation factor (beta m\|I - beta u\|I) is the difference between multivariable and univariate coefficient in the IPD data set. The adaptation factor adapts beta u\|L, the set of univariate estimates from the literature.

Adaptation method 2 is a bit more complex: beta m\|(I+L) = beta m\|I + *c* \* (beta u\|L - beta u\|I), with c a factor between 0 and 1.

-   With *c*=0, adaptation method 2 is ignoring literature information; we simply use the mulktivariable coefficients from our local data.

-   With *c*=1, adaptation method 2 is the same as adaptation method 1, rewritten as:\
    beta m\|(I+L) = beta m\|I + beta u\|L - beta u\|I.

-   The optimal value if c can be derived as a combination of the variance estimates of the components beta m\|, beta u\|L, and beta u\|I with the correlation between univariate and multivariable coefficient in the individual patient data: *r*(beta m\|I, beta u\|I). The latter correlation can be estimated empirically by a bootstrap procedure. We estimate coefficients in many bootstap samples drawn with replacement and calculate the correlation after excluding outliers.

```{r adaptation.1}
set.seed(1)
# we need the user written function
full.uni.mult.cor	<- bootcor.uni.mult(full, group=full$y, B=500, maxit=10, 
                                      trim=.1,save.indices = T)

# outliers are removed by 'outliers' package
# full.uni.mult.cor[4]  # matrix with results per bootstrap

# now do adaptation method approach
adapt.factors	<- adapt.coefficients	<- adapt.var <- adapt.coef.Gr	<- adapt.var.Gr	<- 
  rep(0,7)

# Adaptation method 1, simple, just take the difference in own data between m and u estimates for coefficients
for (i in 1:7) {
fit.ind.x		<- lrm.fit(y=full$y,x=full$x[,i],maxit=25)
adapt.coef.Gr[i] <- full$coefficients[i+1] + 
                1 * (lit.coefficients[i] - fit.ind.x$coefficients[2])
adapt.var.Gr[i]	<- full$var[i+1,i+1] - fit.ind.x$var[2,2] + lit.se[i]^2	
 } # end adaptation 1

# Adaptation method 2: optimal adaptation factors
for (i in 1:7) {
fit.ind.x		<- lrm.fit(y=full$y,x=full$x[,i],maxit=25)
adapt.factors[i]	<- (full.uni.mult.cor$r[i] * 
                       sqrt(full$var[i+1,i+1]) * sqrt(fit.ind.x$var[2,2]) ) / 
                      (lit.se[i]^2 + fit.ind.x$var[2,2])

adapt.coefficients[i]	<- full$coefficients[i+1] + 
                    adapt.factors[i] * (lit.coefficients[i] - fit.ind.x$coefficients[2])

adapt.var[i]		<- full$var[i+1,i+1] * (1-(full.uni.mult.cor$r[i]^2 * fit.ind.x$var[2,2] / 
				(lit.se[i]^2 + fit.ind.x$var[2,2]))) } # end adaptation 2


# Store all coefficients and related measures in a nice matrix
tab14 <- cbind(lit.coefficients, local.coef, full$coef[-1], full.uni.mult.cor$r,
               adapt.factors, adapt.coef.Gr, adapt.coefficients, 
               full.shrunk.coefficients[-1], full.penalized$coef[-1] )
rownames(tab14) <- names(full$coefficients[-1])
colnames(tab14) <- c("random.coef.u", "local.coef.u", "local.coef.m", "r",
                     "adapt.opt", "adapted.1.m", "adapted.2.m",
                     "shrunk.m", "penalized.m")
kable(tab14, digits=2, caption = "Coefficients for AAA modeling")
```

#### Adaptation results: coefficients in Table 14.6

The first 3 columns show the estimates that are input for the adaptation approaches:

1.  the random effect pooled estimates for the univariate associations (**random.coef.u**)
2.  the local, or IPD, univariate associations (**local.coef.u**); and
3.  the local multivariable associations (**local.coef.m**).

The empirically estimated correlation *r* between local univariate and multivariable coefficients was between 0.36 for `SEX` and around 0.9 for other the predictors. The optimal adaptation factor (**adapt.opt**) was closely related to the estimate of *r*. Adaptation method 1 or 2 lead to similar estimates for the multivariable coefficient (beta m\|(I+L), denoted as **adapted.1.m** and **adapted.2.m**). The adapted estimates are larger than shrunk or penalized estimates that only consider the local IPD.

```{r adaptation.se}
# Store all standard error estimates in a nice matrix
tab14 <- cbind(lit.se, local.se, sqrt(diag(full$var)[-1]),
               sqrt(adapt.var.Gr), sqrt(adapt.var), 
              sqrt(diag(full.penalized$var)[-1] ))
# Reduction in variance
reduct.var <- 100 - round(100*adapt.var / diag(full$var)[-1])
tab14 <- cbind(tab14, reduct.var)

rownames(tab14) <- names(full$coefficients[-1])
colnames(tab14) <- c("random.u", "local.u", "local.m",
                     "adapted.1", "adapted.2",
                     "penalized.m", "% reduction variance adapt 2")
kable(tab14, digits=2, caption = "SE estimates and reduction in variance by adaptation")
```

#### Adaptation results: variance in Table 14.6

As discussed above, the standard errors (SEs) from random effect pooling of literature data are notably smaller than those for the local univariate coefficients. The local multivariable coefficients have larger SEs, as always for logistic regression models ([Robinson & Jewell, 1991](https://www.jstor.org/stable/1403444#metadata_info_tab_contents "Some Surprising Results about Covariate Adjustment in Logistic Regression Models")).

The adaptation methods suggest substantially smaller SEs. For SEX the reduction is only 13%, reflecting the poor correlation between multivariable and univariable coefficients in the IPD (*r*=0.36). Over 50% reduction in variance is obtained for the other estimates; assuming a global prediction model holds with respect to the predictor effects.

```{r adaptation.score}
# Round the coefficients after shrinkage 0.9
rounded.coef <- round(0.9*adapt.coefficients,1)
# we estimate the intercept
X	<- as.matrix(full$x)
X[,2] <- X[,2] - 7 # center around age=7 decades (70 years)
# calculate linear predictor, used as offset in lrm.fit
offset.AAA	<- X %*% rounded.coef
fit.offset  <- lrm.fit(y=full$y, offset=offset.AAA)
# Recalibrate baseline risk to 5% (odds(0.05.0.95)); observed was 18/238, or odds 18/220
recal.intercept <- fit.offset$coef[1] + log((0.05/0.95)/(18/220))

###################################################################
# Alternative, crude naive Bayes approach 
# Recalibrate literature coefficients with one calibration factor 
# calculate linear predictor, used as the only x variable in lrm.fit
lit.score	<- X %*% lit.coefficients
fit.lit  <- lrm.fit(y=full$y, x=lit.score)

print(fit.lit, digits=2) # uni coefs model fit

# 3 formulas for risk calculation
recal.intercept.lit <- fit.lit$coef[1] + log((0.05/0.95)/(18/220))

tab14 <- cbind(full$coefficients, 
               full.penalized$coefficients, 
               c(recal.intercept, rounded.coef),
               c(NA, lit.coefficients),
               c(recal.intercept.lit, fit.lit$coef[2]*lit.coefficients))

rownames(tab14) <- names(full$coefficients)
colnames(tab14) <- c("full.coef", "penalized.m", "adapted", "lit coefs", "lit score")
kable(tab14, digits=1, 
      caption = "Rounded coefficients with IPD only, adaptation approach, or naive Bayes score")
```

#### A score for prediction in AAA patients: Table 14.6

Prediction of mortality in AAA patients could be based on the simple full model estimates, based on maximum likelihood (**full.coef)**. Shrinkage or penalization leads to estimates closer to zero (**penalized.m**). The adaptation results were shrunk with an overall factor of 0.9, based on the empirical behavior in the GUSTO data set, as described in Table 14.3, section 14.1.9 of *Clinical Prediction Models*. The adapted estimates are rather somewhat in between the **full.coef** and **penalized.m**estimates, and are kind of compromise with the univariate literature estimates (**lit coefs)**. Smaller adapted estimates than in the penalized model are obtained for `MI` (history of a myocardial infarction), reflecting the finding of a univariate estimate of 1.5 in the IPD versus 1.0 in the literature.

An alternative approach is to calibrate a score based on the univariate literature coefficients. The calibration factor is 0.69, estimated by using the score as a single predictor:

`lit.score  <- X %*% lit.coefficients`

`fit.lit     <- lrm.fit(y=full$y, x=lit.score)`

Finally, we can calibrate the scores to an average risk of 5%. This recalibration was implemented by the log(odds ratio) for odds(5%) / odds(case study) to the estimated logistic regression model intercept :

`fit.offset$coef[1] + log((0.05/0.95)/(18/220))`

```{r adaptation.performance}
# fit with adapted coefficients in score to obtain c statistics
adapt.fit <- lrm.fit(y=full$y, x=offset.AAA)
tab14 <- matrix(c(full$stats[6], full.penalized$stats[6], adapt.fit$stats[6], fit.lit$stats[6]), nrow=1)
rownames(tab14) <- "Apparent c statistic"
colnames(tab14) <- c("full.coef", "penalized.m", "adapted", "lit score")
kable(tab14, digits=3, caption = "C stats")

# calibration
tab14 <- matrix(c(adapt.fit$coef[2], 1/adapt.fit$coef[2] ), nrow=1)
rownames(tab14) <- ""
colnames(tab14) <- c("adapted coefficient", "adapted effective shrinkage")
kable(tab14, digits=3, caption = "Calibration insights")

```

The apparent discriminative performance is all similar for these sets of coefficients, with c statistics around 0.83. We noted from the bootstrap procedure above that the internally validated estimate was 0.80 rather than 0.83. We do not know what the validated performance is for the model variants based on literature data (**adapted**; **lit score**).

For calibration, we noted that the estimated shrinkage factor for the full model was around 0.7. When we fit a model with adapted coefficients, the coefficient is 1.29; so an effective shrinkage of 1/1.29 = 0.78 was built in by considering the literature estimates in an adaptation approach.

------------------------------------------------------------------------

## Conclusions {-}

Findings from other studies can be used in various ways to develop a locally applicable prediction model.

-   The modeling with `impact` data for TBI patients illustrated an IPD MA approach. A global model can readily be derived with a local baseline risk estimate.

-   The modeling with `AAA` data for aneurysm surgery illustrated two variants of adaptation approaches:

    1.  simple, subtracting the difference in IPD data between univariate and multivariable coefficient from univariate literature estimates;
    2.  a more sophisticated approach, with an optimized adaptation factor from a bootstrap procedure

-   We can also recalibrate a simple score based on univariate literature coefficients. This is a variant of naive Bayes modeling.

By definition, estimates that borrow information of other studies are more stable than per study estimates; the assumption is that the observed predictor associations in other studies are relevant for the local setting. For baseline risk, substantial heterogeneity was observed in the TBI case study. For the AAA study, calibration to an average risk of 5% was implemented.

Local validation studies are needed to confirm the applicability of prediction models; followed by updating where needed ([Binuya 2022](https://pubmed.ncbi.nlm.nih.gov/36510134/ "Methodological guidance for the evaluation and updating of clinical prediction models: a systematic review")).
