[["index.html", "Clinical prediction models Preface", " Clinical prediction models Ewout Steyerberg 2022-11-16 Preface Work in progress This bookdown-based website containing the supplementary materials from ‘Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating’, by E. W. Steyerberg (2009). "],["introduction.html", "1 Introduction", " 1 Introduction Chapter 1 has no additional material. "],["applications.html", "2 Applications of Prediction Models", " 2 Applications of Prediction Models See here for additional material. "],["design.html", "3 Study Design for Prediction Modeling", " 3 Study Design for Prediction Modeling Chapter 3 has no additional material. "],["statmods.html", "4 Statistical Models for Prediction", " 4 Statistical Models for Prediction Chapter 4 has no additional material. "],["overfitting.html", "5 Overfitting and Optimism in Prediction Models 5.1 Figures 5.2 to 5.5", " 5 Overfitting and Optimism in Prediction Models 5.1 Figures 5.2 to 5.5 5.1.1 Fig 5.2: Noise in estimating 10% mortality per center # Surg mortality; 10% par(mfrow = c(1, 1), mar = c(5, 5, 1, 1)) for (mort in c(.1)) { ## ,0.05,0.02,.01)) { # 4 mortalities or only 1 plot( x = seq(from = -.025, to = .975, by = .05), dbinom(x = 0:20, 20, mort), axes = F, type = &quot;s&quot;, lwd = 2, xlim = c(-.05, .35), ylim = c(0, .33), col = mycolors[2], xlab = paste(&quot;Observed mortality, true mortality &quot;, round(100 * mort, 0), &quot;%&quot;, sep = &quot;&quot;), ylab = &quot;probability density&quot; ) axis(side = 1, at = c(0, .1, .2, .3), labels = c(&quot;0%&quot;, &quot;10%&quot;, &quot;20%&quot;, &quot;30%&quot;)) axis(side = 2, at = c(0, 0.1, .2, .3, .4, .5, .6, .7), labels = c(&quot;0%&quot;, &quot;10%&quot;, &quot;20%&quot;, &quot;30%&quot;, &quot;40%&quot;, &quot;50%&quot;, &quot;60%&quot;, &quot;70%&quot;)) text(x = mort, y = .02 + dbinom(x = max(round(mort * 20), 0), 20, mort), labels = paste(&quot;n=20&quot;), col = mycolors[2]) for (i in c(50, 200)) { # add more sample sizes lines( x = seq(from = 0 - (0.5 * 1 / i), to = 1 - (0.5 * 1 / i), by = 1 / i), dbinom(x = 0:i, i, mort), type = &quot;s&quot;, lty = ifelse(i == 50, 2, 4), lwd = 2, col = mycolors[ifelse(i == 50, 3, 4)] ) text(x = mort, y = .02 + dbinom(x = max(round(mort * i), 0), i, mort), labels = paste(&quot;n=&quot;, i, sep = &quot;&quot;), col = mycolors[ifelse(i == 50, 3, 4)]) } # end loop n=50,200 } # end loop mort ## End Fig 5.2 ## ## function for Fig 5.3 and Fig 5.4: Noise vs Heterogeneity illustrate_noise_heterogeneity &lt;- function(n = 20, mort = 0.1, tau = c(0, .01, .02, .03)) { par(mfrow = c(2, 2), pty = &quot;m&quot;, mar = c(2.5, 4, 1.5, 1)) # Make data set with 100 centers, each 20 patients, 10% mortality, variability sd 0 to 0.03 seedn &lt;- 102 set.seed(seedn) ncenter &lt;- 50 nsubjects &lt;- n # n can be changed # simple SD used on probability scale, can be improved upon for (sdtau in tau) { # set for tau can be changed truemort &lt;- rnorm(n = ncenter, mean = mort, sd = sdtau) # mort can be changed mortmat &lt;- as.matrix(cbind(1:ncenter, sapply(truemort, FUN = function(x) rbinom(n = 1, nsubjects, x)) / nsubjects, truemort)) # Start plotting plot(x = 0, y = 0, pch = &quot;&quot;, xlim = c(-.2, 1.2), ylim = c(-.03, .35), axes = F, xlab = &quot;&quot;, ylab = ifelse(sdtau == 0 | sdtau == .02, &quot;Mortality&quot;, &quot;&quot;)) axis(side = 2, at = c(0, .1, .2, .3), labels = c(&quot;0%&quot;, &quot;10%&quot;, &quot;20%&quot;, &quot;30%&quot;), las = 1) axis(side = 1, at = c(0, 1), labels = c(&quot;Observed&quot;, &quot;True mortality&quot;)) text(x = 1, y = .3, ifelse(sdtau == 0, &quot;No heterogeneity&quot;, ifelse(sdtau != 0, paste(&quot;+/-&quot;, sdtau)) ), cex = 1, adj = 1, font = 2) for (i in (1:ncenter)) { set.seed(i + seedn) lines( x = c(0 + runif(1, min = -.07, max = .07), 1), y = c(mortmat[i, 2] + runif(1, min = -.001, max = .01), mortmat[i, 3]), col = mycolors[rep(1:10, 10)[i]] ) set.seed(i + seedn) points( x = c(0 + runif(1, min = -.07, max = .07), 1), y = c(mortmat[i, 2] + runif(1, min = -.001, max = .01), mortmat[i, 3]), pch = c(&quot;o&quot;, &quot;+&quot;), col = mycolors[rep(1:10, 10)[i]] ) } } } # end function that illustrates the impact of noise (determined by n) vs heterogeneity (determined by sdtau) 5.1.2 Figs 5.3 and 5.4 These plots llustrate the impact of noise (determined by n, 20 or 200) vs heterogeneity (determined by sdtau (0 - 0.03)). With small n, such as n=20 per center, mortality such as 10% cannot be estimated reliably. Reliable estimation of a center’s performance requires a large n, such as n=200. 5.1.3 n=20 5.1.4 n=200 "],["altmods.html", "6 Choosing Between Alternative Models 6.1 Non-linearity illustrations", " 6 Choosing Between Alternative Models 6.1 Non-linearity illustrations 6.1.1 Prepare GUSTO data Some logistic regression fits with linear, square, rcs, linear spline terms # Import gusto; publicly available gusto &lt;- read.csv(&quot;data/gusto_age.csv&quot;)[-1] Fmort &lt;- as.data.frame(read.csv(&quot;data/Fmort.csv&quot;))[-1] Fmort$age10 &lt;- Fmort$age / 10 Fmort$age102 &lt;- Fmort$age10^2 6.1.2 anova results for the different fits We note minor differences between the continuous fits, and a clear loss of information for the dichtomization at age 65 years anova(agegusto.linear) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1729 1 &lt;.0001 ## TOTAL 1729 1 &lt;.0001 anova(agegusto.square) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1858.3 2 &lt;.0001 ## Nonlinear 13.2 1 3e-04 ## TOTAL 1858.3 2 &lt;.0001 anova(agegusto.rcs) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1878.5 4 &lt;.0001 ## Nonlinear 24.7 3 &lt;.0001 ## TOTAL 1878.5 4 &lt;.0001 anova(agegusto.linearspline) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1847 2 &lt;.0001 ## TOTAL 1847 2 &lt;.0001 anova(agegusto.cat65) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1263 1 &lt;.0001 ## TOTAL 1263 1 &lt;.0001 6.1.3 Plotting of age effects Plot age effect first at lp scale (logodds), then at probability scale Age effect at logodds scale; Age effect at probability scale Fig 6.1 6.1.4 Start surgical mortality by age in Medicare Age effect at logodds scale 6.1.5 anova results for the fit of age, with interaction by type of surgery Type of surgery is clearly most relevant (chi2 &gt;13500) in all fits. Age is als relevant (chi2&gt;3000), and a square term is not needed (chi2 = 2); the interaction adds a little bit (chi2 95). With these large numbers (1.1M patients), most effects have p&lt;.0001. We will evaluate the differences between fits with or without interaction term graphically further down # Look for model improvements anova(fitplot2) # linear age effect, no interaction with surgery ## Wald Statistics Response: mort ## ## Factor Chi-Square d.f. P ## surgery 13500 13 &lt;.0001 ## age 3167 1 &lt;.0001 ## TOTAL 16446 14 &lt;.0001 anova(fitage2) # age square added ## Wald Statistics Response: mort ## ## Factor Chi-Square d.f. P ## surgery 13499.66 13 &lt;.0001 ## age10 18.13 1 &lt;.0001 ## age102 2.33 1 0.127 ## TOTAL 16424.97 15 &lt;.0001 anova(fitplot) # interaction added to linear age effect ## Wald Statistics Response: mort ## ## Factor Chi-Square d.f. P ## surgery (Factor+Higher Order Factors) 13566.0 26 &lt;.0001 ## All Interactions 94.5 13 &lt;.0001 ## age (Factor+Higher Order Factors) 3280.7 14 &lt;.0001 ## All Interactions 94.5 13 &lt;.0001 ## surgery * age (Factor+Higher Order Factors) 94.5 13 &lt;.0001 ## TOTAL 16620.3 27 &lt;.0001 6.1.6 Plotting of predicted age effects, with interaction by type of surgery; add 95% CI Plot age effects at logodds scale with 95% CI 6.1.7 Plotting of age effects with original data points Fit with interaction (solid lines) and no interaction (dashed lines) "],["missing.html", "7 Missing values", " 7 Missing values Chapter 7 additional material upcoming. "],["missing-case.html", "8 Case Study on Dealing with Missing Values", " 8 Case Study on Dealing with Missing Values Chapter 8 additional material upcoming. "],["coding.html", "9 Coding of Categorical and Continuous Predictors 9.1 Figues 9.1 to 9.6", " 9 Coding of Categorical and Continuous Predictors 9.1 Figues 9.1 to 9.6 GUSTO-I is a data set with patients suffering from an acute myocardial infarction, where we want to predict 30-day mortality. TBI is a data set with patients suffering from a moderate or severe traumatic brain injury. # Import gusto gusto &lt;- read.csv(&quot;data/gusto_age_STE.csv&quot;)[, -1] # Import sample4; n=785 gustos &lt;- read.csv(&quot;data/Gustos4Age.csv&quot;) # Import TBI data; n=2159 TBI &lt;- read.csv(&quot;data/TBI2vars.csv&quot;) 9.1.1 Fig 9.1: Age linear; add square; rcs in GUSTO-I agegusto.linear &lt;- lrm(DAY30 ~ AGE, data = gusto, x = T, y = T) agegusto.square &lt;- lrm(DAY30 ~ pol(AGE, 2), data = gusto, x = T, y = T) agegusto.rcs &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gusto, x = T, y = T) ## dichotomize agegusto.cat65 &lt;- lrm(DAY30 ~ ifelse(AGE &lt; 65, 0, 1), data = gusto, x = T, y = T) ## 3 categories agegusto.3cat &lt;- lrm(DAY30 ~ ifelse(AGE &lt; 60, 0, ifelse(AGE &lt; 70, 1, 2)), data = gusto, x = T, y = T) # Predict for age 20:95 newdata.age &lt;- data.frame(&quot;AGE&quot; = seq(20, 95, by = 0.1)) pred.agegusto.linear &lt;- predict(agegusto.linear, newdata.age) pred.agegusto.square &lt;- predict(agegusto.square, newdata.age) pred.agegusto.rcs &lt;- predict(agegusto.rcs, newdata.age) pred.agegusto.cat65 &lt;- predict(agegusto.cat65, newdata.age) pred.agegusto.3cat &lt;- predict(agegusto.3cat, newdata.age) # Make plot dd &lt;- datadist(gusto) options(datadist = &quot;dd&quot;) # for rms par(mfrow = c(1, 1)) plot( x = newdata.age[, 1], y = pred.agegusto.linear, xlim = c(20, 92), ylim = c(-7, 0), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Age in years&quot;, ylab = &quot;logit of 30-day mortality&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) axis(1, at = c(40, 50, 60, 65, 70, 80, 90)) lines(x = newdata.age[, 1], y = pred.agegusto.cat65, lty = 2, lwd = 2, col = mycolors[3]) lines(x = newdata.age[, 1], y = pred.agegusto.3cat, lty = 3, lwd = 2, col = mycolors[4]) scat1d(x = gusto$AGE, side = 1, frac = .05, col = &quot;darkblue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;linear&quot;, &quot;&lt;65 vs &gt;=65&quot;, &quot;&lt;60, 60-69, 70+&quot;), lty = c(1, 2, 3), lwd = 2, cex = 1, bty = &quot;n&quot;, col = mycolors[2:4] ) 9.1.2 Fig 9.2: Impact of number of ST elevations (STE) # winsorize at STE 10 gusto$STE &lt;- ifelse(gusto$STE &gt; 10, 10, gusto$STE) gusto$STE.f &lt;- as.factor(gusto$STE) STE.linear &lt;- lrm(DAY30 ~ STE, data = gusto, x = T, y = T) STE.square &lt;- lrm(DAY30 ~ pol(STE, 2), data = gusto, x = T, y = T) STE.rcs &lt;- lrm(DAY30 ~ rcs(STE, 5), data = gusto, x = T, y = T) STE.factor &lt;- lrm(DAY30 ~ STE.f, data = gusto, x = T, y = T) # dichotomize STE.cat4 &lt;- lrm(DAY30 ~ ifelse(STE &lt; 5, 0, 1), data = gusto, x = T, y = T) # predict for STE 0:10 newdata.STE &lt;- data.frame(&quot;STE&quot; = seq(0, 10, by = 1)) pred.STE.linear &lt;- predict(STE.linear, newdata.STE) pred.STE.square &lt;- predict(STE.square, newdata.STE) pred.STE.rcs &lt;- predict(STE.rcs, newdata.STE) pred.STE.cat4 &lt;- predict(STE.cat4, newdata.STE) par(mfrow = c(1, 1)) plot( x = newdata.STE[, 1], y = pred.STE.linear, las = 1, xlab = &quot;Number of leads with ST elevation&quot;, ylab = &quot;logit of 30-day mortality&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) lines(x = newdata.STE[, 1], y = pred.STE.square, lty = 2, lwd = 2, col = mycolors[3]) lines(x = newdata.STE[, 1], y = pred.STE.rcs, lty = 3, lwd = 3, col = mycolors[4]) lines(x = newdata.STE[1:5, 1], y = pred.STE.cat4[1:5], lty = 4, lwd = 2, col = mycolors[5]) lines(x = newdata.STE[6:11, 1], y = pred.STE.cat4[6:11], lty = 4, lwd = 2, col = mycolors[5]) # Original data points, with size proportional to sqrt(events) STEmort &lt;- log(by(gusto$DAY, gusto$STE, mean) / (1 - by(gusto$DAY, gusto$STE, mean)))[1:11] STEw &lt;- sqrt(by(gusto$DAY, gusto$STE, sum)[1:11]) / 10 points(x = newdata.STE[, 1], y = STEmort, pch = 1, lwd = 2, cex = STEw, col = &quot;black&quot;) scat1d(x = gusto$STE, side = 1, frac = .05, col = &quot;darkblue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;data&quot;, &quot;linear&quot;, &quot;.. +square&quot;, &quot;rcs&quot;, &quot;&lt;=4, vs &gt;4&quot;), lty = c(NA, 1, 2, 3, 4), pch = c(1, NA, NA, NA, NA), lwd = 2, cex = 1, bty = &quot;n&quot;, col = mycolors[1:5] ) 9.1.3 Fig 9.3: Non-linearities in small sample (n=751); and full GUSTO-I (n=40,830) # Examine non-linearities in gustos sample # Age age.linear &lt;- lrm(DAY30 ~ AGE, data = gustos, x = T, y = T, linear.predictors = F) age.rcs1 &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gustos, x = T, y = T, linear.predictors = F) age.fp1 &lt;- mfp(DAY30 ~ fp(AGE, df = 4), alpha = 1, data = gustos, family = binomial) # selected: -2 and 3 age.gam1 &lt;- gam(DAY30 ~ s(AGE), data = gustos, family = binomial) # examine predictions for age 20:95 age.mat &lt;- matrix(nrow = 751, ncol = 5) names(age.mat) &lt;- list(NULL, Cs(AGE, linear, fp, rcs, gam)) AGE &lt;- seq(20, 95, by = 0.1) age.mat[, 1] &lt;- AGE age.mat[, 2] &lt;- predict(age.linear, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat[, 3] &lt;- predict(age.fp1, newdata = as.data.frame(x = AGE)) age.mat[, 4] &lt;- predict(age.rcs1, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat[, 5] &lt;- predict(age.gam1, newdata = as.data.frame(x = AGE)) # Plot for n=785, Fig 9.3, part A # par(mfrow = c(1, 2)) plot( x = age.mat[, 1], y = age.mat[, 2], xlim = c(20, 92), ylim = c(-8, 0.2), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Age in years&quot;, ylab = &quot;logit of 30-day mortality&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) axis(1, at = c(30, 40, 50, 60, 70, 80, 90)) lines(x = age.mat[, 1], y = age.mat[, 3], lty = 2, lwd = 2, col = mycolors[3]) lines(x = age.mat[, 1], y = age.mat[, 4], lty = 3, lwd = 3, col = mycolors[4]) lines(x = age.mat[, 1], y = age.mat[, 5], lty = 4, lwd = 3, col = mycolors[5]) histSpike(x = gustos$AGE, side = 1, frac = .1, col = &quot;darkblue&quot;, add = T) legend(&quot;topleft&quot;, legend = c(&quot;linear&quot;, &quot;fp&quot;, &quot;rcs&quot;, &quot;gam&quot;), lty = 1:4, lwd = 2, cex = 1.2, bty = &quot;n&quot;, col = mycolors[2:5] ) title(&quot;GUSTO-I, n=785&quot;) # End plot n=785 #################### ## Now: N=40,830 ## age.linear.2 &lt;- lrm(DAY30 ~ AGE, data = gusto, x = T, y = T, linear.predictors = F) age.rcs1.2 &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gusto, x = T, y = T, linear.predictors = F) age.fp1.2 &lt;- mfp(DAY30 ~ fp(AGE, df = 4), alpha = 1, data = gusto, family = binomial) # selected: -2 and 3 age.gam1.2 &lt;- gam(DAY30 ~ s(AGE), data = gusto, family = binomial) # examine predictions for age 20:95 age.mat.2 &lt;- matrix(nrow = 751, ncol = 5) names(age.mat.2) &lt;- list(NULL, Cs(AGE, linear, fp, rcs, gam)) age.mat.2[, 1] &lt;- AGE age.mat.2[, 2] &lt;- predict(age.linear.2, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat.2[, 3] &lt;- predict(age.fp1.2, newdata = as.data.frame(x = AGE)) age.mat.2[, 4] &lt;- predict(age.rcs1.2, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat.2[, 5] &lt;- predict(age.gam1.2, newdata = as.data.frame(x = AGE)) # Plot for n=40830 plot( x = age.mat.2[, 1], y = age.mat.2[, 2], xlim = c(20, 92), ylim = c(-8, 0.2), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Age in years&quot;, ylab = &quot;&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) axis(1, at = c(30, 40, 50, 60, 70, 80, 90)) lines(x = age.mat.2[, 1], y = age.mat.2[, 3], lty = 2, lwd = 2, col = mycolors[3]) lines(x = age.mat.2[, 1], y = age.mat.2[, 4], lty = 3, lwd = 3, col = mycolors[4]) lines(x = age.mat.2[, 1], y = age.mat.2[, 5], lty = 4, lwd = 3, col = mycolors[5]) histSpike(x = gusto$AGE, side = 1, frac = .2, col = &quot;darkblue&quot;, add = T) title(&quot;GUSTO-I, n=40,830&quot;) 9.1.4 Fig 9.4: Glucose and hb in TBI We evaluate the predictors ‘glucose’ and ‘hemoglobin’ in TBI. Both are first truncated, after inspecting boxplots (Fig 9.4), and then analyzed for their prognostic value, with plots for illustration of the estimated relations (Fig 9.5). # glucose quantile(TBI$glucose, probs = c(.005, .01, .02, .98, .99, .995), na.rm = T) ## 0.5% 1% 2% 98% 99% 99.5% ## 1.57 2.26 4.28 18.34 20.92 23.39 # 0.5% 1% 2% 98% 99% 99.5% # 1.57 2.26 4.28 18.34 20.92 23.39 # So, winsorize / truncate at 3 and 20 # use simple function to do the winsorizing: # {ifelse(x&lt;lower,upper, ifelse(x&gt;upper,upper, x))} winsorize &lt;- function(x, lower = quantile(x, probs = 0.01), upper = quantile(x, probs = 0.99)) { ifelse(x &lt; lower, lower, ifelse(x &gt; upper, upper, x) ) } TBI$glucoset &lt;- winsorize(TBI$glucose, 3, 20) # systolic bp quantile(TBI$hb, probs = c(.005, .01, .02, .98, .99, .995), na.rm = T) ## 0.5% 1% 2% 98% 99% 99.5% ## 4.87 5.40 6.30 16.50 16.80 17.00 # 0.5% 1% 2% 98% 99% 99.5% # 4.87 5.40 6.30 16.50 16.80 17.00 # So, winsorize / truncate at 6 and 17 TBI$hbt &lt;- winsorize(TBI$hb, 6, 17) # boxplots for illustration par(mfrow = c(1, 2)) boxplot(x = cbind(TBI$glucose, TBI$glucoset), outcol = c(&quot;red&quot;, mycolors[4]), border = mycolors[c(10, 4)], xaxt = &quot;n&quot;, ylab = &quot;glucose (mmol/L)&quot;) axis(1, at = c(1, 2), labels = c(&quot;before &quot;, &quot;after winsorizing&quot;)) title(&quot;Glucose&quot;) boxplot(x = cbind(TBI$hb, TBI$hbt), outcol = c(&quot;red&quot;, mycolors[4]), border = mycolors[c(10, 4)], xaxt = &quot;n&quot;, ylab = &quot;hb (mmol/L)&quot;) axis(1, at = c(1, 2), labels = c(&quot;before &quot;, &quot;after winsorizing&quot;)) title(&quot;Hemoglobin&quot;) 9.1.5 Fig 9.5: Non-linear association of glucose 9.1.6 Fig 9.6: Systolic blood pressure in TBI We now examine the prognostic value of systolic blood pressure (BP) in TBI patients. We expect low BP (hypotension) to be especially risky. quantile(TBI$d.sysbpt, probs = c(.01, .25, .75, .99), na.rm = T) ## 1% 25% 75% 99% ## 92.9 121.1 142.1 171.8 TBIs &lt;- TBI[!is.na(TBI$d.sysbpt), ] # 1% 25% 75% 99% # 92.9 121.1 142.1 171.8 g1 &lt;- lrm(d.gos &lt; 2 ~ rcs(d.sysbpt, 3), data = TBIs) g2 &lt;- lrm(d.gos &lt; 3 ~ rcs(d.sysbpt, 3), data = TBIs) g3 &lt;- lrm(d.gos &lt; 4 ~ rcs(d.sysbpt, 3), data = TBIs) g4 &lt;- lrm(d.gos &lt; 5 ~ rcs(d.sysbpt, 3), data = TBIs) # Define categorical variants of systolic BP TBIs$d.sysbpt.c &lt;- as.factor(ifelse(TBIs$d.sysbpt &lt; 120, 1, ifelse(TBIs$d.sysbpt &gt; 150, 2, 0))) g1t &lt;- lrm(d.gos &lt; 2 ~ d.sysbpt.c, data = TBIs) g2t &lt;- lrm(d.gos &lt; 3 ~ d.sysbpt.c, data = TBIs) g3t &lt;- lrm(d.gos &lt; 4 ~ d.sysbpt.c, data = TBIs) g4t &lt;- lrm(d.gos &lt; 5 ~ d.sysbpt.c, data = TBIs) dd &lt;- datadist(TBIs) options(datadist = &quot;dd&quot;) # Odds ratios exp(coef(g1t)) # OR 2.78 for low BP and 1.25 for high BP ## Intercept d.sysbpt.c=1 d.sysbpt.c=2 ## 0.226 2.777 1.245 describe(TBI$d.sysbpt) ## TBI$d.sysbpt ## n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 ## 2159 0 1566 1 131.4 17.61 107.0 112.2 121.1 131.3 142.1 ## .90 .95 ## 151.1 156.2 ## ## lowest : 60.0 64.2 65.6 71.3 73.7, highest: 182.9 184.2 184.7 188.3 207.4 describe(TBIs$d.sysbpt.c) ## TBIs$d.sysbpt.c ## n missing distinct ## 2159 0 3 ## ## Value 0 1 2 ## Frequency 1435 474 250 ## Proportion 0.665 0.220 0.116 # data in matrix for plot d.sysbpt &lt;- seq(60, 210, by = 1) # 151 elements g.mat &lt;- matrix(nrow = 151, ncol = 5) names(g.mat) &lt;- list(NULL, Cs(d.sysbpt, g1, g2, g3, g4)) g.mat[, 1] &lt;- d.sysbpt g.mat[, 2] &lt;- predict(g1, newdata = as.data.frame(x = d.sysbpt)) g.mat[, 3] &lt;- predict(g2, newdata = as.data.frame(x = d.sysbpt)) g.mat[, 4] &lt;- predict(g3, newdata = as.data.frame(x = d.sysbpt)) g.mat[, 5] &lt;- predict(g4, newdata = as.data.frame(x = d.sysbpt)) # Plot: Fig 9.6, part I par(mfrow = c(1, 2)) plot( x = g.mat[, 1], y = g.mat[, 2], xlim = c(50, 210), ylim = c(-2.7, 2.7), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Systolic blood pressure (mmHg)&quot;, ylab = &quot;logit GOS at 6 months&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[1] ) axis(1, at = c(100, 150, 200)) lines(x = g.mat[, 1], y = g.mat[, 3], lty = 2, lwd = 2, col = mycolors[2]) lines(x = g.mat[, 1], y = g.mat[, 4], lty = 3, lwd = 3, col = mycolors[3]) lines(x = g.mat[, 1], y = g.mat[, 5], lty = 4, lwd = 3, col = mycolors[4]) histSpike(x = TBI$d.sysbpt, side = 3, frac = .2, col = &quot;darkblue&quot;, add = T) legend(&quot;bottomleft&quot;, legend = c(&quot;&lt;good recovery&quot;, &quot;unfavorable&quot;, &quot;mort+veg&quot;, &quot;mortality&quot;), lty = 4:1, lwd = 2, cex = 0.8, bty = &quot;n&quot;, col = mycolors[4:1]) title(&quot;continuous, rcs 3 knots&quot;) ## Plot 9.6, part II d.sysbpt.c &lt;- c(0, 1, 2) d.sysbpt &lt;- seq(60, 210, by = .1) # 1501 elements g.mat2 &lt;- matrix(nrow = 1501, ncol = 5) g1t.c &lt;- predict(g1t, newdata = as.data.frame(x = d.sysbpt.c)) g2t.c &lt;- predict(g2t, newdata = as.data.frame(x = d.sysbpt.c)) g3t.c &lt;- predict(g3t, newdata = as.data.frame(x = d.sysbpt.c)) g4t.c &lt;- predict(g4t, newdata = as.data.frame(x = d.sysbpt.c)) names(g.mat2) &lt;- list(NULL, Cs(d.sysbpt, g1, g2, g3, g4)) g.mat2[, 1] &lt;- d.sysbpt g.mat2[, 2] &lt;- ifelse(d.sysbpt &lt; 120, g1t.c[2], ifelse(d.sysbpt &gt; 150, g1t.c[3], g1t.c[1])) g.mat2[, 3] &lt;- ifelse(d.sysbpt &lt; 120, g2t.c[2], ifelse(d.sysbpt &gt; 150, g2t.c[3], g2t.c[1])) g.mat2[, 4] &lt;- ifelse(d.sysbpt &lt; 120, g3t.c[2], ifelse(d.sysbpt &gt; 150, g3t.c[3], g3t.c[1])) g.mat2[, 5] &lt;- ifelse(d.sysbpt &lt; 120, g4t.c[2], ifelse(d.sysbpt &gt; 150, g4t.c[3], g4t.c[1])) # Plot plot( x = g.mat2[, 1], y = g.mat2[, 2], xlim = c(50, 210), ylim = c(-2.7, 2.7), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Systolic blood pressure (mmHg)&quot;, ylab = &quot;&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[1] ) axis(1, at = c(100, 150, 200)) lines(x = g.mat2[, 1], y = g.mat2[, 3], lty = 2, lwd = 2, col = mycolors[2]) lines(x = g.mat2[, 1], y = g.mat2[, 4], lty = 3, lwd = 3, col = mycolors[3]) lines(x = g.mat2[, 1], y = g.mat2[, 5], lty = 4, lwd = 3, col = mycolors[4]) histSpike(x = TBI$d.sysbpt, side = 3, frac = .2, col = &quot;darkblue&quot;, add = T) title(&quot;3 categories&quot;) "],["restrictions.html", "10 Restrictions on Candidate Predictors", " 10 Restrictions on Candidate Predictors Chapter 10 additional material upcoming. "],["selection.html", "11 Selection of Main Effects", " 11 Selection of Main Effects Chapter 11 additional material upcoming. "],["additivity-linearity.html", "12 Assumptions in Regression Models - Additivity and Linearity 12.1 GUSTO-I interaction analysis 12.2 MFP and other non-linear analyses in n544 data", " 12 Assumptions in Regression Models - Additivity and Linearity 12.1 GUSTO-I interaction analysis 12.1.1 Examine interactions # Import gusto, gustoB, and sample4 data sets gusto &lt;- read.csv(&quot;data/gusto1.csv&quot;) # GUSTO sample with 40830 patients gustoB &lt;- read.csv(&quot;data/gustoB.csv&quot;) # GUSTO part B sample with 20318 patients gustos &lt;- read.csv(&quot;data/sample4.csv&quot;) # GUSTO sample4 with 785 patients source(&quot;R/auc.nonpara.mw.R&quot;) source(&quot;R/ci.auc.R&quot;) source(&quot;R/val.prob.ci.2.R&quot;) # levels(gustos$HRT) &lt;- c(&quot;No tachycardia&quot;, &quot;Tachycardia&quot;) dd &lt;- datadist(gustos) ## Warning in datadist(gustos): ESAMP is constant ## Warning in datadist(gustos): GRPL is constant ## Warning in datadist(gustos): REGL is constant options(datadist = &quot;dd&quot;) Evaluate interactions with age in a full model, which includes 8 predictors in total. The data set is small (sample4, n=785, 52 events) ### Full model and age interactions full &lt;- lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) fullint &lt;- lrm(DAY30 ~ AGE * (KILLIP + HIG + DIA + HYP + HRT + TTR + SEX), data = gustos, x = T, y = T, linear.predictors = F) anova(fullint) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE (Factor+Higher Order Factors) 29.93 8 0.0002 ## All Interactions 11.01 7 0.1380 ## KILLIP (Factor+Higher Order Factors) 4.96 2 0.0839 ## All Interactions 1.01 1 0.3160 ## HIG (Factor+Higher Order Factors) 6.32 2 0.0424 ## All Interactions 2.20 1 0.1380 ## DIA (Factor+Higher Order Factors) 3.70 2 0.1571 ## All Interactions 0.64 1 0.4220 ## HYP (Factor+Higher Order Factors) 5.62 2 0.0603 ## All Interactions 1.46 1 0.2267 ## HRT (Factor+Higher Order Factors) 13.50 2 0.0012 ## All Interactions 4.49 1 0.0341 ## TTR (Factor+Higher Order Factors) 8.31 2 0.0157 ## All Interactions 2.80 1 0.0942 ## SEX (Factor+Higher Order Factors) 0.79 2 0.6736 ## All Interactions 0.42 1 0.5151 ## AGE * KILLIP (Factor+Higher Order Factors) 1.01 1 0.3160 ## AGE * HIG (Factor+Higher Order Factors) 2.20 1 0.1380 ## AGE * DIA (Factor+Higher Order Factors) 0.64 1 0.4220 ## AGE * HYP (Factor+Higher Order Factors) 1.46 1 0.2267 ## AGE * HRT (Factor+Higher Order Factors) 4.49 1 0.0341 ## AGE * TTR (Factor+Higher Order Factors) 2.80 1 0.0942 ## AGE * SEX (Factor+Higher Order Factors) 0.42 1 0.5151 ## TOTAL INTERACTION 11.01 7 0.1380 ## TOTAL 64.59 15 &lt;.0001 ### Select only interaction AGE * HRT fullints &lt;- lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = T) anova(fullints) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE (Factor+Higher Order Factors) 26.67 2 &lt;.0001 ## All Interactions 2.70 1 0.1004 ## KILLIP 3.52 1 0.0605 ## HIG 5.38 1 0.0204 ## DIA 3.14 1 0.0763 ## HYP 3.49 1 0.0618 ## HRT (Factor+Higher Order Factors) 11.67 2 0.0029 ## All Interactions 2.70 1 0.1004 ## TTR 5.66 1 0.0174 ## SEX 0.19 1 0.6602 ## AGE * HRT (Factor+Higher Order Factors) 2.70 1 0.1004 ## TOTAL 65.30 9 &lt;.0001 12.1.2 Fig 12.1 Make 2 plots with linear interaction, in the small n=785 sample, and in the full n=40830 sample 12.1.3 Fig 12.2 Make 4 plots with main effects, linear interaction, and 2 variants of interaction only above age 55. The variable is (Age-55)[+]. In the last graph, the green dotted line follows the angle from below age 55 years (only 1 Age effect is estimated for the HRT==0 and HRT==1 and age&lt;55 patients). In the pre-final graph, there are 2 separate angles from age 55 for HRT==0 and HRT==1 (barely noticable for the green dotted line). 12.1.4 Smart coding illustration Smart coding of age effect: separate for no HRT (HRT==0) and for HRT (HRT==1) # Smart coding of age effect: separate for no HRT (HRT==0) and for HRT (HRT==1) gustos$AGE0 &lt;- gustos$AGE * (1 - gustos$HRT) gustos$AGE1 &lt;- gustos$AGE * gustos$HRT # Standard lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = F) ## Logistic Regression Model ## ## lrm(formula = DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + ## TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = F) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 785 LR chi2 86.28 R2 0.270 C 0.831 ## 0 733 d.f. 9 R2(9,785)0.094 Dxy 0.662 ## 1 52 Pr(&gt; chi2) &lt;0.0001 R2(9,145.7)0.412 gamma 0.662 ## max |deriv| 1e-09 Brier 0.051 tau-a 0.082 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -9.7431 1.6574 -5.88 &lt;0.0001 ## AGE 0.0759 0.0240 3.16 0.0016 ## KILLIP 0.4595 0.2448 1.88 0.0605 ## HIG 0.7900 0.3407 2.32 0.0204 ## DIA 0.7804 0.4403 1.77 0.0763 ## HYP 1.0403 0.5570 1.87 0.0618 ## HRT -3.6376 2.8352 -1.28 0.1995 ## TTR 0.8201 0.3447 2.38 0.0174 ## SEX -0.1534 0.3490 -0.44 0.6602 ## AGE * HRT 0.0655 0.0399 1.64 0.1004 ## # Smart coding lrm(DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## Logistic Regression Model ## ## lrm(formula = DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + ## HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 785 LR chi2 86.28 R2 0.270 C 0.831 ## 0 733 d.f. 9 R2(9,785)0.094 Dxy 0.662 ## 1 52 Pr(&gt; chi2) &lt;0.0001 R2(9,145.7)0.412 gamma 0.662 ## max |deriv| 1e-09 Brier 0.051 tau-a 0.082 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -9.7431 1.6574 -5.88 &lt;0.0001 ## AGE0 0.0759 0.0240 3.16 0.0016 ## AGE1 0.1414 0.0332 4.26 &lt;0.0001 ## HRT -3.6376 2.8352 -1.28 0.1995 ## KILLIP 0.4595 0.2448 1.88 0.0605 ## HIG 0.7900 0.3407 2.32 0.0204 ## DIA 0.7804 0.4403 1.77 0.0763 ## HYP 1.0403 0.5570 1.87 0.0618 ## TTR 0.8201 0.3447 2.38 0.0174 ## SEX -0.1534 0.3490 -0.44 0.6602 ## # Identical fit, easier interpretation # Age 55 as reference for HRT effect gustos$AGE0 &lt;- (gustos$AGE - 55) * (1 - gustos$HRT) gustos$AGE1 &lt;- (gustos$AGE - 55) * gustos$HRT lrm(DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## Logistic Regression Model ## ## lrm(formula = DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + ## HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 785 LR chi2 86.28 R2 0.270 C 0.831 ## 0 733 d.f. 9 R2(9,785)0.094 Dxy 0.662 ## 1 52 Pr(&gt; chi2) &lt;0.0001 R2(9,145.7)0.412 gamma 0.662 ## max |deriv| 4e-11 Brier 0.051 tau-a 0.082 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -5.5703 0.5746 -9.69 &lt;0.0001 ## AGE0 0.0759 0.0240 3.16 0.0016 ## AGE1 0.1414 0.0332 4.26 &lt;0.0001 ## HRT -0.0333 0.7043 -0.05 0.9623 ## KILLIP 0.4595 0.2448 1.88 0.0605 ## HIG 0.7900 0.3407 2.32 0.0204 ## DIA 0.7804 0.4403 1.77 0.0763 ## HYP 1.0403 0.5570 1.87 0.0618 ## TTR 0.8201 0.3447 2.38 0.0174 ## SEX -0.1534 0.3490 -0.44 0.6602 ## # Even nicer interpretation, HRT effect for age=55 The fit of each of the models is identical (always LR chi2=86.28); each model allows for linear interaction between AGE and HRT. The interpretation of the model is easier if the age effects are estimated for HRT==1 and for HRT==0. Scaling is easier by subtracting 55 from AGE (AGE-55); this implies the HRT effect relates to age 55. 12.1.5 Table 12.2 Better predictions? Assess the performance if the models created in n=785 in an independent validation part, GustoB, n=20318. Plots created with a modification of Frank Harrell’s val.prob() function: # Validate in independent part, named gustoB # main effects lrm.val.full &lt;- predict(full, newdata = gustoB, type = &quot;lp&quot;) # simple interaction lrm.val.int1 &lt;- predict(fullints, newdata = gustoB, type = &quot;lp&quot;) # Plot val.prob.ci.2( y = gustoB[, &quot;DAY30&quot;], logit = lrm.val.full, riskdist = &quot;predicted&quot;, logistic.cal = F, smooth = &quot;rcs&quot;, nr.knots = 3, g = 8, xlim = c(0, .5), ylim = c(0, .5), legendloc = c(0.18, 0.15), statloc = c(0, .4), roundstats = 3, xlab = &quot;Predicted probability from n=785&quot;, ylab = &quot;Observed proportion in n=20318&quot; ) Conclusions Discrimination: worse with interactions than without. Calibration: We note some overfitting, as expected by a fit in a small sample. 12.2 MFP and other non-linear analyses in n544 data Upcoming. "],["estimation.html", "13 Modern Estimation Methods", " 13 Modern Estimation Methods Chapter 13 additional material upcoming. "],["estimation-external.html", "14 Estimation with External Information", " 14 Estimation with External Information Chapter 14 additional material upcoming. "],["evaluation.html", "15 Evaluation of Performance", " 15 Evaluation of Performance Chapter 15 additional material upcoming. "],["usefulness.html", "16 Evaluation of Clinical Usefulness", " 16 Evaluation of Clinical Usefulness Chapter 16 additional material upcoming. "],["validation.html", "17 Validation of Prediction Models", " 17 Validation of Prediction Models Chapter 17 additional material upcoming. "],["presentation.html", "18 Presentation Formats", " 18 Presentation Formats Chapter 18 additional material upcoming. "],["external-validity.html", "19 Patterns of External Validity 19.1 Fig 19.7: Case-control design disturbs calibration", " 19 Patterns of External Validity 19.0.1 Fig 19.1: a missed predictor Z ## Actual correlations in n=1000 ## r=0: -0.0114 r=.33: 0.312 r=.5: 0.475 19.0.2 Fig 19.2: a missed predictor Z with identical distribution –&gt; no impact 19.0.3 Fig 19.3: more or less severe cases selected 19.0.4 Fig 19.4: more or less heterogeneous cases selected 19.0.5 Fig 19.5: more or less severe cases selected by Z –&gt; Selection by missed predictor leads to miscalibration 19.0.6 Fig 19.6: more or less heterogeneous cases selected by Z –&gt; Selection by missed predictor has minor impact 19.1 Fig 19.7: Case-control design disturbs calibration Disturbance is exactly as expected by ratio of selecting cases:controls (log(2)) Select all cases, and half of the controls –&gt; same as shift in intercept 19.1.1 Fig 19.8: Overfitting disturbs discrimination and calibration 19.1.2 Fig 19.9: Different coeficients (model misspecification) disturbs discrimination and calibration 19.1.3 Scenarios: Table 19.4 Change of setting may especially impact calibration RCT vs survey may impact discrimination (more homogeneity) and calibration 19.1.4 Uncertainty in validation simulations "],["updating.html", "20 Updating for a New Setting", " 20 Updating for a New Setting Chapter 20 additional material upcoming. "],["updating-multiple.html", "21 Updating for Multiple Settings", " 21 Updating for Multiple Settings Chapter 21 additional material upcoming. "],["case-study-gusto.html", "22 Case Study on a Prediction of 30-Day Mortality", " 22 Case Study on a Prediction of 30-Day Mortality Chapter 22 additional material upcoming. "],["case-study-cardiovascular.html", "23 Case Study on Survival Analysis: Prediction of Cardiovascular Event", " 23 Case Study on Survival Analysis: Prediction of Cardiovascular Event Chapter 23 additional material upcoming. "],["lessons-datasets.html", "24 Overall Lessons and Data Sets", " 24 Overall Lessons and Data Sets Chapter 24 additional material upcoming. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
