[["index.html", "Clinical prediction models Preface", " Clinical prediction models Ewout Steyerberg 2023-01-12 Preface Work in progress This bookdown-based website containing the supplementary materials from ‘Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating’, by E. W. Steyerberg (2009). "],["introduction.html", "1 Introduction", " 1 Introduction Chapter 1 has no additional material. "],["applications.html", "2 Applications of Prediction Models", " 2 Applications of Prediction Models See here for additional material. "],["design.html", "3 Study Design for Prediction Modeling", " 3 Study Design for Prediction Modeling Chapter 3 has no additional material. "],["statmods.html", "4 Statistical Models for Prediction", " 4 Statistical Models for Prediction Chapter 4 has no additional material. "],["overfitting.html", "5 Overfitting and Optimism in Prediction Models 5.1 Figures 5.2 to 5.5", " 5 Overfitting and Optimism in Prediction Models 5.1 Figures 5.2 to 5.5 5.1.1 Fig 5.2: Noise in estimating 10% mortality per center # Surg mortality; 10% par(mfrow = c(1, 1), mar = c(5, 5, 1, 1)) for (mort in c(.1)) { ## ,0.05,0.02,.01)) { # 4 mortalities or only 1 plot( x = seq(from = -.025, to = .975, by = .05), dbinom(x = 0:20, 20, mort), axes = F, type = &quot;s&quot;, lwd = 2, xlim = c(-.05, .35), ylim = c(0, .33), col = mycolors[2], xlab = paste(&quot;Observed mortality, true mortality &quot;, round(100 * mort, 0), &quot;%&quot;, sep = &quot;&quot;), ylab = &quot;probability density&quot; ) axis(side = 1, at = c(0, .1, .2, .3), labels = c(&quot;0%&quot;, &quot;10%&quot;, &quot;20%&quot;, &quot;30%&quot;)) axis(side = 2, at = c(0, 0.1, .2, .3, .4, .5, .6, .7), labels = c(&quot;0%&quot;, &quot;10%&quot;, &quot;20%&quot;, &quot;30%&quot;, &quot;40%&quot;, &quot;50%&quot;, &quot;60%&quot;, &quot;70%&quot;)) text(x = mort, y = .02 + dbinom(x = max(round(mort * 20), 0), 20, mort), labels = paste(&quot;n=20&quot;), col = mycolors[2]) for (i in c(50, 200)) { # add more sample sizes lines( x = seq(from = 0 - (0.5 * 1 / i), to = 1 - (0.5 * 1 / i), by = 1 / i), dbinom(x = 0:i, i, mort), type = &quot;s&quot;, lty = ifelse(i == 50, 2, 4), lwd = 2, col = mycolors[ifelse(i == 50, 3, 4)] ) text(x = mort, y = .02 + dbinom(x = max(round(mort * i), 0), i, mort), labels = paste(&quot;n=&quot;, i, sep = &quot;&quot;), col = mycolors[ifelse(i == 50, 3, 4)]) } # end loop n=50,200 } # end loop mort ## End Fig 5.2 ## ## function for Fig 5.3 and Fig 5.4: Noise vs Heterogeneity illustrate_noise_heterogeneity &lt;- function(n = 20, mort = 0.1, tau = c(0, .01, .02, .03)) { par(mfrow = c(2, 2), pty = &quot;m&quot;, mar = c(2.5, 4, 1.5, 1)) # Make data set with 100 centers, each 20 patients, 10% mortality, variability sd 0 to 0.03 seedn &lt;- 102 set.seed(seedn) ncenter &lt;- 50 nsubjects &lt;- n # n can be changed # simple SD used on probability scale, can be improved upon for (sdtau in tau) { # set for tau can be changed truemort &lt;- rnorm(n = ncenter, mean = mort, sd = sdtau) # mort can be changed mortmat &lt;- as.matrix(cbind(1:ncenter, sapply(truemort, FUN = function(x) rbinom(n = 1, nsubjects, x)) / nsubjects, truemort)) # Start plotting plot(x = 0, y = 0, pch = &quot;&quot;, xlim = c(-.2, 1.2), ylim = c(-.03, .35), axes = F, xlab = &quot;&quot;, ylab = ifelse(sdtau == 0 | sdtau == .02, &quot;Mortality&quot;, &quot;&quot;)) axis(side = 2, at = c(0, .1, .2, .3), labels = c(&quot;0%&quot;, &quot;10%&quot;, &quot;20%&quot;, &quot;30%&quot;), las = 1) axis(side = 1, at = c(0, 1), labels = c(&quot;Observed&quot;, &quot;True mortality&quot;)) text(x = 1, y = .3, ifelse(sdtau == 0, &quot;No heterogeneity&quot;, ifelse(sdtau != 0, paste(&quot;+/-&quot;, sdtau)) ), cex = 1, adj = 1, font = 2) for (i in (1:ncenter)) { set.seed(i + seedn) lines( x = c(0 + runif(1, min = -.07, max = .07), 1), y = c(mortmat[i, 2] + runif(1, min = -.001, max = .01), mortmat[i, 3]), col = mycolors[rep(1:10, 10)[i]] ) set.seed(i + seedn) points( x = c(0 + runif(1, min = -.07, max = .07), 1), y = c(mortmat[i, 2] + runif(1, min = -.001, max = .01), mortmat[i, 3]), pch = c(&quot;o&quot;, &quot;+&quot;), col = mycolors[rep(1:10, 10)[i]] ) } } } # end function that illustrates the impact of noise (determined by n) vs heterogeneity (determined by sdtau) 5.1.2 Figs 5.3 and 5.4 These plots llustrate the impact of noise (determined by n, 20 or 200) vs heterogeneity (determined by sdtau (0 - 0.03)). With small n, such as n=20 per center, mortality such as 10% cannot be estimated reliably. Reliable estimation of a center’s performance requires a large n, such as n=200. 5.1.3 n=20 5.1.4 n=200 "],["altmods.html", "6 Choosing Between Alternative Models 6.1 Non-linearity illustrations", " 6 Choosing Between Alternative Models 6.1 Non-linearity illustrations 6.1.1 Prepare GUSTO data Some logistic regression fits with linear, square, rcs, linear spline terms # Import gusto; publicly available gusto &lt;- read.csv(&quot;data/gusto_age.csv&quot;)[-1] Fmort &lt;- as.data.frame(read.csv(&quot;data/Fmort.csv&quot;))[-1] Fmort$age10 &lt;- Fmort$age / 10 Fmort$age102 &lt;- Fmort$age10^2 6.1.2 anova results for the different fits We note minor differences between the continuous fits, and a clear loss of information for the dichtomization at age 65 years anova(agegusto.linear) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1728.89 1 &lt;.0001 ## TOTAL 1728.89 1 &lt;.0001 anova(agegusto.square) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1858.27 2 &lt;.0001 ## Nonlinear 13.21 1 3e-04 ## TOTAL 1858.27 2 &lt;.0001 anova(agegusto.rcs) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1878.45 4 &lt;.0001 ## Nonlinear 24.71 3 &lt;.0001 ## TOTAL 1878.45 4 &lt;.0001 anova(agegusto.linearspline) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1846.73 2 &lt;.0001 ## TOTAL 1846.73 2 &lt;.0001 anova(agegusto.cat65) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE 1262.57 1 &lt;.0001 ## TOTAL 1262.57 1 &lt;.0001 6.1.3 Plotting of age effects Plot age effect first at lp scale (logodds), then at probability scale Age effect at logodds scale; Age effect at probability scale Fig 6.1 6.1.4 Start surgical mortality by age in Medicare Age effect at logodds scale 6.1.5 anova results for the fit of age, with interaction by type of surgery Type of surgery is clearly most relevant (chi2 &gt;13500) in all fits. Age is als relevant (chi2&gt;3000), and a square term is not needed (chi2 = 2); the interaction adds a little bit (chi2 95). With these large numbers (1.1M patients), most effects have p&lt;.0001. We will evaluate the differences between fits with or without interaction term graphically further down # Look for model improvements anova(fitplot2) # linear age effect, no interaction with surgery ## Wald Statistics Response: mort ## ## Factor Chi-Square d.f. P ## surgery 13500.19 13 &lt;.0001 ## age 3167.14 1 &lt;.0001 ## TOTAL 16445.99 14 &lt;.0001 anova(fitage2) # age square added ## Wald Statistics Response: mort ## ## Factor Chi-Square d.f. P ## surgery 13499.66 13 &lt;.0001 ## age10 18.13 1 &lt;.0001 ## age102 2.33 1 0.127 ## TOTAL 16424.97 15 &lt;.0001 anova(fitplot) # interaction added to linear age effect ## Wald Statistics Response: mort ## ## Factor Chi-Square d.f. P ## surgery (Factor+Higher Order Factors) 13566.02 26 &lt;.0001 ## All Interactions 94.55 13 &lt;.0001 ## age (Factor+Higher Order Factors) 3280.66 14 &lt;.0001 ## All Interactions 94.55 13 &lt;.0001 ## surgery * age (Factor+Higher Order Factors) 94.55 13 &lt;.0001 ## TOTAL 16620.27 27 &lt;.0001 6.1.6 Plotting of predicted age effects, with interaction by type of surgery; add 95% CI Plot age effects at logodds scale with 95% CI 6.1.7 Plotting of age effects with original data points Fit with interaction (solid lines) and no interaction (dashed lines) "],["missing.html", "7 Missing values", " 7 Missing values Chapter 7 additional material upcoming. "],["missing-case.html", "8 Case Study on Dealing with Missing Values", " 8 Case Study on Dealing with Missing Values Chapter 8 additional material upcoming. "],["coding.html", "9 Coding of Categorical and Continuous Predictors 9.1 Figues 9.1 to 9.6", " 9 Coding of Categorical and Continuous Predictors 9.1 Figues 9.1 to 9.6 GUSTO-I is a data set with patients suffering from an acute myocardial infarction, where we want to predict 30-day mortality. TBI is a data set with patients suffering from a moderate or severe traumatic brain injury. # Import gusto gusto &lt;- read.csv(&quot;data/gusto_age_STE.csv&quot;)[, -1] # Import sample4; n=785 gustos &lt;- read.csv(&quot;data/Gustos4Age.csv&quot;) # Import TBI data; n=2159 TBI &lt;- read.csv(&quot;data/TBI2vars.csv&quot;) 9.1.1 Fig 9.1: Age linear; add square; rcs in GUSTO-I agegusto.linear &lt;- lrm(DAY30 ~ AGE, data = gusto, x = T, y = T) agegusto.square &lt;- lrm(DAY30 ~ pol(AGE, 2), data = gusto, x = T, y = T) agegusto.rcs &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gusto, x = T, y = T) ## dichotomize agegusto.cat65 &lt;- lrm(DAY30 ~ ifelse(AGE &lt; 65, 0, 1), data = gusto, x = T, y = T) ## 3 categories agegusto.3cat &lt;- lrm(DAY30 ~ ifelse(AGE &lt; 60, 0, ifelse(AGE &lt; 70, 1, 2)), data = gusto, x = T, y = T) # Predict for age 20:95 newdata.age &lt;- data.frame(&quot;AGE&quot; = seq(20, 95, by = 0.1)) pred.agegusto.linear &lt;- predict(agegusto.linear, newdata.age) pred.agegusto.square &lt;- predict(agegusto.square, newdata.age) pred.agegusto.rcs &lt;- predict(agegusto.rcs, newdata.age) pred.agegusto.cat65 &lt;- predict(agegusto.cat65, newdata.age) pred.agegusto.3cat &lt;- predict(agegusto.3cat, newdata.age) # Make plot dd &lt;- datadist(gusto) options(datadist = &quot;dd&quot;) # for rms par(mfrow = c(1, 1)) plot( x = newdata.age[, 1], y = pred.agegusto.linear, xlim = c(20, 92), ylim = c(-7, 0), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Age in years&quot;, ylab = &quot;logit of 30-day mortality&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) axis(1, at = c(40, 50, 60, 65, 70, 80, 90)) lines(x = newdata.age[, 1], y = pred.agegusto.cat65, lty = 2, lwd = 2, col = mycolors[3]) lines(x = newdata.age[, 1], y = pred.agegusto.3cat, lty = 3, lwd = 2, col = mycolors[4]) scat1d(x = gusto$AGE, side = 1, frac = .05, col = &quot;darkblue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;linear&quot;, &quot;&lt;65 vs &gt;=65&quot;, &quot;&lt;60, 60-69, 70+&quot;), lty = c(1, 2, 3), lwd = 2, cex = 1, bty = &quot;n&quot;, col = mycolors[2:4] ) 9.1.2 Fig 9.2: Impact of number of ST elevations (STE) # winsorize at STE 10 gusto$STE &lt;- ifelse(gusto$STE &gt; 10, 10, gusto$STE) gusto$STE.f &lt;- as.factor(gusto$STE) STE.linear &lt;- lrm(DAY30 ~ STE, data = gusto, x = T, y = T) STE.square &lt;- lrm(DAY30 ~ pol(STE, 2), data = gusto, x = T, y = T) STE.rcs &lt;- lrm(DAY30 ~ rcs(STE, 5), data = gusto, x = T, y = T) STE.factor &lt;- lrm(DAY30 ~ STE.f, data = gusto, x = T, y = T) # dichotomize STE.cat4 &lt;- lrm(DAY30 ~ ifelse(STE &lt; 5, 0, 1), data = gusto, x = T, y = T) # predict for STE 0:10 newdata.STE &lt;- data.frame(&quot;STE&quot; = seq(0, 10, by = 1)) pred.STE.linear &lt;- predict(STE.linear, newdata.STE) pred.STE.square &lt;- predict(STE.square, newdata.STE) pred.STE.rcs &lt;- predict(STE.rcs, newdata.STE) pred.STE.cat4 &lt;- predict(STE.cat4, newdata.STE) par(mfrow = c(1, 1)) plot( x = newdata.STE[, 1], y = pred.STE.linear, las = 1, xlab = &quot;Number of leads with ST elevation&quot;, ylab = &quot;logit of 30-day mortality&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) lines(x = newdata.STE[, 1], y = pred.STE.square, lty = 2, lwd = 2, col = mycolors[3]) lines(x = newdata.STE[, 1], y = pred.STE.rcs, lty = 3, lwd = 3, col = mycolors[4]) lines(x = newdata.STE[1:5, 1], y = pred.STE.cat4[1:5], lty = 4, lwd = 2, col = mycolors[5]) lines(x = newdata.STE[6:11, 1], y = pred.STE.cat4[6:11], lty = 4, lwd = 2, col = mycolors[5]) # Original data points, with size proportional to sqrt(events) STEmort &lt;- log(by(gusto$DAY, gusto$STE, mean) / (1 - by(gusto$DAY, gusto$STE, mean)))[1:11] STEw &lt;- sqrt(by(gusto$DAY, gusto$STE, sum)[1:11]) / 10 points(x = newdata.STE[, 1], y = STEmort, pch = 1, lwd = 2, cex = STEw, col = &quot;black&quot;) scat1d(x = gusto$STE, side = 1, frac = .05, col = &quot;darkblue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;data&quot;, &quot;linear&quot;, &quot;.. +square&quot;, &quot;rcs&quot;, &quot;&lt;=4, vs &gt;4&quot;), lty = c(NA, 1, 2, 3, 4), pch = c(1, NA, NA, NA, NA), lwd = 2, cex = 1, bty = &quot;n&quot;, col = mycolors[1:5] ) 9.1.3 Fig 9.3: Non-linearities in small sample (n=751); and full GUSTO-I (n=40,830) # Examine non-linearities in gustos sample # Age age.linear &lt;- lrm(DAY30 ~ AGE, data = gustos, x = T, y = T, linear.predictors = F) age.rcs1 &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gustos, x = T, y = T, linear.predictors = F) age.fp1 &lt;- mfp(DAY30 ~ fp(AGE, df = 4), alpha = 1, data = gustos, family = binomial) # selected: -2 and 3 age.gam1 &lt;- gam(DAY30 ~ s(AGE), data = gustos, family = binomial) # examine predictions for age 20:95 age.mat &lt;- matrix(nrow = 751, ncol = 5) names(age.mat) &lt;- list(NULL, Cs(AGE, linear, fp, rcs, gam)) AGE &lt;- seq(20, 95, by = 0.1) age.mat[, 1] &lt;- AGE age.mat[, 2] &lt;- predict(age.linear, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat[, 3] &lt;- predict(age.fp1, newdata = as.data.frame(x = AGE)) age.mat[, 4] &lt;- predict(age.rcs1, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat[, 5] &lt;- predict(age.gam1, newdata = as.data.frame(x = AGE)) # Plot for n=785, Fig 9.3, part A # par(mfrow = c(1, 2)) plot( x = age.mat[, 1], y = age.mat[, 2], xlim = c(20, 92), ylim = c(-8, 0.2), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Age in years&quot;, ylab = &quot;logit of 30-day mortality&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) axis(1, at = c(30, 40, 50, 60, 70, 80, 90)) lines(x = age.mat[, 1], y = age.mat[, 3], lty = 2, lwd = 2, col = mycolors[3]) lines(x = age.mat[, 1], y = age.mat[, 4], lty = 3, lwd = 3, col = mycolors[4]) lines(x = age.mat[, 1], y = age.mat[, 5], lty = 4, lwd = 3, col = mycolors[5]) histSpike(x = gustos$AGE, side = 1, frac = .1, col = &quot;darkblue&quot;, add = T) legend(&quot;topleft&quot;, legend = c(&quot;linear&quot;, &quot;fp&quot;, &quot;rcs&quot;, &quot;gam&quot;), lty = 1:4, lwd = 2, cex = 1.2, bty = &quot;n&quot;, col = mycolors[2:5] ) title(&quot;GUSTO-I, n=785&quot;) # End plot n=785 #################### ## Now: N=40,830 ## age.linear.2 &lt;- lrm(DAY30 ~ AGE, data = gusto, x = T, y = T, linear.predictors = F) age.rcs1.2 &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gusto, x = T, y = T, linear.predictors = F) age.fp1.2 &lt;- mfp(DAY30 ~ fp(AGE, df = 4), alpha = 1, data = gusto, family = binomial) # selected: -2 and 3 age.gam1.2 &lt;- gam(DAY30 ~ s(AGE), data = gusto, family = binomial) # examine predictions for age 20:95 age.mat.2 &lt;- matrix(nrow = 751, ncol = 5) names(age.mat.2) &lt;- list(NULL, Cs(AGE, linear, fp, rcs, gam)) age.mat.2[, 1] &lt;- AGE age.mat.2[, 2] &lt;- predict(age.linear.2, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat.2[, 3] &lt;- predict(age.fp1.2, newdata = as.data.frame(x = AGE)) age.mat.2[, 4] &lt;- predict(age.rcs1.2, newdata = as.data.frame(x = AGE), type = &quot;lp&quot;) age.mat.2[, 5] &lt;- predict(age.gam1.2, newdata = as.data.frame(x = AGE)) # Plot for n=40830 plot( x = age.mat.2[, 1], y = age.mat.2[, 2], xlim = c(20, 92), ylim = c(-8, 0.2), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Age in years&quot;, ylab = &quot;&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[2] ) axis(1, at = c(30, 40, 50, 60, 70, 80, 90)) lines(x = age.mat.2[, 1], y = age.mat.2[, 3], lty = 2, lwd = 2, col = mycolors[3]) lines(x = age.mat.2[, 1], y = age.mat.2[, 4], lty = 3, lwd = 3, col = mycolors[4]) lines(x = age.mat.2[, 1], y = age.mat.2[, 5], lty = 4, lwd = 3, col = mycolors[5]) histSpike(x = gusto$AGE, side = 1, frac = .2, col = &quot;darkblue&quot;, add = T) title(&quot;GUSTO-I, n=40,830&quot;) 9.1.4 Fig 9.4: Glucose and hb in TBI We evaluate the predictors ‘glucose’ and ‘hemoglobin’ in TBI. Both are first truncated, after inspecting boxplots (Fig 9.4), and then analyzed for their prognostic value, with plots for illustration of the estimated relations (Fig 9.5). # glucose quantile(TBI$glucose, probs = c(.005, .01, .02, .98, .99, .995), na.rm = T) ## 0.5% 1% 2% 98% 99% 99.5% ## 1.57 2.26 4.28 18.34 20.92 23.39 # 0.5% 1% 2% 98% 99% 99.5% # 1.57 2.26 4.28 18.34 20.92 23.39 # So, winsorize / truncate at 3 and 20 # use simple function to do the winsorizing: # {ifelse(x&lt;lower,upper, ifelse(x&gt;upper,upper, x))} winsorize &lt;- function(x, lower = quantile(x, probs = 0.01), upper = quantile(x, probs = 0.99)) { ifelse(x &lt; lower, lower, ifelse(x &gt; upper, upper, x) ) } TBI$glucoset &lt;- winsorize(TBI$glucose, 3, 20) # systolic bp quantile(TBI$hb, probs = c(.005, .01, .02, .98, .99, .995), na.rm = T) ## 0.5% 1% 2% 98% 99% 99.5% ## 4.87 5.40 6.30 16.50 16.80 17.00 # 0.5% 1% 2% 98% 99% 99.5% # 4.87 5.40 6.30 16.50 16.80 17.00 # So, winsorize / truncate at 6 and 17 TBI$hbt &lt;- winsorize(TBI$hb, 6, 17) # boxplots for illustration par(mfrow = c(1, 2)) boxplot(x = cbind(TBI$glucose, TBI$glucoset), outcol = c(&quot;red&quot;, mycolors[4]), border = mycolors[c(10, 4)], xaxt = &quot;n&quot;, ylab = &quot;glucose (mmol/L)&quot;) axis(1, at = c(1, 2), labels = c(&quot;before &quot;, &quot;after winsorizing&quot;)) title(&quot;Glucose&quot;) boxplot(x = cbind(TBI$hb, TBI$hbt), outcol = c(&quot;red&quot;, mycolors[4]), border = mycolors[c(10, 4)], xaxt = &quot;n&quot;, ylab = &quot;hb (mmol/L)&quot;) axis(1, at = c(1, 2), labels = c(&quot;before &quot;, &quot;after winsorizing&quot;)) title(&quot;Hemoglobin&quot;) 9.1.5 Fig 9.5: Non-linear association of glucose 9.1.6 Fig 9.6: Systolic blood pressure in TBI We now examine the prognostic value of systolic blood pressure (BP) in TBI patients. We expect low BP (hypotension) to be especially risky. quantile(TBI$d.sysbpt, probs = c(.01, .25, .75, .99), na.rm = T) ## 1% 25% 75% 99% ## 92.9 121.1 142.1 171.8 TBIs &lt;- TBI[!is.na(TBI$d.sysbpt), ] # 1% 25% 75% 99% # 92.9 121.1 142.1 171.8 g1 &lt;- lrm(d.gos &lt; 2 ~ rcs(d.sysbpt, 3), data = TBIs) g2 &lt;- lrm(d.gos &lt; 3 ~ rcs(d.sysbpt, 3), data = TBIs) g3 &lt;- lrm(d.gos &lt; 4 ~ rcs(d.sysbpt, 3), data = TBIs) g4 &lt;- lrm(d.gos &lt; 5 ~ rcs(d.sysbpt, 3), data = TBIs) # Define categorical variants of systolic BP TBIs$d.sysbpt.c &lt;- as.factor(ifelse(TBIs$d.sysbpt &lt; 120, 1, ifelse(TBIs$d.sysbpt &gt; 150, 2, 0))) g1t &lt;- lrm(d.gos &lt; 2 ~ d.sysbpt.c, data = TBIs) g2t &lt;- lrm(d.gos &lt; 3 ~ d.sysbpt.c, data = TBIs) g3t &lt;- lrm(d.gos &lt; 4 ~ d.sysbpt.c, data = TBIs) g4t &lt;- lrm(d.gos &lt; 5 ~ d.sysbpt.c, data = TBIs) dd &lt;- datadist(TBIs) options(datadist = &quot;dd&quot;) # Odds ratios exp(coef(g1t)) # OR 2.78 for low BP and 1.25 for high BP ## Intercept d.sysbpt.c=1 d.sysbpt.c=2 ## 0.226 2.777 1.245 describe(TBI$d.sysbpt) ## TBI$d.sysbpt ## n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 ## 2159 0 1566 1 131.4 17.61 107.0 112.2 121.1 131.3 142.1 ## .90 .95 ## 151.1 156.2 ## ## lowest : 60.0 64.2 65.6 71.3 73.7, highest: 182.9 184.2 184.7 188.3 207.4 describe(TBIs$d.sysbpt.c) ## TBIs$d.sysbpt.c ## n missing distinct ## 2159 0 3 ## ## Value 0 1 2 ## Frequency 1435 474 250 ## Proportion 0.665 0.220 0.116 # data in matrix for plot d.sysbpt &lt;- seq(60, 210, by = 1) # 151 elements g.mat &lt;- matrix(nrow = 151, ncol = 5) names(g.mat) &lt;- list(NULL, Cs(d.sysbpt, g1, g2, g3, g4)) g.mat[, 1] &lt;- d.sysbpt g.mat[, 2] &lt;- predict(g1, newdata = as.data.frame(x = d.sysbpt)) g.mat[, 3] &lt;- predict(g2, newdata = as.data.frame(x = d.sysbpt)) g.mat[, 4] &lt;- predict(g3, newdata = as.data.frame(x = d.sysbpt)) g.mat[, 5] &lt;- predict(g4, newdata = as.data.frame(x = d.sysbpt)) # Plot: Fig 9.6, part I par(mfrow = c(1, 2)) plot( x = g.mat[, 1], y = g.mat[, 2], xlim = c(50, 210), ylim = c(-2.7, 2.7), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Systolic blood pressure (mmHg)&quot;, ylab = &quot;logit GOS at 6 months&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[1] ) axis(1, at = c(100, 150, 200)) lines(x = g.mat[, 1], y = g.mat[, 3], lty = 2, lwd = 2, col = mycolors[2]) lines(x = g.mat[, 1], y = g.mat[, 4], lty = 3, lwd = 3, col = mycolors[3]) lines(x = g.mat[, 1], y = g.mat[, 5], lty = 4, lwd = 3, col = mycolors[4]) histSpike(x = TBI$d.sysbpt, side = 3, frac = .2, col = &quot;darkblue&quot;, add = T) legend(&quot;bottomleft&quot;, legend = c(&quot;&lt;good recovery&quot;, &quot;unfavorable&quot;, &quot;mort+veg&quot;, &quot;mortality&quot;), lty = 4:1, lwd = 2, cex = 0.8, bty = &quot;n&quot;, col = mycolors[4:1]) title(&quot;continuous, rcs 3 knots&quot;) ## Plot 9.6, part II d.sysbpt.c &lt;- c(0, 1, 2) d.sysbpt &lt;- seq(60, 210, by = .1) # 1501 elements g.mat2 &lt;- matrix(nrow = 1501, ncol = 5) g1t.c &lt;- predict(g1t, newdata = as.data.frame(x = d.sysbpt.c)) g2t.c &lt;- predict(g2t, newdata = as.data.frame(x = d.sysbpt.c)) g3t.c &lt;- predict(g3t, newdata = as.data.frame(x = d.sysbpt.c)) g4t.c &lt;- predict(g4t, newdata = as.data.frame(x = d.sysbpt.c)) names(g.mat2) &lt;- list(NULL, Cs(d.sysbpt, g1, g2, g3, g4)) g.mat2[, 1] &lt;- d.sysbpt g.mat2[, 2] &lt;- ifelse(d.sysbpt &lt; 120, g1t.c[2], ifelse(d.sysbpt &gt; 150, g1t.c[3], g1t.c[1])) g.mat2[, 3] &lt;- ifelse(d.sysbpt &lt; 120, g2t.c[2], ifelse(d.sysbpt &gt; 150, g2t.c[3], g2t.c[1])) g.mat2[, 4] &lt;- ifelse(d.sysbpt &lt; 120, g3t.c[2], ifelse(d.sysbpt &gt; 150, g3t.c[3], g3t.c[1])) g.mat2[, 5] &lt;- ifelse(d.sysbpt &lt; 120, g4t.c[2], ifelse(d.sysbpt &gt; 150, g4t.c[3], g4t.c[1])) # Plot plot( x = g.mat2[, 1], y = g.mat2[, 2], xlim = c(50, 210), ylim = c(-2.7, 2.7), las = 1, xaxt = &quot;n&quot;, xlab = &quot;Systolic blood pressure (mmHg)&quot;, ylab = &quot;&quot;, cex.lab = 1.2, type = &quot;l&quot;, lwd = 2, col = mycolors[1] ) axis(1, at = c(100, 150, 200)) lines(x = g.mat2[, 1], y = g.mat2[, 3], lty = 2, lwd = 2, col = mycolors[2]) lines(x = g.mat2[, 1], y = g.mat2[, 4], lty = 3, lwd = 3, col = mycolors[3]) lines(x = g.mat2[, 1], y = g.mat2[, 5], lty = 4, lwd = 3, col = mycolors[4]) histSpike(x = TBI$d.sysbpt, side = 3, frac = .2, col = &quot;darkblue&quot;, add = T) title(&quot;3 categories&quot;) "],["restrictions.html", "10 Restrictions on Candidate Predictors", " 10 Restrictions on Candidate Predictors Chapter 10 additional material upcoming. "],["selection.html", "11 Selection of Main Effects", " 11 Selection of Main Effects Chapter 11 additional material upcoming. "],["additivity-linearity.html", "12 Assumptions in Regression Models - Additivity and Linearity 12.1 GUSTO-I interaction analysis 12.2 MFP and other non-linear analyses in n544 data", " 12 Assumptions in Regression Models - Additivity and Linearity 12.1 GUSTO-I interaction analysis 12.1.1 Examine interactions # Import gusto, gustoB, and sample4 data sets gusto &lt;- read.csv(&quot;data/gusto1.csv&quot;) # GUSTO sample with 40830 patients gustoB &lt;- read.csv(&quot;data/gustoB.csv&quot;) # GUSTO part B sample with 20318 patients gustos &lt;- read.csv(&quot;data/sample4.csv&quot;) # GUSTO sample4 with 785 patients source(&quot;R/auc.nonpara.mw.R&quot;) source(&quot;R/ci.auc.R&quot;) source(&quot;R/val.prob.ci.2.R&quot;) # levels(gustos$HRT) &lt;- c(&quot;No tachycardia&quot;, &quot;Tachycardia&quot;) dd &lt;- datadist(gustos) options(datadist = &quot;dd&quot;) Evaluate interactions with age in a full model, which includes 8 predictors in total. The data set is small (sample4, n=785, 52 events) ### Full model and age interactions full &lt;- lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) fullint &lt;- lrm(DAY30 ~ AGE * (KILLIP + HIG + DIA + HYP + HRT + TTR + SEX), data = gustos, x = T, y = T, linear.predictors = F) anova(fullint) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE (Factor+Higher Order Factors) 29.93 8 0.0002 ## All Interactions 11.01 7 0.1380 ## KILLIP (Factor+Higher Order Factors) 4.96 2 0.0839 ## All Interactions 1.01 1 0.3160 ## HIG (Factor+Higher Order Factors) 6.32 2 0.0424 ## All Interactions 2.20 1 0.1380 ## DIA (Factor+Higher Order Factors) 3.70 2 0.1571 ## All Interactions 0.64 1 0.4220 ## HYP (Factor+Higher Order Factors) 5.62 2 0.0603 ## All Interactions 1.46 1 0.2267 ## HRT (Factor+Higher Order Factors) 13.50 2 0.0012 ## All Interactions 4.49 1 0.0341 ## TTR (Factor+Higher Order Factors) 8.31 2 0.0157 ## All Interactions 2.80 1 0.0942 ## SEX (Factor+Higher Order Factors) 0.79 2 0.6736 ## All Interactions 0.42 1 0.5151 ## AGE * KILLIP (Factor+Higher Order Factors) 1.01 1 0.3160 ## AGE * HIG (Factor+Higher Order Factors) 2.20 1 0.1380 ## AGE * DIA (Factor+Higher Order Factors) 0.64 1 0.4220 ## AGE * HYP (Factor+Higher Order Factors) 1.46 1 0.2267 ## AGE * HRT (Factor+Higher Order Factors) 4.49 1 0.0341 ## AGE * TTR (Factor+Higher Order Factors) 2.80 1 0.0942 ## AGE * SEX (Factor+Higher Order Factors) 0.42 1 0.5151 ## TOTAL INTERACTION 11.01 7 0.1380 ## TOTAL 64.59 15 &lt;.0001 ### Select only interaction AGE * HRT fullints &lt;- lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = T) anova(fullints) ## Wald Statistics Response: DAY30 ## ## Factor Chi-Square d.f. P ## AGE (Factor+Higher Order Factors) 26.67 2 &lt;.0001 ## All Interactions 2.70 1 0.1004 ## KILLIP 3.52 1 0.0605 ## HIG 5.38 1 0.0204 ## DIA 3.14 1 0.0763 ## HYP 3.49 1 0.0618 ## HRT (Factor+Higher Order Factors) 11.67 2 0.0029 ## All Interactions 2.70 1 0.1004 ## TTR 5.66 1 0.0174 ## SEX 0.19 1 0.6602 ## AGE * HRT (Factor+Higher Order Factors) 2.70 1 0.1004 ## TOTAL 65.30 9 &lt;.0001 12.1.2 Fig 12.1 Make 2 plots with linear interaction, in the small n=785 sample, and in the full n=40830 sample 12.1.3 Fig 12.2 Make 4 plots with main effects, linear interaction, and 2 variants of interaction only above age 55. The variable is (Age-55)[+]. In the last graph, the green dotted line follows the angle from below age 55 years (only 1 Age effect is estimated for the HRT==0 and HRT==1 and age&lt;55 patients). In the pre-final graph, there are 2 separate angles from age 55 for HRT==0 and HRT==1 (barely noticable for the green dotted line). 12.1.4 Smart coding illustration Smart coding of age effect: separate for no HRT (HRT==0) and for HRT (HRT==1) # Smart coding of age effect: separate for no HRT (HRT==0) and for HRT (HRT==1) gustos$AGE0 &lt;- gustos$AGE * (1 - gustos$HRT) gustos$AGE1 &lt;- gustos$AGE * gustos$HRT # Standard lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = F) ## Logistic Regression Model ## ## lrm(formula = DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + ## TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = F) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 785 LR chi2 86.28 R2 0.270 C 0.831 ## 0 733 d.f. 9 R2(9,785)0.094 Dxy 0.662 ## 1 52 Pr(&gt; chi2) &lt;0.0001 R2(9,145.7)0.412 gamma 0.662 ## max |deriv| 1e-09 Brier 0.051 tau-a 0.082 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -9.7431 1.6574 -5.88 &lt;0.0001 ## AGE 0.0759 0.0240 3.16 0.0016 ## KILLIP 0.4595 0.2448 1.88 0.0605 ## HIG 0.7900 0.3407 2.32 0.0204 ## DIA 0.7804 0.4403 1.77 0.0763 ## HYP 1.0403 0.5570 1.87 0.0618 ## HRT -3.6376 2.8352 -1.28 0.1995 ## TTR 0.8201 0.3447 2.38 0.0174 ## SEX -0.1534 0.3490 -0.44 0.6602 ## AGE * HRT 0.0655 0.0399 1.64 0.1004 ## # Smart coding lrm(DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## Logistic Regression Model ## ## lrm(formula = DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + ## HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 785 LR chi2 86.28 R2 0.270 C 0.831 ## 0 733 d.f. 9 R2(9,785)0.094 Dxy 0.662 ## 1 52 Pr(&gt; chi2) &lt;0.0001 R2(9,145.7)0.412 gamma 0.662 ## max |deriv| 1e-09 Brier 0.051 tau-a 0.082 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -9.7431 1.6574 -5.88 &lt;0.0001 ## AGE0 0.0759 0.0240 3.16 0.0016 ## AGE1 0.1414 0.0332 4.26 &lt;0.0001 ## HRT -3.6376 2.8352 -1.28 0.1995 ## KILLIP 0.4595 0.2448 1.88 0.0605 ## HIG 0.7900 0.3407 2.32 0.0204 ## DIA 0.7804 0.4403 1.77 0.0763 ## HYP 1.0403 0.5570 1.87 0.0618 ## TTR 0.8201 0.3447 2.38 0.0174 ## SEX -0.1534 0.3490 -0.44 0.6602 ## # Identical fit, easier interpretation # Age 55 as reference for HRT effect gustos$AGE0 &lt;- (gustos$AGE - 55) * (1 - gustos$HRT) gustos$AGE1 &lt;- (gustos$AGE - 55) * gustos$HRT lrm(DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## Logistic Regression Model ## ## lrm(formula = DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + ## HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 785 LR chi2 86.28 R2 0.270 C 0.831 ## 0 733 d.f. 9 R2(9,785)0.094 Dxy 0.662 ## 1 52 Pr(&gt; chi2) &lt;0.0001 R2(9,145.7)0.412 gamma 0.662 ## max |deriv| 4e-11 Brier 0.051 tau-a 0.082 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -5.5703 0.5746 -9.69 &lt;0.0001 ## AGE0 0.0759 0.0240 3.16 0.0016 ## AGE1 0.1414 0.0332 4.26 &lt;0.0001 ## HRT -0.0333 0.7043 -0.05 0.9623 ## KILLIP 0.4595 0.2448 1.88 0.0605 ## HIG 0.7900 0.3407 2.32 0.0204 ## DIA 0.7804 0.4403 1.77 0.0763 ## HYP 1.0403 0.5570 1.87 0.0618 ## TTR 0.8201 0.3447 2.38 0.0174 ## SEX -0.1534 0.3490 -0.44 0.6602 ## # Even nicer interpretation, HRT effect for age=55 The fit of each of the models is identical (always LR chi2=86.28); each model allows for linear interaction between AGE and HRT. The interpretation of the model is easier if the age effects are estimated for HRT==1 and for HRT==0. Scaling is easier by subtracting 55 from AGE (AGE-55); this implies the HRT effect relates to age 55. 12.1.5 Table 12.2 Better predictions? Assess the performance if the models created in n=785 in an independent validation part, GustoB, n=20318. Plots created with a modification of Frank Harrell’s val.prob() function: # Validate in independent part, named gustoB # main effects lrm.val.full &lt;- predict(full, newdata = gustoB, type = &quot;lp&quot;) # simple interaction lrm.val.int1 &lt;- predict(fullints, newdata = gustoB, type = &quot;lp&quot;) # Plot val.prob.ci.2( y = gustoB[, &quot;DAY30&quot;], logit = lrm.val.full, riskdist = &quot;predicted&quot;, logistic.cal = F, smooth = &quot;rcs&quot;, nr.knots = 3, g = 8, xlim = c(0, .5), ylim = c(0, .5), legendloc = c(0.18, 0.15), statloc = c(0, .4), roundstats = 3, xlab = &quot;Predicted probability from n=785&quot;, ylab = &quot;Observed proportion in n=20318&quot; ) Conclusions Discrimination: worse with interactions than without. Calibration: We note some overfitting, as expected by a fit in a small sample. 12.2 MFP and other non-linear analyses in n544 data Upcoming. "],["estimation.html", "13 Modern Estimation Methods", " 13 Modern Estimation Methods Chapter 13 additional material upcoming. "],["estimation-external.html", "14 Estimation with External Information Learning from external information The data: TBI (n=11022) and AAA (n=238) data sets The impact study The AAA study Conclusions", " 14 Estimation with External Information Learning from external information 14.0.1 A local model with external information: Table 14.1 We can use external information to optimize the development of a prediction model with local or global applicability. Table 14.1 provides the basic idea of model development with a focus on a locally applicable model. (#tab:Table14.1)Table 14.1: local vs global models Modeling aspect Local model Global model Model specification Mixture of IPD and literature Focus on consensus in literature Model coefficients IPD with literature as background Meta-analysis of literature Baseline risk IPD Literature The data: TBI (n=11022) and AAA (n=238) data sets The impact data set includes patients with moderate / severe TBI for 15 studies (Steyerberg 2019). We can learn from the combined information in an Individual Patient Data Meta-Analysis (IPD MA). The AAA data set includes a small set of patients undergoing elective surgery for Abdominal Aortic Aneurysm (Steyerberg 1995). We can learn from univariate information in the literature where many studies are available (Debray 2012). Specifically, we borrow information from the univariate coefficients in other studies and perform an adaptation of the multivariable coefficients in the AAA data set. The resulting model is stabilized such that better local and global performance is expected. # impact data come with metamisc package Debray; read here data(&quot;impact&quot;, package = &quot;metamisc&quot;) impact$name &lt;- as.factor(impact$name) impact$ct &lt;- as.factor(impact$ct) impact$age10 &lt;- impact$age/10 - 3.5 # reference 35-year-old patient, close to mean age impact$motor_score &lt;- as.factor(impact$motor_score) impact$pupil &lt;- as.factor(impact$pupil) levels(impact$ct) &lt;- c(&quot;I/II&quot;, &quot;III/IV&quot;, &quot;V/VI&quot;) # correct an error in Debray labels impact$motor.lin &lt;- as.numeric(impact$motor_score) # 1/2, 3, 4, 5/6 linear impact$pupil.lin &lt;- as.numeric(impact$pupil) # 1, 2, 3 linear impact$study &lt;- as.factor(ifelse(impact$name==&quot;TINT&quot;, 1, ifelse(impact$name==&quot;TIUS&quot;, 2, ifelse(impact$name==&quot;SLIN&quot;, 3, ifelse(impact$name==&quot;SAP&quot;, 4, ifelse(impact$name==&quot;PEG&quot;, 5, ifelse(impact$name==&quot;HIT I&quot;, 6, ifelse(impact$name==&quot;UK4&quot;, 7, ifelse(impact$name==&quot;TCDB&quot;, 8, ifelse(impact$name==&quot;SKB&quot;, 9, ifelse(impact$name==&quot;EBIC&quot;, 10, ifelse(impact$name==&quot;HIT II&quot;, 11, ifelse(impact$name==&quot;NABIS&quot;, 12, ifelse(impact$name==&quot;CSTAT&quot;, 13, ifelse(impact$name==&quot;PHARMOS&quot;, 14, ifelse(impact$name==&quot;APOE&quot;, 15,NA)))))))))))))))) names &lt;- levels(impact[,1]) AAA &lt;- read.csv(&quot;data/AAA.csv&quot;, row.names = 1) The impact study 14.0.2 Descriptives Overall results are presented below. We note that missing values were imputed (using mice, a single imputation for simple illustrations). html(describe(impact), scroll=TRUE) .earrows {color:silver;font-size:11px;} fcap { font-family: Verdana; font-size: 12px; color: MidnightBlue } smg { font-family: Verdana; font-size: 10px; color: &#808080; } hr.thinhr { margin-top: 0.15em; margin-bottom: 0.15em; } span.xscript { position: relative; } span.xscript sub { position: absolute; left: 0.1em; bottom: -1ex; } impact Descriptives impact 15 Variables 11022 Observations name .hmisctable329073 { border: none; font-size: 85%; } .hmisctable329073 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable329073 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 11022015 lowest : APOE CSTAT EBIC HIT I HIT II , highest: SLIN TCDB TINT TIUS UK4 Value APOE CSTAT EBIC HIT I HIT II NABIS PEG PHARMOS SAP Frequency 756 517 822 350 819 385 1510 856 919 Proportion 0.069 0.047 0.075 0.032 0.074 0.035 0.137 0.078 0.083 Value SKB SLIN TCDB TINT TIUS UK4 Frequency 126 409 603 1118 1041 791 Proportion 0.011 0.037 0.055 0.101 0.094 0.072 type .hmisctable790984 { border: none; font-size: 85%; } .hmisctable790984 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable790984 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 1102202 Value OBS RCT Frequency 2972 8050 Proportion 0.27 0.73 age .hmisctable657861 { border: none; font-size: 85%; } .hmisctable657861 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable657861 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 110220800.99934.9317.5817182231465965 lowest : 14 15 16 17 18 , highest: 89 90 91 92 93 motor_score .hmisctable328706 { border: none; font-size: 85%; } .hmisctable328706 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable328706 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 1102204 Value 1/2 3 4 5/6 Frequency 2850 2285 2438 3449 Proportion 0.259 0.207 0.221 0.313 pupil .hmisctable209497 { border: none; font-size: 85%; } .hmisctable209497 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable209497 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 1102203 Value Both None One Frequency 7325 2296 1401 Proportion 0.665 0.208 0.127 ct .hmisctable621346 { border: none; font-size: 85%; } .hmisctable621346 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable621346 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 1102203 Value I/II III/IV V/VI Frequency 4479 2239 4304 Proportion 0.406 0.203 0.390 hypox .hmisctable190644 { border: none; font-size: 85%; } .hmisctable190644 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable190644 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 11022020.50723750.21550.3381 hypots .hmisctable662852 { border: none; font-size: 85%; } .hmisctable662852 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable662852 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 11022020.42418750.17010.2824 tsah .hmisctable309158 { border: none; font-size: 85%; } .hmisctable309158 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable309158 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 11022020.74450120.45470.4959 edh .hmisctable325310 { border: none; font-size: 85%; } .hmisctable325310 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable325310 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 11022020.34614640.13280.2304 mort .hmisctable717981 { border: none; font-size: 85%; } .hmisctable717981 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable717981 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 11022020.57828740.26080.3856 age10 .hmisctable684423 { border: none; font-size: 85%; } .hmisctable684423 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable684423 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 110220800.999-0.0069591.758-1.8-1.7-1.3-0.4 1.1 2.4 3.0 lowest : -2.1 -2.0 -1.9 -1.8 -1.7 , highest: 5.4 5.5 5.6 5.7 5.8 motor.lin .hmisctable293231 { border: none; font-size: 85%; } .hmisctable293231 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable293231 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd 11022040.9322.5881.311 Value 1 2 3 4 Frequency 2850 2285 2438 3449 Proportion 0.259 0.207 0.221 0.313 pupil.lin .hmisctable159446 { border: none; font-size: 85%; } .hmisctable159446 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable159446 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd 11022030.6951.4630.6678 Value 1 2 3 Frequency 7325 2296 1401 Proportion 0.665 0.208 0.127 study .hmisctable321651 { border: none; font-size: 85%; } .hmisctable321651 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable321651 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 11022015 lowest : 1 2 3 4 5 , highest: 11 12 13 14 15 Value 1 2 3 4 5 6 7 8 9 10 11 12 Frequency 1118 1041 409 919 1510 350 791 603 126 822 819 385 Proportion 0.101 0.094 0.037 0.083 0.137 0.032 0.072 0.055 0.011 0.075 0.074 0.035 Value 13 14 15 Frequency 517 856 756 Proportion 0.047 0.078 0.069 kable(table(impact$name, impact$mort), row.names = T, caption = &quot;Mortality by study&quot;) (#tab:impact.data.describe)Mortality by study 0 1 APOE 639 117 CSTAT 402 115 EBIC 541 281 HIT I 251 99 HIT II 631 188 NABIS 284 101 PEG 1148 362 PHARMOS 711 145 SAP 707 212 SKB 92 34 SLIN 315 94 TCDB 339 264 TINT 840 278 TIUS 816 225 UK4 432 359 14.0.3 Analyses for Table 14.2 We analyze various model variants for the impact data, with 3 key predictors: age10: age per decade, continuous; centered at age 40 motor.lin: the Motor score component from the Glasgow Coma Scale, continuous for codes 1/2, 3, 4, 5/6 pupil.lin: pupillary reactivity, continuous for codes 1, 2, 3 relating to both, one, or no reacting pupils The models are as follows: A naive analyses of the merged data, ignoring the clustering nature of the data (rms::lrm) Per study analyses for each of the 15 studies (rms::lrm) Stratified analysis, with study as a factor variable to estimate common predictor effects (rms::lrm) One-stage meta-analysis, with study as a random effect to estimate common predictor effects (lme4::glmer) Two-stage univariate meta-analysis, with pooling of the per study estimates obtained in step 2, to obtain random effect estimates for the predictor effects (metamisc::uvmeta) The output includes estimates for the model intercept and the predictor effects. Standard errors (SE) are obtained for each estimate. Moreover, the 1-step and 2-step meta-analyses estimate the heterogeneity parameter tau, which reflects between study differences in the estimates. This heterogeneity is considered in the random effect 95% confidence intervals, and in 95% prediction intervals. # store the results in 2 matrices to produce something close to Table 14.2 coef.matrix &lt;- matrix(nrow=23, ncol=4) se.matrix &lt;- matrix(nrow=23, ncol=4) dimnames(coef.matrix) &lt;- dimnames(se.matrix) &lt;- list(c(levels(impact$study), &quot;naive&quot;, &quot;stratified&quot;, &quot;one-stage&quot;, &quot;tau-1&quot;, &quot;two-stage&quot;, &quot;tau-2&quot;, &quot;Low pred&quot;, &quot;High pred&quot;) ,Cs(Intercept,age,motor,pupils)) # naive merged model fit0 &lt;- lrm(mort~age10+motor.lin+pupil.lin, data=impact) coef.matrix[16,] &lt;- coef(fit0) se.matrix[16,] &lt;- diag(se(fit0)) for (i in 1:15) { # fit stratified models fit.lin &lt;-lrm(mort~age10+motor.lin+pupil.lin, data=impact, subset=impact$study==i) coef.matrix[i,] &lt;- coef(fit.lin) se.matrix[i,] &lt;- diag(se(fit.lin)) } # global model with stratification; intercept is for Study==1 fit.stratified &lt;- lrm(mort~age10+motor.lin+pupil.lin +name, data=impact) coef.matrix[17,2:4] &lt;- coef(fit.stratified)[2:4] se.matrix[17,2:4] &lt;- diag(se(fit.stratified))[2:4] # Estimate heterogeneity in 1 step, global model for covariate effects fit.lin.meta &lt;- glmer(mort~ (1 | name) + age10 + motor.lin+pupil.lin, family =binomial(), data = impact) coef.matrix[18,] &lt;- fit.lin.meta@beta se.matrix[18,] &lt;- sqrt(diag(vcov(fit.lin.meta))) coef.matrix[19,1] &lt;- fit.lin.meta@theta # Estimate heterogeneity in 2 step model for (i in 1:4) { impact.meta &lt;- uvmeta( r=coef.matrix[1:15,i], r.se=se.matrix[1:15,i]) coef.matrix[20,i] &lt;- impact.meta$est se.matrix[20,i] &lt;- impact.meta$se coef.matrix[21,i] &lt;- sqrt(impact.meta$tau) coef.matrix[22,i] &lt;- impact.meta$pi.lb coef.matrix[23,i] &lt;- impact.meta$pi.ub } # end loop over 4 columns of per study estimates # Made Tabe 14.2; SE estimates separate kable(coef.matrix, row.names = T, col.names = NA, caption = &quot;Coefficients and heterogeneity by study&quot;) (#tab:impact.lrm)Coefficients and heterogeneity by study Intercept age motor pupils 1 -0.348 0.275 -0.479 0.380 2 -0.177 0.288 -0.577 0.360 3 -0.916 0.339 -0.367 0.495 4 -1.564 0.260 -0.259 0.616 5 -0.748 0.266 -0.604 0.534 6 -0.892 0.337 -0.515 0.586 7 0.060 0.412 -0.607 0.640 8 0.302 0.485 -0.677 0.629 9 -1.023 0.427 -0.239 0.414 10 -0.432 0.420 -0.690 0.696 11 -0.752 0.335 -0.440 0.277 12 -0.976 0.220 -0.386 0.526 13 -1.382 0.295 -0.456 0.806 14 -1.157 0.255 -0.360 0.298 15 -1.375 0.553 -0.846 1.035 naive -0.720 0.339 -0.512 0.555 stratified NA 0.343 -0.517 0.528 one-stage -0.677 0.342 -0.517 0.530 tau-1 0.356 NA NA NA two-stage -0.735 0.343 -0.507 0.538 tau-2 0.438 0.070 0.121 0.135 Low pred -1.730 0.182 -0.784 0.227 High pred 0.260 0.503 -0.231 0.849 14.0.4 Estimation of coefficients: naive, stratified and IPD-MA We consider the estimates for the multivariable coefficients of 3 predictors: age, motor score, and pupils. Per study differences are modest; a forest plot might visualize the patterns. The model is: lrm(mort~age10+motor.lin+pupil.lin, data=impact, subset=impact$study==i) A naive summary estimate ignores the pooling: lrm(mort~age10+motor.lin+pupil.lin, data=impact) It produces overall estimates that are only slightly different than the estimates from a stratified model (with study as fixed effect). Also, similar estimates come for a one-step random effect analysis (with lrm4::glmer), or a two-step random effect analysis (with metamisc::uvmeta). Overall, the summary effect estimates for the predictors are quite similar. The intercept differences between the studies are more substantial, but the summary estimates of the overall baseline risks are similar, all close to the naive estimate of -0.55. For the stratified analysis, a specific study intercept is taken as the reference, so the overall estimate is not available directly from lrm. The between study variability is quantified with the parameter tau: slot @beta in glmer, and $tau in metamisc::uvmeta. The tau estimates were 0.356 with a one-step random effect analysis (with lrm4::glmer); and 0.438 with a two-step random effect analysis (with metamisc::uvmeta). Note that the latter tau estimate for the intercept depends on the coding of the predictors; it is the heterogeneity for a reference patient with zero values for the covariates, in our case; age 35, poor motor score and both pupils reacting. The prediction interval (from metamisc::uvmeta) was quite wide: \\[-1.7 - .2\\], or odds ranging from exp(-1.73)=0.177 to exp(0.26)=1.3, or mortality risks between plogis(-1.73)=15% to plogis(0.26)=56% for a reference patient with all covariate values set to zero (age 35, poor motor score and both pupils reacting). We can also obtain prediction intervals for the predictor effects with the two-stage approach, which shows wider intervals than the standard 95% confidence intervals (either from fixed or random effect meta-analysis). 14.0.5 Estimation of standard errors The SE estimates (see below) for the predictor effects are very similar with a naive, stratified, or one-stage approach: 0.015 for age, around 0.02 for motor, and 0.03 for pupils. The pooled SEs are substantially smaller than the SE estimates per study. Even for the larger studies, the SEs are substantially wider: around 0.05 for age, around 0.07 for motor, and 0.09 for pupils. These SEs are fixed effect estimates, assuming a single, uniform association of each predictor with the outcome, 6-month mortality. The random effect estimates allow for between study heterogeneity, and are wider: 0.024 for age, around 0.04 for motor, and 0.05 for pupils. For the model intercept, the naive approach underestimates the uncertainty by ignoring the clustering of the data. The one-stage and two-stage meta-analysis approach largely agree (SE = 0.12 - 0.14). kable(se.matrix, row.names = T, col.names = NA, caption = &quot;SE estimates&quot;) (#tab:impact.se.estimates)SE estimates Intercept age motor pupils 1 0.271 0.050 0.071 0.093 2 0.322 0.062 0.076 0.113 3 0.423 0.089 0.119 0.143 4 0.267 0.052 0.070 0.105 5 0.213 0.046 0.061 0.095 6 0.394 0.084 0.121 0.169 7 0.322 0.046 0.081 0.121 8 0.349 0.065 0.089 0.152 9 0.652 0.158 0.197 0.288 10 0.316 0.046 0.082 0.131 11 0.282 0.056 0.080 0.120 12 0.402 0.095 0.109 0.164 13 0.355 0.084 0.107 0.134 14 0.302 0.066 0.091 0.110 15 0.488 0.068 0.134 0.185 naive 0.079 0.015 0.021 0.032 stratified NA 0.015 0.022 0.032 one-stage 0.123 0.015 0.022 0.032 tau-1 NA NA NA NA two-stage 0.142 0.024 0.041 0.050 tau-2 NA NA NA NA Low pred NA NA NA NA High pred NA NA NA NA 14.0.6 Estimation for a specific study, assuming a global model holds We can estimate the intercept for study 14 with fixed estimates from the stratified model: Mortality | Study 14 ~ intercept14 + offset(global linear predictor). # Stratified model, fixed effects fit.stratified &lt;- lrm(mort~age10+motor.lin+pupil.lin +name, data=impact) study14 &lt;- impact[impact$study==14, ] # selected reference set fit.14 &lt;- lrm.fit(y=study14$mort, offset=as.matrix(study14[,c(&quot;age10&quot;, &quot;motor.lin&quot;, &quot;pupil.lin&quot;)]) %*% fit.stratified$coefficients[2:4] ) cat(&quot;Fixed effect estimates for study 14, Tirilazad US (TIUS)\\n&quot;) ## Fixed effect estimates for study 14, Tirilazad US (TIUS) print(c(coef(fit.14), fit.stratified$coefficients[2:4]) ) ## Intercept age10 motor.lin pupil.lin ## -1.179 0.343 -0.517 0.528 The AAA study 14.0.7 Descriptives The AAA data set is rather small, with n=238 patients undergoing elective surgery for an Abdominal Aortic Aneurysm, and only 18 events (STATUS==1). The data set consists of patients operated on at the University Hospital Leiden (the Netherlands) between 1977 and 1988 (Steyerberg 1995). html(describe(AAA), scroll=TRUE) .earrows {color:silver;font-size:11px;} fcap { font-family: Verdana; font-size: 12px; color: MidnightBlue } smg { font-family: Verdana; font-size: 10px; color: &#808080; } hr.thinhr { margin-top: 0.15em; margin-bottom: 0.15em; } span.xscript { position: relative; } span.xscript sub { position: absolute; left: 0.1em; bottom: -1ex; } AAA Descriptives AAA 8 Variables 238 Observations SEX .hmisctable528110 { border: none; font-size: 85%; } .hmisctable528110 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable528110 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 238020.241210.088240.1616 AGE10 .hmisctable649277 { border: none; font-size: 74%; } .hmisctable649277 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable649277 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 2380550.9996.6350.84325.4005.6006.1006.7007.1007.5007.815 lowest : 4.3 4.5 4.9 4.9 5.0 , highest: 7.9 7.9 8.0 8.2 8.4 MI .hmisctable967793 { border: none; font-size: 85%; } .hmisctable967793 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable967793 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 238020.553580.24370.3702 CHF .hmisctable333366 { border: none; font-size: 85%; } .hmisctable333366 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable333366 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 238020.669800.33610.4482 ISCHEMIA .hmisctable225557 { border: none; font-size: 85%; } .hmisctable225557 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable225557 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 238020.681830.34870.4562 LUNG .hmisctable684353 { border: none; font-size: 85%; } .hmisctable684353 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable684353 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 238020.46450.18910.3079 RENAL .hmisctable466833 { border: none; font-size: 85%; } .hmisctable466833 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable466833 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 238020.177150.063030.1186 STATUS .hmisctable346297 { border: none; font-size: 85%; } .hmisctable346297 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable346297 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 238020.21180.075630.1404 14.0.8 Literature, univariate estimates: Table 14.4 We consider the development of a prediction model for in-hospital mortality after elective surgery for AAA. We consider predictors that are identified as relevant in the literature and that are available in the local data. We performed a literature review to obtain univariate estimates of regression coefficients from publications in PubMed between 1980 and 1994. The selection was limited to English-language studies, which had to contain frequency data on the association of a potential predictor and surgical mortality, either in tables or mentioned in the text. The pooled log odds ratio estimates (=logistic regression coefficients) from a fixed effect or random effect procedure. The estimates agreed up to 2 decimals usually. We compare the univariate coefficients from the literature (fixed or random pooled estimates) to the estimates from the local data. the effect for SEX was small, a slightly stronger in the literature (0.36 vs 0.28) the age effect (per decade) was strong. A literature coefficient of 0.79 (slightly smaller than the local 0.98 estimate) implies a exp(0.79)=2.2 times higher odds of mortality per 10 years older. So, as often in surgical procedures, age was a strong predictor (Finlayson 2001) the univariate estimates for cardiac comorbidity (a history of MI, presence of CHF, presence of ISCHEMIA) were all a bit smaller in the literature than the local data LUNG or RENAL comorbidity showed quite similar univariate associations in the literature and the IPD, so using or ignoring the literature would result in similar coefficient estimates The standard errors (SE) were smaller for the literature estimates, as expected. The pooled estimates of the predictors SEX and AGE10 were based on large numbers and many studies, especially for age. This leads to a relatively small fixed effect SE. For other predictors, the number of studies was small leading to a modestly smaller SE, e.g. for cardiac, lung and renal comorbidity. The random effect SE was larger for SEX and AGE10, including the between study heterogeneity in effect estimates. We used the random effect estimates of the coefficients with the random effect SE for the adaptation procedures. # The following estimates come from a univariable logistic regression analysis # Fixed effects, better random effect estimates lit.coefficients.f &lt;- c(log(1.44),log(2.20),log(2.80),log(4.89),log(4.58), log(3.75),log(2.43)) lit.se.f &lt;- c(0.08,0.06,0.27,0.33,0.31,0.25,0.23) # Random effects lit.coefficients &lt;- c(0.3606,0.788,1.034,1.590,1.514,1.302,0.8502) lit.se &lt;- c(0.176,0.112,0.317,0.4109,0.378,0.2595,0.2367) # Local data multivariable estimates full &lt;- lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA,x=T,y=T) # Local data univariate estimates local.coef &lt;- local.se &lt;- rep(NA, 7) for (i in 1:7) { fit.ind.x &lt;- lrm.fit(y=full$y,x=full$x[,i],maxit=25) local.coef[i] &lt;- fit.ind.x$coef[2] local.se[i] &lt;- sqrt(fit.ind.x$var[2,2]) } # Table 14.4 tab14 &lt;- cbind(local.coef, local.se, lit.coefficients.f, lit.se.f, lit.coefficients, lit.se) rownames(tab14) &lt;- names(full$coefficients[-1]) colnames(tab14) &lt;- c(&quot;local.coef&quot;, &quot;local.se&quot;, &quot;fixed.coef&quot;, &quot;fixed.se&quot;, &quot;random.coef&quot;, &quot;random.se&quot;) kable(tab14, digits=2, caption = &quot;Univariate coefficients&quot;) (#tab:AAA.lit.estimates)Univariate coefficients local.coef local.se fixed.coef fixed.se random.coef random.se SEX 0.28 0.79 0.36 0.08 0.36 0.18 AGE10 0.98 0.38 0.79 0.06 0.79 0.11 MI 1.50 0.50 1.03 0.27 1.03 0.32 CHF 1.78 0.55 1.59 0.33 1.59 0.41 ISCHEMIA 1.72 0.55 1.52 0.31 1.51 0.38 RENAL 1.24 0.70 1.32 0.25 1.30 0.26 LUNG 0.84 0.53 0.89 0.23 0.85 0.24 14.0.9 A simple prediction model: Table 14.5 We may fit a simple model for in-hospital mortality based on predictors that are known to be relevant from the literature, and that are available in the local data. The standard logistic model with maximum likelihood is obtained from: lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA) We may apply a shrinkage factor, estimated by bootstrapping using the rms::validate function. And finally we explore penalized logistic regression, using rms::pentrace. We find that the estimated coefficients are shrunken towards zero with both the bootstrap shrinkage approach and the penalized maximum likelihood approach. the estimated shrinkage factor is 0.66, the reduction in c statistic is from 0.83 to 0.76, the reduction in R^2 is from 24 to 10%; this is all in agreement with the small effective sample size (18 events). full &lt;- lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA,x=T,y=T) # Estimate shrinkage factor set.seed(1) full.validate &lt;- validate(full, B=500, maxit=10) ## ## Divergence or singularity in 3 samples # Apply shrinkage factor on model coefficients full.shrunk.coefficients &lt;- full.validate[&quot;Slope&quot;,&quot;index.corrected&quot;] * full$coefficients # Adjust intercept with lp as offset variable lp.AAA &lt;- full$x %*% full.shrunk.coefficients[2:(ncol(full$x)+1)] fit.offset &lt;- lrm.fit(y=full$y, offset=lp.AAA) full.shrunk.coefficients[1] &lt;- fit.offset$coef[1] # Full model, penalized estimation penalty &lt;- pentrace(full,penalty=c(0.5,1,2,3,4,6,8,12,16,24,36,48),maxit=25) plot(penalty) cat(&quot;The optimal penalty was&quot;, penalty$penalty, &quot;for effective df around 3.5 rather than 7 df\\n&quot;) ## The optimal penalty was 16 for effective df around 3.5 rather than 7 df full.penalized &lt;- update(full, penalty=penalty$penalty) # the 3 sets of coefs coef.mat &lt;- cbind(full$coef, full.shrunk.coefficients, full.penalized$coef ) colnames(coef.mat) &lt;- c(&quot;Full, ML&quot;, &quot;Shrinkage&quot;, &quot;Penalized&quot;) kable(coef.mat, digits=2, caption = &quot;Estimated local multivariable regression coefficients in IPD&quot;) (#tab:AAA.models)Estimated local multivariable regression coefficients in IPD Full, ML Shrinkage Penalized Intercept -8.10 -6.04 -5.60 SEX 0.30 0.19 0.16 AGE10 0.58 0.38 0.32 MI 0.74 0.49 0.55 CHF 1.04 0.68 0.64 ISCHEMIA 0.99 0.65 0.60 RENAL 1.12 0.74 0.71 LUNG 0.61 0.40 0.37 # shrinkage factor: slope of lp cstat &lt;- full.validate[1,] cstat[c(1:3,5)] &lt;- full.validate[1,c(1:3,5)] * 0.5 + 0.5 # c = D/2 + 0.5 cstat[4] &lt;- full.validate[1,4] * 0.5 # optimism in c # show key performance measures, including c statistic, R2, and estimated shrinkage factor kable(rbind(cstat, full.validate[1:4,]), digits = 2, caption = &quot;Internally validated performance&quot; ) (#tab:AAA.models)Internally validated performance index.orig training test optimism index.corrected n cstat 0.83 0.86 0.79 0.07 0.76 497 Dxy 0.66 0.72 0.57 0.15 0.52 497 R2 0.24 0.32 0.17 0.14 0.10 497 Intercept 0.00 0.00 -0.69 0.69 -0.69 497 Slope 1.00 1.00 0.66 0.34 0.66 497 14.0.10 Adaptation of univariate coefficients: Table 14.6 We implement two variants of an “adaptation method” that takes advantage of univariate literature data in the estimation of the multivariable regression coefficients in a local prediction model. Adaptation method 1 is simple: beta m|(I+L) = beta u|L + (beta m|I - beta u|I), where beta m|(I+L) is the set of multivariable coefficients (“m”) for the predictors considering both individual patient data (“I”) and literature data (“L”). The univariate coefficients are denoted as “beta u”. The adaptation factor (beta m|I - beta u|I) is the difference between multivariable and univariate coefficient in the IPD data set. The adaptation factor adapts beta u|L, the set of univariate estimates from the literature. Adaptation method 2 is a bit more complex: beta m|(I+L) = beta m|I + c * (beta u|L - beta u|I), with c a factor between 0 and 1. With c=0, adaptation method 2 is ignoring literature information; we simply use the mulktivariable coefficients from our local data. With c=1, adaptation method 2 is the same as adaptation method 1, rewritten as: beta m|(I+L) = beta m|I + beta u|L - beta u|I. The optimal value if c can be derived as a combination of the variance estimates of the components beta m|, beta u|L, and beta u|I with the correlation between univariate and multivariable coefficient in the individual patient data: r(beta m|I, beta u|I). The latter correlation can be estimated empirically by a bootstrap procedure. We estimate coefficients in many bootstap samples drawn with replacement and calculate the correlation after excluding outliers. set.seed(1) # we need the user written function full.uni.mult.cor &lt;- bootcor.uni.mult(full, group=full$y, B=500, maxit=10, trim=.1,save.indices = T) ## Bootsample: 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 ## Number of valid bootstraps for correlation and shrinkage: 484 ## Correlation coefficients: 0.36 0.84 0.91 0.9 0.89 0.97 0.91 ## Shrinkage: 0.72 # outliers are removed by &#39;outliers&#39; package # full.uni.mult.cor[4] # matrix with results per bootstrap # now do adaptation method approach adapt.factors &lt;- adapt.coefficients &lt;- adapt.var &lt;- adapt.coef.Gr &lt;- adapt.var.Gr &lt;- rep(0,7) # Adaptation method 1, simple, just take the difference in own data between m and u estimates for coefficients for (i in 1:7) { fit.ind.x &lt;- lrm.fit(y=full$y,x=full$x[,i],maxit=25) adapt.coef.Gr[i] &lt;- full$coefficients[i+1] + 1 * (lit.coefficients[i] - fit.ind.x$coefficients[2]) adapt.var.Gr[i] &lt;- full$var[i+1,i+1] - fit.ind.x$var[2,2] + lit.se[i]^2 } # end adaptation 1 # Adaptation method 2: optimal adaptation factors for (i in 1:7) { fit.ind.x &lt;- lrm.fit(y=full$y,x=full$x[,i],maxit=25) adapt.factors[i] &lt;- (full.uni.mult.cor$r[i] * sqrt(full$var[i+1,i+1]) * sqrt(fit.ind.x$var[2,2]) ) / (lit.se[i]^2 + fit.ind.x$var[2,2]) adapt.coefficients[i] &lt;- full$coefficients[i+1] + adapt.factors[i] * (lit.coefficients[i] - fit.ind.x$coefficients[2]) adapt.var[i] &lt;- full$var[i+1,i+1] * (1-(full.uni.mult.cor$r[i]^2 * fit.ind.x$var[2,2] / (lit.se[i]^2 + fit.ind.x$var[2,2]))) } # end adaptation 2 # Store all coefficients and related measures in a nice matrix tab14 &lt;- cbind(lit.coefficients, local.coef, full$coef[-1], full.uni.mult.cor$r, adapt.factors, adapt.coef.Gr, adapt.coefficients, full.shrunk.coefficients[-1], full.penalized$coef[-1] ) rownames(tab14) &lt;- names(full$coefficients[-1]) colnames(tab14) &lt;- c(&quot;random.coef.u&quot;, &quot;local.coef.u&quot;, &quot;local.coef.m&quot;, &quot;r&quot;, &quot;adapt.opt&quot;, &quot;adapted.1.m&quot;, &quot;adapted.2.m&quot;, &quot;shrunk.m&quot;, &quot;penalized.m&quot;) kable(tab14, digits=2, caption = &quot;Coefficients for AAA modeling&quot;) (#tab:adaptation.1)Coefficients for AAA modeling random.coef.u local.coef.u local.coef.m r adapt.opt adapted.1.m adapted.2.m shrunk.m penalized.m SEX 0.36 0.28 0.30 0.36 0.38 0.38 0.33 0.19 0.16 AGE10 0.79 0.98 0.58 0.84 0.79 0.38 0.42 0.38 0.32 MI 1.03 1.50 0.74 0.91 0.74 0.27 0.40 0.49 0.55 CHF 1.59 1.78 1.04 0.90 0.63 0.85 0.92 0.68 0.64 ISCHEMIA 1.51 1.72 0.99 0.89 0.68 0.79 0.86 0.65 0.60 RENAL 1.30 1.24 1.12 0.97 0.94 1.18 1.18 0.74 0.71 LUNG 0.85 0.84 0.61 0.91 0.84 0.62 0.62 0.40 0.37 14.0.10.1 Adaptation results: coefficients in Table 14.6 The first 3 columns show the estimates that are input for the adaptation approaches: the random effect pooled estimates for the univariate associations (random.coef.u) the local, or IPD, univariate associations (local.coef.u); and the local multivariable associations (local.coef.m). The empirically estimated correlation r between local univariate and multivariable coefficients was between 0.36 for SEX and around 0.9 for other the predictors. The optimal adaptation factor (adapt.opt) was closely related to the estimate of r. Adaptation method 1 or 2 lead to similar estimates for the multivariable coefficient (beta m|(I+L), denoted as adapted.1.m and adapted.2.m). The adapted estimates are larger than shrunk or penalized estimates that only consider the local IPD. # Store all standard error estimates in a nice matrix tab14 &lt;- cbind(lit.se, local.se, sqrt(diag(full$var)[-1]), sqrt(adapt.var.Gr), sqrt(adapt.var), sqrt(diag(full.penalized$var)[-1] )) # Reduction in variance reduct.var &lt;- 100 - round(100*adapt.var / diag(full$var)[-1]) tab14 &lt;- cbind(tab14, reduct.var) rownames(tab14) &lt;- names(full$coefficients[-1]) colnames(tab14) &lt;- c(&quot;random.u&quot;, &quot;local.u&quot;, &quot;local.m&quot;, &quot;adapted.1&quot;, &quot;adapted.2&quot;, &quot;penalized.m&quot;, &quot;% reduction variance adapt 2&quot;) kable(tab14, digits=2, caption = &quot;SE estimates and reduction in variance by adaptation&quot;) (#tab:adaptation.se)SE estimates and reduction in variance by adaptation random.u local.u local.m adapted.1 adapted.2 penalized.m % reduction variance adapt 2 SEX 0.18 0.79 0.86 0.40 0.81 0.61 13 AGE10 0.11 0.38 0.39 0.14 0.23 0.24 65 MI 0.32 0.50 0.57 0.41 0.36 0.39 60 CHF 0.41 0.55 0.59 0.47 0.41 0.38 52 ISCHEMIA 0.38 0.55 0.62 0.48 0.42 0.38 54 RENAL 0.26 0.70 0.77 0.41 0.31 0.62 83 LUNG 0.24 0.53 0.59 0.34 0.33 0.43 69 14.0.10.2 Adaptation results: variance in Table 14.6 As discussed above, the standard errors (SEs) from random effect pooling of literature data are notably smaller than those for the local univariate coefficients. The local multivariable coefficients have larger SEs, as always for logistic regression models (Robinson &amp; Jewell, 1991). The adaptation methods suggest substantially smaller SEs. For SEX the reduction is only 13%, reflecting the poor correlation between multivariable and univariable coefficients in the IPD (r=0.36). Over 50% reduction in variance is obtained for the other estimates; assuming a global prediction model holds with respect to the predictor effects. # Round the coefficients after shrinkage 0.9 rounded.coef &lt;- round(0.9*adapt.coefficients,1) # we estimate the intercept X &lt;- as.matrix(full$x) X[,2] &lt;- X[,2] - 7 # center around age=7 decades (70 years) # calculate linear predictor, used as offset in lrm.fit offset.AAA &lt;- X %*% rounded.coef fit.offset &lt;- lrm.fit(y=full$y, offset=offset.AAA) # Recalibrate baseline risk to 5% (odds(0.05.0.95)); observed was 18/238, or odds 18/220 recal.intercept &lt;- fit.offset$coef[1] + log((0.05/0.95)/(18/220)) ################################################################### # Alternative, crude naive Bayes approach # Recalibrate literature coefficients with one calibration factor # calculate linear predictor, used as the only x variable in lrm.fit lit.score &lt;- X %*% lit.coefficients fit.lit &lt;- lrm.fit(y=full$y, x=lit.score) print(fit.lit, digits=2) # uni coefs model fit ## Logistic Regression Model ## ## lrm.fit(x = lit.score, y = full$y) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 238 LR chi2 25.14 R2 0.242 C 0.835 ## 0 220 d.f. 1 R2(1,238)0.096 Dxy 0.669 ## 1 18 Pr(&gt; chi2) &lt;0.0001 R2(1,49.9)0.384 gamma 0.670 ## max |deriv| 1e-08 Brier 0.061 tau-a 0.094 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -4.08 0.56 -7.35 &lt;0.0001 ## x[1] 0.69 0.16 4.44 &lt;0.0001 ## # 3 formulas for risk calculation recal.intercept.lit &lt;- fit.lit$coef[1] + log((0.05/0.95)/(18/220)) tab14 &lt;- cbind(full$coefficients, full.penalized$coefficients, c(recal.intercept, rounded.coef), c(NA, lit.coefficients), c(recal.intercept.lit, fit.lit$coef[2]*lit.coefficients)) rownames(tab14) &lt;- names(full$coefficients) colnames(tab14) &lt;- c(&quot;full.coef&quot;, &quot;penalized.m&quot;, &quot;adapted&quot;, &quot;lit coefs&quot;, &quot;lit score&quot;) kable(tab14, digits=1, caption = &quot;Rounded coefficients with IPD only, adaptation approach, or naive Bayes score&quot;) (#tab:adaptation.score)Rounded coefficients with IPD only, adaptation approach, or naive Bayes score full.coef penalized.m adapted lit coefs lit score Intercept -8.1 -5.6 -4.1 NA -4.5 SEX 0.3 0.2 0.3 0.4 0.3 AGE10 0.6 0.3 0.4 0.8 0.5 MI 0.7 0.5 0.4 1.0 0.7 CHF 1.0 0.6 0.8 1.6 1.1 ISCHEMIA 1.0 0.6 0.8 1.5 1.0 RENAL 1.1 0.7 1.1 1.3 0.9 LUNG 0.6 0.4 0.6 0.9 0.6 14.0.10.3 A score for prediction in AAA patients: Table 14.6 Prediction of mortality in AAA patients could be based on the simple full model estimates, based on maximum likelihood (full.coef). Shrinkage or penalization leads to estimates closer to zero (penalized.m). The adaptation results were shrunk with an overall factor of 0.9, based on the empirical behavior in the GUSTO data set, as described in Table 14.3, section 14.1.9 of Clinical Prediction Models. The adapted estimates are rather somewhat in between the full.coef and penalized.mestimates, and are kind of compromise with the univariate literature estimates (lit coefs). Smaller adapted estimates than in the penalized model are obtained for MI (history of a myocardial infarction), reflecting the finding of a univariate estimate of 1.5 in the IPD versus 1.0 in the literature. An alternative approach is to calibrate a score based on the univariate literature coefficients. The calibration factor is 0.69, estimated by using the score as a single predictor: lit.score &lt;- X %*% lit.coefficients fit.lit &lt;- lrm.fit(y=full$y, x=lit.score) Finally, we can calibrate the scores to an average risk of 5%. This recalibration was implemented by the log(odds ratio) for odds(5%) / odds(case study) to the estimated logistic regression model intercept : fit.offset$coef[1] + log((0.05/0.95)/(18/220)) # fit with adapted coefficients in score to obtain c statistics adapt.fit &lt;- lrm.fit(y=full$y, x=offset.AAA) tab14 &lt;- matrix(c(full$stats[6], full.penalized$stats[6], adapt.fit$stats[6], fit.lit$stats[6]), nrow=1) rownames(tab14) &lt;- &quot;Apparent c statistic&quot; colnames(tab14) &lt;- c(&quot;full.coef&quot;, &quot;penalized.m&quot;, &quot;adapted&quot;, &quot;lit score&quot;) kable(tab14, digits=3, caption = &quot;C stats&quot;) (#tab:adaptation.performance)C stats full.coef penalized.m adapted lit score Apparent c statistic 0.832 0.833 0.829 0.835 # calibration tab14 &lt;- matrix(c(adapt.fit$coef[2], 1/adapt.fit$coef[2] ), nrow=1) rownames(tab14) &lt;- &quot;&quot; colnames(tab14) &lt;- c(&quot;adapted coefficient&quot;, &quot;adapted effective shrinkage&quot;) kable(tab14, digits=3, caption = &quot;Calibration insights&quot;) (#tab:adaptation.performance)Calibration insights adapted coefficient adapted effective shrinkage 1.29 0.778 The apparent discriminative performance is all similar for these sets of coefficients, with c statistics around 0.83. We noted from the bootstrap procedure above that the internally validated estimate was 0.80 rather than 0.83. We do not know what the validated performance is for the model variants based on literature data (adapted; lit score). For calibration, we noted that the estimated shrinkage factor for the full model was around 0.7. When we fit a model with adapted coefficients, the coefficient is 1.29; so an effective shrinkage of 1/1.29 = 0.78 was built in by considering the literature estimates in an adaptation approach. Conclusions Findings from other studies can be used in various ways to develop a locally applicable prediction model. The modeling with impact data for TBI patients illustrated an IPD MA approach. A global model can readily be derived with a local baseline risk estimate. The modeling with AAA data for aneurysm surgery illustrated two variants of adaptation approaches: simple, subtracting the difference in IPD data between univariate and multivariable coefficient from univariate literature estimates; a more sophisticated approach, with an optimized adaptation factor from a bootstrap procedure We can also recalibrate a simple score based on univariate literature coefficients. This is a variant of naive Bayes modeling. By definition, estimates that borrow information of other studies are more stable than per study estimates; the assumption is that the observed predictor associations in other studies are relevant for the local setting. For baseline risk, substantial heterogeneity was observed in the TBI case study. For the AAA study, calibration to an average risk of 5% was implemented. Local validation studies are needed to confirm the applicability of prediction models; followed by updating where needed (Binuya 2022). "],["evaluation.html", "15 Evaluation of Performance", " 15 Evaluation of Performance Chapter 15 additional material upcoming. "],["usefulness.html", "16 Evaluation of Clinical Usefulness", " 16 Evaluation of Clinical Usefulness Chapter 16 additional material upcoming. "],["validation.html", "17 Validation of Prediction Models", " 17 Validation of Prediction Models Chapter 17 additional material upcoming. "],["presentation.html", "18 Presentation Formats 18.1 Fit logistic models in n544 data set; 6 predictors Fig 18.1 18.2 Scores for continuous predictors Table 18.2 Fig 18.2 End Table 18.2 (score chart) and Fig 18.2 (graphic translation from score to probability) Table 18.3 Fig 18.3 Fig 18.4 Apparent performance of penalized vs rescaled vs simplified models", " 18 Presentation Formats 18.1 Fit logistic models in n544 data set; 6 predictors We develop a prediction model with 6 predictors for presence of residual tumor in 544 men treated for metastatic nonseminomatous testicular cancer. Predictors were pre-specified based on review of the medical literature. We first consider and full model and 2 shrinkage variants: uniform shrinkage and a penalized model. We then study various formats for presentation of the prediction model for application in medical practice. The formats differ in user-friendliness and accuracy: for some simplifications we sacrifice nearly nothing and for other a lot in terms of c statistic. The formats are: nomogram (Fig 18.1) score chart (Table 18.2 and Fig 18.2) simple table (Table 18.3) iso-probability lines (Fig 18.3) tree on a meta-model (Fig 18.4) # Fit a full 6 predictor model in n544 options(prType=&#39;html&#39;) html(describe(n544), scroll=TRUE) .earrows {color:silver;font-size:11px;} fcap { font-family: Verdana; font-size: 12px; color: MidnightBlue } smg { font-family: Verdana; font-size: 10px; color: &#808080; } hr.thinhr { margin-top: 0.15em; margin-bottom: 0.15em; } span.xscript { position: relative; } span.xscript sub { position: absolute; left: 0.1em; bottom: -1ex; } n544 Descriptives n544 11 Variables 544 Observations Teratoma .hmisctable927586 { border: none; font-size: 85%; } .hmisctable927586 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable927586 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 544020.7462520.46320.4982 Pre.AFP .hmisctable636950 { border: none; font-size: 85%; } .hmisctable636950 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable636950 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 544020.6751860.34190.4508 Pre.HCG .hmisctable894878 { border: none; font-size: 85%; } .hmisctable894878 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable894878 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 544020.7042050.37680.4705 Pre.lnLDH n missing distinct Info Mean Gmd .05 .10 .25 544 0 472 1 0.4582 0.7327 -0.38735 -0.26645 -0.03266 .50 .75 .90 .95 0.31299 0.92886 1.33528 1.65328 lowest : -1.066 -0.948 -0.767 -0.731 -0.565 , highest: 2.428 2.449 2.487 2.624 2.766 sqpost .hmisctable391133 { border: none; font-size: 74%; } .hmisctable391133 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable391133 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 5440930.9965.1332.922 1.414 2.236 3.162 4.472 6.652 9.66910.000 lowest : 1.41 1.73 2.00 2.05 2.24 , highest: 10.95 12.25 13.04 13.78 17.32 reduc10 .hmisctable901042 { border: none; font-size: 74%; } .hmisctable901042 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable901042 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 54402000.9984.4824.307-2.500 0.000 2.000 5.167 7.337 8.78510.000 lowest : -13.81 -10.00 -8.33 -8.11 -6.00 , highest: 9.12 9.17 9.29 9.33 10.00 Necrosis .hmisctable256598 { border: none; font-size: 85%; } .hmisctable256598 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable256598 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 544020.7432450.45040.496 Tumor .hmisctable892796 { border: none; font-size: 85%; } .hmisctable892796 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable892796 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoSumMeanGmd 544020.7432990.54960.496 Reduction .hmisctable589191 { border: none; font-size: 74%; } .hmisctable589191 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable589191 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 54402000.99844.8243.07-25.00 0.00 20.00 51.67 73.37 87.85100.00 lowest : -138.1 -100.0 -83.3 -81.1 -60.0 , highest: 91.2 91.7 92.9 93.3 100.0 LDHst .hmisctable402988 { border: none; font-size: 74%; } .hmisctable402988 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable402988 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 544047212.031.6050.67890.76610.96791.36752.53173.80145.2242 lowest : 0.344 0.388 0.464 0.481 0.568 , highest: 11.338 11.581 12.026 13.787 15.891 Post.size .hmisctable142744 { border: none; font-size: 74%; } .hmisctable142744 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable142744 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 5440930.99633.2633.37 2.00 5.00 10.00 20.00 44.25 93.50100.00 lowest : 2.00 3.00 4.00 4.22 5.00 , highest: 120.00 150.00 170.00 190.00 300.00 # Start the fitting of 3 models set.seed(1) full &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+log(LDHst)+sqrt(Post.size)+Reduction, data=n544,x=T,y=T,linear.predictors=T) print(full) Logistic Regression Model lrm(formula = Necrosis ~ Teratoma + Pre.AFP + Pre.HCG + log(LDHst) + sqrt(Post.size) + Reduction, data = n544, x = T, y = T, linear.predictors = T) Model LikelihoodRatio Test DiscriminationIndexes Rank Discrim.Indexes Obs 544 LR χ2 211.56 R2 0.431 C 0.839 0 299 d.f. 6 R26,544 0.315 Dxy 0.677 1 245 Pr(>χ2) R26,404 0.399 γ 0.678 max |∂log L/∂β| 1×10-7 Brier 0.163 τa 0.336 β S.E. Wald Z Pr(>|Z|) Intercept  -1.0425  0.6086 -1.71 0.0867 Teratoma   0.9094  0.2140 4.25 Pre.AFP   0.9025  0.2333 3.87 0.0001 Pre.HCG   0.7827  0.2305 3.40 0.0007 LDHst   0.9854  0.2089 4.72 Post.size  -0.2915  0.0815 -3.58 0.0003 Reduction   0.0158  0.0052 3.04 0.0024 18.1.1 Internal validation by bootstrapping ## Validate model with bootstrapping set.seed(1) val.full &lt;- validate(full, B=100) val.full[1,c(1:3,5) ] &lt;- val.full[1,c(1:3,5) ]/2 + 0.5 # transform Dxy to C val.full[1,4 ] &lt;- val.full[1,4]/2 # optimism in c rownames(val.full)[1] &lt;- &quot;C&quot; kable(as.data.frame(val.full[1:4,]), digits=3) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) index.orig training test optimism index.corrected n C 0.839 0.839 0.835 0.004 0.835 100 R2 0.431 0.434 0.423 0.011 0.420 100 Intercept 0.000 0.000 0.000 0.000 0.000 100 Slope 1.000 1.000 0.978 0.022 0.978 100 So, limited optimism in c index (Dxy/2+0.5): 0.004. And limited need for shrinkage, since the calibration slope is close to 1: 0.98. For illustration, we consider 2 shrinkage approaches: 1. shrink coefficients with a uniform factor (from bootstrap validation (or by a heuristic formula as described by Copas 1983 and Van Houwelingen 1990 ). 2. penalize coefficients with a penalty factor as estimated by the optimal AIC in the pentrace function. Empirical comparisons between shrinkage apporaches are here: Steyerberg 2000. A recent study compared the stability of shrinkage estimators: Van Calster 2020. 18.1.2 Shrunk model and penalization # Apply linear shrinkage full.shrunk &lt;- full full.shrunk$coef &lt;- val.full[4,5] * full.shrunk$coef # use result from bootstrapping # val.full[4,5] is shrinkage factor; heuristic estimate (LR - df) / LR = (211-6)/211=0.97 # Estimate new intercept, with shrunk lp as offset variable, i.e. coef fixed at unity full.shrunk$coef[1] &lt;- lrm.fit(y=full$y, offset= full$x %*% full.shrunk$coef[2:7])$coef[1] # Make a penalized model with pentrace function p &lt;- pentrace(full, c(0,1,2,3,4,5,6,7,8,10,12,14,20)) plot(p, which=&#39;aic.c&#39;, xlim=c(0,15), ylim=c(196,201)) lines(x=c(4,4), y=c(196, 1000)) The optimal penalty factor is 4. Let’s update the full model fit with this penalty term. 18.1.3 Fit penalized model and compare fits full.pen &lt;- update(full, penalty=p$penalty) ## compare coefs of 3 model variants fit.coefs &lt;- cbind(full=full$coefficients, shrunk=full.shrunk$coef, penalized=full.pen$coefficients) kable(as.data.frame(fit.coefs, digits=3)) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) full shrunk penalized Intercept -1.042 -1.021 -1.090 Teratoma 0.909 0.890 0.873 Pre.AFP 0.903 0.883 0.860 Pre.HCG 0.783 0.766 0.729 LDHst 0.985 0.964 0.884 Post.size -0.292 -0.285 -0.261 Reduction 0.016 0.015 0.016 Fig 18.1 18.1.4 Make nomogram from penalized model par(mfrow=c(1,1), mar=c(1,1,4,.5)) # Need some stats for the data to plot nomogram dd &lt;- datadist(n544) options(datadist=&quot;dd&quot;) nom &lt;- nomogram(full.pen, fun=c(function(x)(1-plogis(x)), plogis), lp=T,lp.at=c(-2,-1,0,1,2,3), LDHst=c(.5,1,1.5,2,3), Post.size=c(50,20,10,5,2), Reduction=c(0,35,70,100), fun.at=c(seq(.1,.9,by=.1),0.95), funlabel=c(&quot;p(residual tumor)&quot;, &quot;p(necrosis)&quot;), vnames=&quot;lab&quot;, maxscale=10) plot(nom) title(&quot;Testicular cancer prediction model: risk of residual tumor&quot;) So, we have a nice presentation that allows for assessing the relative importance of predictors (by length of the lines), and allows for precise estimates of risks of residual tumor (and its complement, necrosis, benign tissue). Instruction to physicians using the model in their care: Determine the patient’s value for each predictor, and draw a straight line upward to the points axis to determine how many points toward benign histology the patient receives. Sum the points received for each predictor and locate this sum on the total points axis. Draw a straight line down to find the patient’s predicted probability of residual tumor or necrosis (benign histology). Instruction to patient: “Mr. X, if we had 100 men exactly like you, we would expect that the chemotherapy was fully successful in approximately &lt;predicted probability from nomogram * 100&gt;, as reflected in fully benign disease at surgical resection of your abdominal lymph nodes.” see Kattan et al. Note that the number 100 may be debated, since the effective sample size for some covariate patterns may be far less. 18.1.5 Score chart creation with categorized and continuous predictors We first search for some nice rounding of logistic regression coefficients. A classic approach is multiplying by 10. We find that multiplying by 10/8, or 1.25 works well. We then continue with searching for a similarly nice scoring for continuous predictors. This is a trial and error process, supported by graphical illustrations. 18.1.6 Consider making rounded scores from penalized coefs scores &lt;- matrix(nrow=10, ncol=length(full.pen$coefficients), dimnames = list(NULL, c(&quot;Multiplier&quot;, names(full.pen$coefficients[-1])))) for (i in 1:10) { scores[i,] &lt;- c(10/i, round(full.pen$coefficients[-1] * 10 / i)) } # consider a range of multipliers kable(as.data.frame(scores, digits=3)) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) Multiplier Teratoma Pre.AFP Pre.HCG LDHst Post.size Reduction 10.00 9 9 7 9 -3 0 5.00 4 4 4 4 -1 0 3.33 3 3 2 3 -1 0 2.50 2 2 2 2 -1 0 2.00 2 2 1 2 -1 0 1.67 1 1 1 1 0 0 1.43 1 1 1 1 0 0 1.25 1 1 1 1 0 0 1.11 1 1 1 1 0 0 1.00 1 1 1 1 0 0 # rounded scores kable(as.data.frame(rbind(mult10=round(10*full.pen$coef[-1]), mult1.25 =round(10/8*full.pen$coef[-1]))), digits=3) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) Teratoma Pre.AFP Pre.HCG LDHst Post.size Reduction mult10 9 9 7 9 -3 0 mult1.25 1 1 1 1 0 0 So, we like the multiplier of 10/8, or 1.25. Let’s search for the mapping of coefficients of continuous predictors on nice scores as well. 18.2 Scores for continuous predictors 18.2.1 LDH Start with LDH, where we use the log scale for standardized LDH values. We could e.g. find: 1 point per ln, so 1 point per 2.7 times normal. We use a score of 10/8 for LDH in a plot. We add reference lines for point allocation. par(mfrow=c(1,1), mar=c(4,4,2,2)) # Use score of 10/8 for LDH, plot score by LDHst LDHplot &lt;- as.data.frame(cbind(n544$LDHst, round(10/8*full.pen$coef[5]*n544$Pre.lnLDH,1))) plot(x=LDHplot[,1], y=LDHplot[,2], xlab=&quot;LDHst (LDH / upper limit of normal LDH)&quot;, ylab=&quot;Rounded score&quot;, axes=F, xlim=c(.5,6.9), ylim=c(-.6,2.2), cex.lab=1.2, main = &quot;LDH&quot;) scat1d(x=n544$LDHst, side=1, col=&quot;red&quot;, lwd=2) axis(side=1, at=c(.5,1,1.5,2,2.5,3,4,5,6,7)) axis(side=2, at=c(-.5, 0,.5,1,1.5,2)) # horizontal reference lines abline(a=0, b=0, lwd=2) abline(a=1, b=0) abline(a=2, b=0) # vertical reference lines lines(x=c(1,1), y=c(-.5, .5), lwd=2) lines(x=c(2.5,2.5), y=c(-.5, 1.5)) lines(x=c(2.5^2,2.5^2), y=c(-.5, 2.5)) We find that a score of 1 corresponds to 2.5 times larger LDH values. Continue with post.size. 18.2.2 Study scores for postchemotherapy size # calculate score by sequence 2 to 100mm; # 2 is minimum for analysis; set 10 to zero in sequence (sqrt(seq(2,50,by=1))-sqrt(10)) # We use 10mm as a reference, set to zero Sizeplot &lt;- as.data.frame(cbind(n544$Post.size, round(10/8*full.pen$coef[6]*(sqrt(n544$Post.size) - sqrt(10)),1)), main = &quot;Postchemotherapy size&quot;) plot(x=Sizeplot[,1], y=Sizeplot[,2], xlab=&quot;Postchemo size (mm)&quot;, ylab=&quot;Rounded score&quot;, axes=F, xlim=c(0,100), ylim=c(-2.1,1), cex.lab=1.2, main = &quot;Postsize&quot;) scat1d(x=n544$Post.size, side=1, col=&quot;red&quot;, lwd=2) axis(side=1, at=c(2,5,10,20,30,40,50,70, 100)) axis(side=2, at=c(-2,-1,-.5,0,0.5)) # horizontal reference lines abline(a=0, b=0, lwd=2) abline(a=.5, b=0) abline(a=-.5, b=0) abline(a=-1, b=0) abline(a=-2, b=0) # vertical reference lines lines(x=c(2,2), y=c(-2.1, .75)) lines(x=c(10,10), y=c(-2.1, 0.25), lwd=2) lines(x=c(20,20), y=c(-2.1, -.1)) lines(x=c(40,40), y=c(-2.1, -0.6)) lines(x=c(100,100), y=c(-2.1, -1.5)) So, approximate scores for clinically meaningful sizes: 2 mm score +.5; 10mm, 0; 20mm, -.5; 40mm -1; 100 -2 Continue with Reduction. This should be simple, since Reduction was modelled as a linear variable, without transformations. 18.2.3 Study scores for reduction in size Redplot &lt;- as.data.frame(cbind(n544$Reduction, round(10/8*full.pen$coef[7]*n544$Reduction,1))) plot(x=Redplot[,1], y=Redplot[,2], xlab=&quot;Reduction in size (%)&quot;, ylab=&quot;Rounded score&quot;, axes=F, xlim=c(0,100), ylim=c(-.1,2.1), cex.lab=1.2, col=&quot;red&quot;, main = &quot;Reduction in size&quot;) scat1d(x=n544$Reduction, side=1,col=&quot;red&quot;, lwd=2) axis(side=1, at=c(0,25,50,75,100)) axis(side=2, at=c(0,1,2)) # horizontal reference lines abline(a=0, b=0, lwd=2) abline(a=1, b=0) abline(a=2, b=0) # vertical reference lines lines(x=c(0,0), y=c(-.1, 0.1), lwd=2) lines(x=c(50,50), y=c(-.1, 1.25)) lines(x=c(100,100), y=c(-.1, 2.5)) So, scores for reduction in size: 50%, score 1; 100%, score 2 We store the scores in new variables for later use in a logistic regression model, to check the impact of rounding on model performance. # A score of 1 corresponds to 2.5 times larger LDH # rescale such that exp(l) does get more than 1 point; 1 point at 2.5 times normal LDH n544$LDHr &lt;- round(n544$Pre.lnLDH*exp(1)/2.5,1) # postsize: coef -.3 per sqrt(mm). Hence -1 with 3.3*sqrt(mm) # So, approximate scores for clinically meaningful sizes: # 2 mm score +.5; 10mm, 0; 20mm, -.5; 40mm -1; 100 -2 # change the intercept by defining score = 0 for 10 mm, which is clinically of most interest n544$SQPOSTr &lt;- round((n544$sqpost - sqrt(10)) / (sqrt(10) - sqrt(40)),1) # Rescale reduction such that effect 1 point # 50% reduction 1 point; i.e. 0% = 0; 50%=1; 100%=2 n544$REDUC5 &lt;- round(n544$reduc10 / 5,1) Table 18.2 18.2.4 Make a score chart Score chart to be made by hand, using the points as derived above. Could also have kept life simpler by multiplying coefs by 10 and rounding. score.fit &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+LDHr+SQPOSTr+REDUC5,data=n544, x=T,y=T) # scores &lt;- as.data.frame(round(10/8*full.pen$coef[-1])) kable(as.data.frame(10/8*score.fit$coef[-1]), digits=3) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) 10/8 * score.fit$coef[-1] Teratoma 1.134 Pre.AFP 1.137 Pre.HCG 0.964 LDHr 1.110 SQPOSTr 1.114 REDUC5 1.008 # # Continuous scores LDHvalues &lt;- c(0.6, 1, 1.6, 2.5, 4, 6) kable(as.data.frame(rbind(LDH=LDHvalues, LDHscores=log(LDHvalues)*exp(1)/2.5)), digits = 2, col.names =NULL) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) LDH 0.60 1 1.60 2.5 4.00 6.00 LDHscores -0.56 0 0.51 1.0 1.51 1.95 Postvalues &lt;- c(2,10,20,40,70) kable(as.data.frame(rbind(Postsize=Postvalues, Postscores=(sqrt(Postvalues) - sqrt(10)) / (sqrt(10) - sqrt(40)))), digits = 2, col.names =NULL) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) Postsize 2.00 10 20.00 40 70.00 Postscores 0.55 0 -0.41 -1 -1.65 Redvalues &lt;- c(0,5,10) kable(as.data.frame(rbind(Reduction=Redvalues, Redscores=Redvalues / 5)), digits = 2, col.names =NULL) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) Reduction 0 5 10 Redscores 0 1 2 So, we see that the continuous values of some predictors map to scores of approximately 0.5 or 1. Intermediate scores can be obtained by interpolation. Fig 18.2 18.2.5 Translate score in probability estimates with 95% CI ##&#39;# Function to relate scores to predictions in graph ### make.score.matrix &lt;-function(fit, scores, shrinkage=1, limits.scores=NULL) { # lpscore for score chart + graph fit &lt;- update(fit, x=T,y=T, se.fit=T) # SE of predictions from original model rounded.lp &lt;- fit$x %*% scores # Linear predictor multiplier &lt;- lrm.fit(y=fit$y, x=rounded.lp)$coef[2] ## For score formula shrunk.beta &lt;- shrinkage * multiplier # Shrinkage built in ## Estimate intercept for scores, using the scores as offset with shrinkage fit.lp &lt;- lrm.fit(y=score.fit$y, offset=shrunk.beta*rounded.lp) ## lp formula; this version is not fully rounded yet lp2 &lt;- fit.lp$coef[1] + shrunk.beta*rounded.lp cat(&quot;\\nlinear predictor:&quot;, fit.lp$coef[1], &quot;+&quot;, shrunk.beta, &quot;* rounded.lp\\n&quot;, &quot; original range of scores&quot;, min(rounded.lp), max(rounded.lp), &quot;\\n&quot;) if (!is.null(limits.scores)) { rounded.lp[rounded.lp &lt; min(limits.scores)] &lt;- min(limits.scores) rounded.lp[rounded.lp &gt; max(limits.scores)] &lt;- max(limits.scores) } cat(&quot;restricted range of scores&quot;, min(rounded.lp), max(rounded.lp), &quot;\\n&quot;) ## Data for graph graph.lp &lt;- cbind(rounded.lp, fit$se.fit) # lp and se.fit in matrix lp.events &lt;- tapply(fit$y,list(round(graph.lp[,1],0)),sum) # events per lp score lp.nonevents &lt;- tapply(1-fit$y,list(round(graph.lp[,1],0)),sum) # non-events per lp score se.lp &lt;- tapply(graph.lp[,2],list(round(graph.lp[,1],0)),mean) # mean se per lp score lp.lp &lt;- tapply(graph.lp[,1],list(round(graph.lp[,1],0)),mean) # mean lp per lp score score &lt;- lp.lp # range of score, rounded to 0 decimals lp3 &lt;- fit.lp$coef[1] + shrunk.beta*round(score,0) # lp on logistic scale, rounded ## data frame with predicted prob + 95% CI p.lp &lt;- as.data.frame(cbind(score=round(score,0), p=plogis(lp3),plow=plogis(lp3-1.96*se.lp),phigh=plogis(lp3+1.96*se.lp), lp.events,lp.nonevents)) p.lp$total &lt;- p.lp$lp.events + p.lp$lp.nonevents p.lp } ## Make Fig 18.2 ## ## Continuous predictors in score score.fit &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+LDHr+SQPOSTr+REDUC5,data=n544, x=T,y=T) scores &lt;- rep(1,6) # All scores a weight of 1 lp.n544 &lt;- make.score.matrix(fit=score.fit, scores=scores, shrinkage=0.95, limits.scores=c(-2,5)) ## ## linear predictor: -1.94 + 0.815 * rounded.lp ## original range of scores -4.3 6.7 ## restricted range of scores -2 5 kable(as.data.frame(lp.n544), digits = 2) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) score p plow phigh lp.events lp.nonevents total -2 -2 0.03 0.01 0.07 0 22 22 -1 -1 0.06 0.03 0.11 2 46 48 0 0 0.13 0.07 0.21 6 53 59 1 1 0.24 0.16 0.36 19 58 77 2 2 0.42 0.31 0.54 48 59 107 3 3 0.62 0.49 0.74 62 37 99 4 4 0.79 0.68 0.87 66 19 85 5 5 0.89 0.82 0.94 42 5 47 par(mfrow=c(1,1),mar=c(4.5,4.5,3,1)) plot(x=lp.n544[,1], y=100*lp.n544[,2], lty = 1, las=1, type = &quot;b&quot;, cex.lab=1.2, ylab = &quot;Predicted probability of necrosis (%)&quot;, ylim=c(-10,100), xlab=&quot;Sum score&quot;, cex=sqrt(lp.n544$total)/5, xaxt=&quot;n&quot;, pch=16, col=&quot;red&quot;) axis(side=1,at=lp.n544[,1], cex=1) # add 95% CI for (i in 1:nrow(lp.n544)) lines(x=c(lp.n544[i,1], lp.n544[i,1]), y=c(100*lp.n544[i,3], 100*lp.n544[i,4]), type=&quot;l&quot;, col=&quot;red&quot;) # add N= ... mtext(&quot;N=&quot;, side = 2, outer = FALSE, line=1.5, at = -10, adj = 0, las=1, cex=1, col=&quot;red&quot;) text(x=lp.n544[,1],y=-10,labels=lp.n544$total, cex=1, col=&quot;red&quot;) End Table 18.2 (score chart) and Fig 18.2 (graphic translation from score to probability) 18.2.6 Categorized coding We can categorize the continuous predictors; a bad idea generally speaking # LDH n544$PRELDH &lt;- ifelse(n544$Pre.lnLDH&lt;log(1),0,1) # PostSize simple coding: 0,1,2 n544$POST2 &lt;- ifelse(n544$sqpost&lt;sqrt(20),2, ifelse(n544$sqpost&lt;sqrt(50),1,0)) # Reduction in categories: increase=-1, 0-49%=0, &gt;=50%=1 n544$REDUCr &lt;- ifelse(n544$Reduction&lt;0,-1, ifelse(n544$Reduction&gt;=50,1,0)) ## Everything coded in categories score.fit2 &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+PRELDH+POST2+REDUCr,data=n544, x=T,y=T) kable(as.data.frame(x=10/8*score.fit2$coef[-1]), digits=2) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) 10/8 * score.fit2$coef[-1] Teratoma 1.15 Pre.AFP 1.08 Pre.HCG 0.83 PRELDH 1.13 POST2 1.08 REDUCr 0.98 So, each category approx 1 point. Table 18.3 18.2.7 Alternative: make a score from 0 - 5 Ultra simple scores “.. categories of the predictors were simplified for practical application. We conclude that a simple statistical model, based on a limited number of patient characteristics, provides better guidelines for patient selection than those currently used in clinical practice. Br J Cancer 1996” score5 &lt;- n544$Teratoma+n544$Pre.AFP+n544$Pre.HCG+n544$PRELDH+n544$REDUCr score5 &lt;- ifelse(n544$REDUCr&lt;0,0,score5) n544$score5 &lt;- score5 # Simple coding: 5 categories for postsize (no difference 20-30 and 30-50 mm) POST5 &lt;- ifelse(n544$sqpost&lt;=sqrt(10),0, ifelse(n544$sqpost&lt;=sqrt(20),1, ifelse(n544$sqpost&lt;=sqrt(30),2, ifelse(n544$sqpost&lt;=sqrt(50),3,4)))) n544$POST5 &lt;- as.factor(POST5) # add the post size categories as a factor full.simple2 &lt;- lrm(Necrosis~POST5+score5, data=n544, x=T,y=T,se.fit=T) pred.simple2 &lt;- aggregate(plogis(predict(full.simple2)), by=list(POST5, score5), FUN=mean) pred.simple2 &lt;- as.data.frame(with(pred.simple2,tapply(x,list(Group.1,Group.2),mean))) kable(as.data.frame(pred.simple2), digits = 2) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) 0 1 2 3 4 5 0 NA 0.32 0.52 0.71 0.85 0.93 1 0.14 0.27 0.45 0.66 0.81 0.91 2 0.06 0.12 0.24 0.42 0.63 0.79 3 0.08 0.16 0.30 0.50 0.69 NA 4 0.04 0.09 0.19 0.35 0.55 NA n544$POST5 &lt;- as.numeric(n544$POST5) # Make simple risk scores per category n544$simple.cat &lt;- ifelse(n544$POST5==3 &amp; score5==4,70, ifelse(n544$POST5==3 &amp; score5==5,80, ifelse(n544$POST5==2 &amp; score5==3,60, ifelse(n544$POST5==2 &amp; score5==4,80, ifelse(n544$POST5==2 &amp; score5==5,90, ifelse(n544$POST5&lt;2 &amp; score5==2,60, ifelse(n544$POST5&lt;2 &amp; score5==3,70, ifelse(n544$POST5&lt;2 &amp; score5==4,80, ifelse(n544$POST5&lt;2 &amp; score5==5,90, 50))))))))) simple.tab &lt;- as.data.frame(with(n544,tapply(simple.cat,list(POST5=POST5,score=score5),mean))) simple.tab[is.na(simple.tab)] &lt;- 50 colnames(simple.tab) &lt;- Cs(score.0, score.1, score.2, score.3, score.4, score.5) rownames(simple.tab) &lt;- Cs(size10, size20, size30, size50, size.gt.50) simple.tab %&gt;% mutate(score.0 = cell_spec(score.0, &quot;html&quot;, color = ifelse(score.0 &gt; 50, &quot;orange&quot;, &quot;red&quot;)), score.1 = cell_spec(score.1, &quot;html&quot;, color = ifelse(score.1 &gt; 50, &quot;orange&quot;, &quot;red&quot;)), score.2 = cell_spec(score.2, &quot;html&quot;, color = ifelse(score.2 &gt; 50, &quot;orange&quot;, &quot;red&quot;)), score.3 = cell_spec(score.3, &quot;html&quot;, color = ifelse(score.3 == 70, &quot;green&quot;, ifelse(score.3 == 60, &quot;orange&quot;,&quot;red&quot;))), score.4 = cell_spec(score.4, &quot;html&quot;, color = ifelse(score.4 == 80, &quot;white&quot;, ifelse(score.4 == 70, &quot;green&quot;, ifelse(score.4 == 60, &quot;orange&quot;,&quot;red&quot;))), background=ifelse(score.4 == 80, &quot;green&quot;,&quot;white&quot; )), score.5 = cell_spec(score.5, &quot;html&quot;, color = ifelse(score.5 &gt;= 80, &quot;white&quot;, ifelse(score.5 == 70, &quot;green&quot;, ifelse(score.5 == 60, &quot;orange&quot;,&quot;red&quot;))), background=ifelse(score.5 == 90, &quot;darkgreen&quot;, ifelse(score.5 == 80, &quot;green&quot;,&quot;white&quot; ))) ) %&gt;% kable(format = &quot;html&quot;, escape = F, row.names = T) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) score.0 score.1 score.2 score.3 score.4 score.5 size10 50 50 60 70 80 90 size20 50 50 50 60 80 90 size30 50 50 50 50 70 80 size50 50 50 50 50 50 50 size.gt.50 50 50 50 50 50 50 # kable(simple.tab, digits = 2) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) In this way we created a very simple table, with probability of necrosis according to post-chemotherapy mass size (rows) and a simple, categorized score of the other 5 predictors. Fig 18.3 18.2.8 Iso-probability lines For radiologists, it is natural to consider the pre-chemotherapy mass size and post-chemotherapy mass size. According to our analysis, the key information is the pre-chemotherapy size and the reduction in size. These pieces of information need to be considered jointly with the 4 other predictors (Teratoma, 3 tumor markers). We attempted to convey this message in iso-probability lines (Fig 18.3) Radiology ## Radiology paper: could graph pre - post with lines for score 1 - 4 score4 &lt;- n544$Teratoma+n544$Pre.AFP+n544$Pre.HCG+n544$Pre.lnLDH full.simple3 &lt;- lrm(Necrosis ~ score4 + sqpost + reduc10, data=n544, x=T, y=T) kable(as.data.frame(full.simple3$coef), digits = 2) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) full.simple3$coef Intercept -1.20 score4 0.89 sqpost -0.27 reduc10 0.17 # Make a presize variable, based on reduction and postsize # reduc = (pre-post) / pre; reduc = 1 - post/pre; reduc-1 = -post/pre; pre = -post/(reduc-1) n544$presize &lt;- - n544$sqpost^2 / ifelse((n544$reduc10/10 - 1) != 0,(n544$reduc10/10 - 1),-.04) # Now calculate iso probability lines: e.g. prob -70%, # Calculate PRESIZE from SQPOST, for given probabilities # presize = f(sqpost, plogis(full)) # pre = -post/(reduc10-10) # for reduc10&lt;10 # range sqpost from 2 to 50; score 4 0 - 4 ## Fig 18.3 ## # matrix with sqpost and scores x &lt;- as.matrix(cbind(rep(sqrt(1:50),5),c(rep(0,50),rep(1,50),rep(2,50),rep(3,50),rep(4,50)))) # linear predictor from simple model &#39;full.simple3&#39; lp.simple3 &lt;- x %*% full.simple3$coef[3:2] + full.simple3$coef[1] # now calculate reduc10 with condition p=70% etc # Solve equation qlogis(.7) = lp.simple3 + full.simple3$coef[4] * n544$REDUC10 reduc10.70 &lt;- (qlogis(.7) - lp.simple3) /full.simple3$coef[4] # Calculate presizes: pre = -post/(reduc10/10-1) presize.70 &lt;- - rep(1:50,5) / (reduc10.70/10-1) # in 1 formula for efficiency presize.90 &lt;- - rep(1:50,5) / (((qlogis(.9) - lp.simple3) /(full.simple3$coef[4])/10) -1) presize.80 &lt;- - rep(1:50,5) / (((qlogis(.8) - lp.simple3) /(full.simple3$coef[4])/10) -1) presize.60 &lt;- - rep(1:50,5) / (((qlogis(.6) - lp.simple3) /(full.simple3$coef[4])/10) -1) presize.50 &lt;- - rep(1:50,5) / (((qlogis(.5) - lp.simple3) /(full.simple3$coef[4])/10) -1) # make Postsize from sqpost x[,1] &lt;- x[,1]^2 # combine results x &lt;- as.matrix(cbind(x,lp.simple3, reduc10.70, presize.70, presize.90,presize.80,presize.60,presize.50 )) colnames(x) &lt;- Cs(postsize, score4,lp,reduc1070, presize70,presize90,presize80,presize60,presize50) x &lt;- as.data.frame(x) x[x&lt;0] &lt;- NA # Make some plots; isoprobability lines for different scores, x-axis=postsize par(mfrow=c(2,2), mar=c(4,4,3,.5)) for (i in 1:4) { xp = x[x$score4==i &amp; !is.na(x$presize50) &amp; x$presize50&lt;100 &amp; x$postsize&lt;50, ] xp = xp[sort(xp$presize50),] plot(y=xp$postsize,x=xp$presize50, main=paste(&quot;Score=&quot;,i,sep=&quot;&quot;), ylab=&#39;Postchemo size (mm)&#39;, xlab=&quot;Prechemo size (mm)&quot;, xlim=c(0,100), ylim=c(0,50),type=&quot;l&quot;, pch=&quot;5&quot;, axes=F,las=1, col=mycolors[5], lwd=2) axis(side=2,at=c(0,10,20,30,50)) axis(side=1,at=c(0,20,50,100)) if (i==1) legend(&quot;topleft&quot;, lty=c(1,2,1,2,1), col=mycolors[c(5,4,2,3,6)], lwd=2,bty=&quot;n&quot;, legend=c(&quot;50%&quot;, &quot;60&quot;, &quot;70%&quot;, &quot;80%&quot;, &quot;90%&quot;)) xp = x[x$score4==i &amp; !is.na(x$presize60), ] xp = xp[sort(xp$presize60),] lines(y=xp$postsize,x=xp$presize60, pch=&quot;6&quot;,lty=2, col=mycolors[4], lwd=2) xp = x[x$score4==i &amp; !is.na(x$presize70), ] xp = xp[sort(xp$presize70),] lines(y=xp$postsize,x=xp$presize70, pch=&quot;7&quot;,lty=1, col=mycolors[2], lwd=2) xp = x[x$score4==i &amp; !is.na(x$presize80), ] xp = xp[sort(xp$presize80),] lines(y=xp$postsize,x=xp$presize80, pch=&quot;8&quot;, lty=2, col=mycolors[3], lwd=2) xp = x[x$score4==i &amp; !is.na(x$presize90), ] xp = xp[sort(xp$presize90),] lines(y=xp$postsize,x=xp$presize90, pch=&quot;9&quot;, lty=1, col=mycolors[6], lwd=2) } # end loop over score 1 - 4 So, we obtain some nice graphics. With score 0, men are at high risk of tumor. With score 4, even a relatively large post-chemotherapy mass size leads to high probabilities of necrosis, especially if the pre-chemotherapy mass was large. Fig 18.4 18.2.9 Meta-model with tree presentation We can present a decision rule for patients with testicular cancer, assuming a threshold of 70% for the probability of necrosis. If the probability is higher than 70%, this is a good prognosis group which could be spared surgery. ## Start with dichotomizing predictions from full as &lt;70% vs &gt;=70% n544$predhigh &lt;- ifelse(plogis(full$linear.predictor)&lt;.7,0,1) # Try to make tree model for this outcome par(mfrow=c(1,1), mar=c(2,2,2,1)) tree.orig &lt;- rpart(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+LDHst+Post.size+Reduction, data=n544) plot(tree.orig) text(tree.orig, use.n=F) title(&quot;Meta-model for original predictions&quot;) tree.meta &lt;- rpart(predhigh ~ Teratoma+Pre.AFP+Pre.HCG+LDHst+Post.size+Reduction, data=n544) plot(tree.meta) text(tree.meta, use.n=F) title(&quot;Meta-model for probability &gt; 70%&quot;) ## Make smooth tree presentation; classification with reduction, teratoma, AFP score3 &lt;- as.numeric(n544$Reduction&gt;70)+as.numeric(n544$Teratoma==1)+as.numeric(n544$Pre.AFP==1) n544$tree.cat &lt;- ifelse(n544$Reduction&gt;50 &amp; score3&gt;=2,1,0) tree.results &lt;- as.data.frame(table(n544$tree.cat, n544$Necrosis)) colnames(tree.results) &lt;- Cs(&quot;Risk group&quot;, Necrosis, N) kable(tree.results, digits = 2) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) Risk group Necrosis N 0 0 264 1 0 35 0 1 134 1 1 111 end tree analysis Apparent performance of penalized vs rescaled vs simplified models What is the c statistic of various formats to predict necrosis at resection after chemotherapy? a) nomogram (Fig 18.1): original or penalized coefficients, negligable difference expected b) score chart (Table 18.2 and Fig 18.2): rounding, minor decline expected c) simple table (Table 18.3): categorization and simplification, much loss expected d) iso-probability lines (Fig 18.3): major predictors (size, reduction) kept continuous, some loss expected e) tree on a meta-model (Fig 18.4): ultra simple, major loss in performance expected # 3 variants of full model fit cstats &lt;- cbind( rcorr.cens(full$linear.predictor, n544$Necrosis)[1], rcorr.cens(full.shrunk$linear.predictor, n544$Necrosis)[1], rcorr.cens(full.pen$linear.predictor, n544$Necrosis)[1], # rounding for score chart rcorr.cens(score.fit$linear.predictor, n544$Necrosis)[1], # simple table rcorr.cens(n544$simple.cat, n544$Necrosis)[1], # Radiology paper rcorr.cens(full.simple3$linear.predictor, n544$Necrosis)[1], # Tree rcorr.cens(n544$tree.cat, n544$Necrosis)[1]) colnames(cstats) &lt;- Cs(full, shrunk, penalized, scores, table, radiology, tree) rownames(cstats) &lt;- &quot;C&quot; dotchart(cstats, xlab=&quot;C statistic&quot;, xlim=c(0.6,.9)) kable(as.data.frame(cstats), digits = 4) %&gt;% kable_styling(full_width=F, position = &quot;left&quot;) full shrunk penalized scores table radiology tree C 0.839 0.839 0.839 0.838 0.725 0.838 0.668 18.2.10 End comparisons of c statistics As expected, the apparent performance of penalized vs rescaled models was very similar. Two formats led to a substantial loss in performance: simple table (Table 18.3): categorization and simplification, c=0.73 cp to c=0.84 tree on a meta-model (Fig 18.4): ultra simple, c=0.67, for a single threshold to classify men as low risk and spare them resection. Please use and improve the code above when desired. The major tools for presentation are: 1. The nomogram function: allows for scores with categorical and continuous predictors, with transformations. 2. The regression formula: can be implemented in wwww tools, including the Evidencio toolbox. "],["external-validity.html", "19 Patterns of External Validity 19.1 Fig 19.7: Case-control design disturbs calibration", " 19 Patterns of External Validity 19.0.1 Fig 19.1: a missed predictor Z ## Actual correlations in n=1000 ## r=0: -0.0114 r=.33: 0.312 r=.5: 0.475 19.0.2 Fig 19.2: a missed predictor Z with identical distribution –&gt; no impact 19.0.3 Fig 19.3: more or less severe cases selected 19.0.4 Fig 19.4: more or less heterogeneous cases selected 19.0.5 Fig 19.5: more or less severe cases selected by Z –&gt; Selection by missed predictor leads to miscalibration 19.0.6 Fig 19.6: more or less heterogeneous cases selected by Z –&gt; Selection by missed predictor has minor impact 19.1 Fig 19.7: Case-control design disturbs calibration Disturbance is exactly as expected by ratio of selecting cases:controls (log(2)) Select all cases, and half of the controls –&gt; same as shift in intercept 19.1.1 Fig 19.8: Overfitting disturbs discrimination and calibration 19.1.2 Fig 19.9: Different coeficients (model misspecification) disturbs discrimination and calibration 19.1.3 Scenarios: Table 19.4 Change of setting may especially impact calibration RCT vs survey may impact discrimination (more homogeneity) and calibration 19.1.4 Uncertainty in validation simulations "],["updating.html", "20 Updating for a New Setting", " 20 Updating for a New Setting Chapter 20 additional material upcoming. "],["updating-multiple.html", "21 Updating for Multiple Settings", " 21 Updating for Multiple Settings Chapter 21 additional material upcoming. "],["case-study-gusto.html", "22 Case Study on a Prediction of 30-Day Mortality", " 22 Case Study on a Prediction of 30-Day Mortality Chapter 22 additional material upcoming. "],["case-study-cardiovascular.html", "23 Case Study on Survival Analysis: Prediction of Cardiovascular Event", " 23 Case Study on Survival Analysis: Prediction of Cardiovascular Event Chapter 23 additional material upcoming. "],["lessons-datasets.html", "24 Overall Lessons and Data Sets", " 24 Overall Lessons and Data Sets Chapter 24 additional material upcoming. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
