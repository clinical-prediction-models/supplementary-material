[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clinical Prediction Models",
    "section": "",
    "text": "Preface\nWork in progress\nThis bookdown-based website containing the supplementary materials from ‘Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating’, by E. W. Steyerberg (2009).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Chapter 1 has no additional material.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-applications-pred-mods.html",
    "href": "chapters/02-applications-pred-mods.html",
    "title": "2  Applications of Prediction Models",
    "section": "",
    "text": "See here for additional material.",
    "crumbs": [
      "Part I: Prediction Models in Medicine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Applications of Prediction Models</span>"
    ]
  },
  {
    "objectID": "chapters/03-design.html",
    "href": "chapters/03-design.html",
    "title": "3  Study Design for Prediction Modeling",
    "section": "",
    "text": "Chapter 3 has no additional material.",
    "crumbs": [
      "Part I: Prediction Models in Medicine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Study Design for Prediction Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/04-stat-mods.html",
    "href": "chapters/04-stat-mods.html",
    "title": "4  Statistical Models for Prediction",
    "section": "",
    "text": "Chapter 4 has no additional material.",
    "crumbs": [
      "Part I: Prediction Models in Medicine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Models for Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/05-overfitting.html",
    "href": "chapters/05-overfitting.html",
    "title": "5  Overfitting and Optimism in Prediction Models",
    "section": "",
    "text": "Figures 5.2 to 5.5",
    "crumbs": [
      "Part I: Prediction Models in Medicine",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overfitting and Optimism in Prediction Models</span>"
    ]
  },
  {
    "objectID": "chapters/05-overfitting.html#figures-5.2-to-5.5",
    "href": "chapters/05-overfitting.html#figures-5.2-to-5.5",
    "title": "5  Overfitting and Optimism in Prediction Models",
    "section": "",
    "text": "Fig 5.2: Noise in estimating 10% mortality per center\n\n\nCode show/hide\n# Surg mortality; 10%\npar(mfrow = c(1, 1), mar = c(5, 5, 1, 1))\nfor (mort in c(.1)) { ## ,0.05,0.02,.01)) { # 4 mortalities or only 1\n  plot(\n    x = seq(from = -.025, to = .975, by = .05), dbinom(x = 0:20, 20, mort), axes = F, type = \"s\", lwd = 2,\n    xlim = c(-.05, .35), ylim = c(0, .33), col = mycolors[2],\n    xlab = paste(\"Observed mortality, true mortality \", round(100 * mort, 0), \"%\", sep = \"\"), ylab = \"probability density\"\n  )\n  axis(side = 1, at = c(0, .1, .2, .3), labels = c(\"0%\", \"10%\", \"20%\", \"30%\"))\n  axis(side = 2, at = c(0, 0.1, .2, .3, .4, .5, .6, .7), labels = c(\"0%\", \"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\"))\n  text(x = mort, y = .02 + dbinom(x = max(round(mort * 20), 0), 20, mort), labels = paste(\"n=20\"), col = mycolors[2])\n\n  for (i in c(50, 200)) { # add more sample sizes\n    lines(\n      x = seq(from = 0 - (0.5 * 1 / i), to = 1 - (0.5 * 1 / i), by = 1 / i), dbinom(x = 0:i, i, mort),\n      type = \"s\", lty = ifelse(i == 50, 2, 4), lwd = 2, col = mycolors[ifelse(i == 50, 3, 4)]\n    )\n    text(x = mort, y = .02 + dbinom(x = max(round(mort * i), 0), i, mort), labels = paste(\"n=\", i, sep = \"\"), col = mycolors[ifelse(i == 50, 3, 4)])\n  } # end loop n=50,200\n} # end loop mort\n\n\n\n\n\n\n\n\n\nCode show/hide\n## End Fig 5.2 ##\n\n\n\n\nCode show/hide\n## function for Fig 5.3 and Fig 5.4: Noise vs Heterogeneity\nillustrate_noise_heterogeneity &lt;- function(n = 20, mort = 0.1, tau = c(0, .01, .02, .03)) {\n  par(mfrow = c(2, 2), pty = \"m\", mar = c(2.5, 4, 1.5, 1))\n  # Make data set with 100 centers, each 20 patients, 10% mortality, variability sd 0 to 0.03\n  seedn &lt;- 102\n  set.seed(seedn)\n  ncenter &lt;- 50\n  nsubjects &lt;- n # n can be changed\n  # simple SD used on probability scale, can be improved upon\n  for (sdtau in tau) { # set for tau can be changed\n    truemort &lt;- rnorm(n = ncenter, mean = mort, sd = sdtau) # mort can be changed\n    mortmat &lt;- as.matrix(cbind(1:ncenter, sapply(truemort, FUN = function(x) rbinom(n = 1, nsubjects, x)) / nsubjects, truemort))\n\n    # Start plotting\n    plot(x = 0, y = 0, pch = \"\", xlim = c(-.2, 1.2), ylim = c(-.03, .35), axes = F, xlab = \"\", ylab = ifelse(sdtau == 0 | sdtau == .02, \"Mortality\", \"\"))\n    axis(side = 2, at = c(0, .1, .2, .3), labels = c(\"0%\", \"10%\", \"20%\", \"30%\"), las = 1)\n    axis(side = 1, at = c(0, 1), labels = c(\"Observed\", \"True mortality\"))\n    text(x = 1, y = .3, ifelse(sdtau == 0, \"No heterogeneity\",\n      ifelse(sdtau != 0, paste(\"+/-\", sdtau))\n    ), cex = 1, adj = 1, font = 2)\n    for (i in (1:ncenter)) {\n      set.seed(i + seedn)\n      lines(\n        x = c(0 + runif(1, min = -.07, max = .07), 1),\n        y = c(mortmat[i, 2] + runif(1, min = -.001, max = .01), mortmat[i, 3]), col = mycolors[rep(1:10, 10)[i]]\n      )\n      set.seed(i + seedn)\n      points(\n        x = c(0 + runif(1, min = -.07, max = .07), 1),\n        y = c(mortmat[i, 2] + runif(1, min = -.001, max = .01), mortmat[i, 3]), pch = c(\"o\", \"+\"), col = mycolors[rep(1:10, 10)[i]]\n      )\n    }\n  }\n} # end function that illustrates the impact of noise (determined by n) vs heterogeneity (determined by sdtau)\n\n\n\n\nFigs 5.3 and 5.4\nThese plots llustrate the impact of noise (determined by n, 20 or 200) vs heterogeneity (determined by sdtau (0 - 0.03)). With small n, such as n=20 per center, mortality such as 10% cannot be estimated reliably. Reliable estimation of a center’s performance requires a large n, such as n=200.\n\n\nn=20\n\n\n\n\n\n\n\n\n\n\n\nn=200",
    "crumbs": [
      "Part I: Prediction Models in Medicine",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overfitting and Optimism in Prediction Models</span>"
    ]
  },
  {
    "objectID": "chapters/06-choosing-alt-mods.html",
    "href": "chapters/06-choosing-alt-mods.html",
    "title": "6  Choosing Between Alternative Models",
    "section": "",
    "text": "Non-linearity illustrations",
    "crumbs": [
      "Part I: Prediction Models in Medicine",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choosing Between Alternative Models</span>"
    ]
  },
  {
    "objectID": "chapters/06-choosing-alt-mods.html#non-linearity-illustrations",
    "href": "chapters/06-choosing-alt-mods.html#non-linearity-illustrations",
    "title": "6  Choosing Between Alternative Models",
    "section": "",
    "text": "Prepare GUSTO data\nSome logistic regression fits with linear, square, rcs, linear spline terms\n\n\nCode show/hide\n# Import gusto; publicly available\nlist.files()\n\n\n [1] \"_freeze\"                       \"_quarto.yml\"                  \n [3] \"chapters\"                      \"clin-pred-mods_bookdown.Rproj\"\n [5] \"data\"                          \"docs\"                         \n [7] \"index.html\"                    \"index.qmd\"                    \n [9] \"R\"                             \"README.md\"                    \n[11] \"site_libs\"                    \n\n\nCode show/hide\ngusto &lt;- read.csv(\"data/gusto_age.csv\")[-1]\nFmort &lt;- as.data.frame(read.csv(\"data/Fmort.csv\"))[-1]\nFmort$age10 &lt;- Fmort$age / 10\nFmort$age102 &lt;- Fmort$age10^2\n\n\n\n\nAnova results for the different fits\nWe note minor differences between the continuous fits, and a clear loss of information for the dichtomization at age 65 years\n\n\nCode show/hide\nanova(agegusto.linear)\n\n\n                Wald Statistics          Response: DAY30 \n\n Factor     Chi-Square d.f. P     \n AGE        1728.89    1    &lt;.0001\n TOTAL      1728.89    1    &lt;.0001\n\n\nCode show/hide\nanova(agegusto.square)\n\n\n                Wald Statistics          Response: DAY30 \n\n Factor     Chi-Square d.f. P     \n AGE        1858.27    2    &lt;.0001\n  Nonlinear   13.21    1    3e-04 \n TOTAL      1858.27    2    &lt;.0001\n\n\nCode show/hide\nanova(agegusto.rcs)\n\n\n                Wald Statistics          Response: DAY30 \n\n Factor     Chi-Square d.f. P     \n AGE        1878.45    4    &lt;.0001\n  Nonlinear   24.71    3    &lt;.0001\n TOTAL      1878.45    4    &lt;.0001\n\n\nCode show/hide\nanova(agegusto.linearspline)\n\n\n                Wald Statistics          Response: DAY30 \n\n Factor     Chi-Square d.f. P     \n AGE        1846.73    2    &lt;.0001\n TOTAL      1846.73    2    &lt;.0001\n\n\nCode show/hide\nanova(agegusto.cat65)\n\n\n                Wald Statistics          Response: DAY30 \n\n Factor     Chi-Square d.f. P     \n AGE        1262.57    1    &lt;.0001\n TOTAL      1262.57    1    &lt;.0001\n\n\n\n\nPlotting of age effects\nPlot age effect first at lp scale (logodds), then at probability scale\nAge effect at logodds scale; Age effect at probability scale\n\n\n\n\n\n\n\n\n\nFig 6.1\n\n\nStart surgical mortality by age in Medicare\nAge effect at logodds scale\n\n\nAnova results for the fit of age, with interaction by type of surgery\nType of surgery is clearly most relevant (chi2 &gt;13500) in all fits. Age is als relevant (chi2&gt;3000), and a square term is not needed (chi2 = 2); the interaction adds a little bit (chi2 95). With these large numbers (1.1M patients), most effects have p&lt;.0001.\nWe will evaluate the differences between fits with or without interaction term graphically further down\n\n\nCode show/hide\n# Look for model improvements\nanova(fitplot2) # linear age effect, no interaction with surgery\n\n\n                Wald Statistics          Response: mort \n\n Factor     Chi-Square d.f. P     \n surgery    13500.19   13   &lt;.0001\n age         3167.14    1   &lt;.0001\n TOTAL      16445.99   14   &lt;.0001\n\n\nCode show/hide\nanova(fitage2) # age square added\n\n\n                Wald Statistics          Response: mort \n\n Factor     Chi-Square d.f. P     \n surgery    13499.66   13   &lt;.0001\n age10         18.13    1   &lt;.0001\n age102         2.33    1   0.127 \n TOTAL      16424.97   15   &lt;.0001\n\n\nCode show/hide\nanova(fitplot) # interaction added to linear age effect\n\n\n                Wald Statistics          Response: mort \n\n Factor                                       Chi-Square d.f. P     \n surgery  (Factor+Higher Order Factors)       13566.02   26   &lt;.0001\n  All Interactions                               94.55   13   &lt;.0001\n age  (Factor+Higher Order Factors)            3280.66   14   &lt;.0001\n  All Interactions                               94.55   13   &lt;.0001\n surgery * age  (Factor+Higher Order Factors)    94.55   13   &lt;.0001\n TOTAL                                        16620.27   27   &lt;.0001\n\n\n\n\nPlotting of predicted age effects, with interaction by type of surgery; add 95% CI\nPlot age effects at logodds scale with 95% CI\n\n\n\n\n\n\n\n\n\n\n\nPlotting of age effects with original data points\nFit with interaction (solid lines) and no interaction (dashed lines)",
    "crumbs": [
      "Part I: Prediction Models in Medicine",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choosing Between Alternative Models</span>"
    ]
  },
  {
    "objectID": "chapters/07-missing.html",
    "href": "chapters/07-missing.html",
    "title": "7  Missing values",
    "section": "",
    "text": "Figure and Table 7.1\nCode show/hide\n###################################\n# Ewout Steyerberg, Aug 18, 2018 #\n# Missing values: illustrate MCAR, MAR, MNAR mechanism\n# Use simple linear models\n###################################\n\nlibrary(rms) # Harrell's library with many useful functions\n\n#########################\n\nset.seed(1) # For identical results at repetition\nn &lt;- 10000 # arbitrary, large, sample size\n# n &lt;- 1000000           # used for book\n\nx2 &lt;- rnorm(n = n, mean = 0, sd = 1) # x2 standard normal\nx1 &lt;- rnorm(n = n, mean = 0, sd = 1) # Uncorrelated x1\n# x1   &lt;- sqrt(.5) * x2 + rnorm(n=n, mean=0, sd=sqrt(1-.5))  # x2 correlated with x1\n\ny1 &lt;- 1 * x1 + 1 * x2 + rnorm(n = n, mean = 0, sd = sqrt(1 - 0)) # generate y\n# var of y1 larger with correlated x1 - x2\n\nplot(x = x1, y = x2, pch = \".\", xlim = c(-4, 4), ylim = c(-4, 4), ps = .1)\nabline(ols(x2 ~ x1))\n\n\n\n\n\n\n\n\n\nCode show/hide\n# Make approx half missing\n# x1: MCAR, MAR and MNAR mechanisms\nx1MCAR &lt;- ifelse(runif(n) &lt; .5, x1, NA) # MCAR mechanism for 50% of x1\nx1MARx &lt;- ifelse(rnorm(n = n, sd = .8) &lt; x2, x1, NA) # MAR on x2, R2 50%, 50% missing (since mean x2==0)\nx1MARy &lt;- ifelse(rnorm(n = n, sd = (sqrt(3) * .8)) &gt; y1, x1, NA) # MAR on y, R2 50%, 50% missing (since mean y1==0)\nx1MNAR &lt;- ifelse(rnorm(n = n, sd = .8) &lt; x1, x1, NA) # MNAR on x1, R2 50%, 50% missing (since mean x1==0)\n\n# y1: MCAR, MAR and MNAR mechanisms\nyMCAR &lt;- ifelse(runif(n) &lt; .5, y1, NA) # MCAR mechanism for 50% of x1\nyMARx2 &lt;- ifelse(rnorm(n = n, sd = .8) &lt; x2, y1, NA) # MAR on x2, R2 39%, 50% missing (since mean x2==0)\nyMNAR &lt;- ifelse(rnorm(n = n, sd = .8) &lt; y1, y1, NA) # MNAR on x1, R2 50%, 50% missing (since mean x1==0)\n\n## Correlations ##\ncor(is.na(x1MCAR), x2)^2\n\n\n[1] 8.531179e-05\n\n\nCode show/hide\ncor(is.na(x1MARx), x2)^2\n\n\n[1] 0.3922284\n\n\nCode show/hide\ncor(is.na(x1MARy), y1)^2\n\n\n[1] 0.3905377\n\n\nCode show/hide\ncor(is.na(x1MNAR), x1)^2\n\n\n[1] 0.3909611\n\n\nCode show/hide\n# y\ncor(is.na(yMCAR), y1)^2\n\n\n[1] 1.549048e-08\n\n\nCode show/hide\ncor(is.na(yMARx2), x2)^2\n\n\n[1] 0.3924581\n\n\nCode show/hide\ncor(is.na(yMNAR), y1)^2\n\n\n[1] 0.5305992\n\n\nCode show/hide\n# End check correlations; 0.388 for those with some correlation, 0.52 for yMNAR\n\n### Fig 7.1 ###\n### Examine relation between x1 and x2\npar(pty = \"s\")\n# MCAR\nplot(x = x1[1:500], y = x2[1:500], pch = 4, xlim = c(-4, 4), ylim = c(-4, 4), cex = 1.2, xlab = \"x1\", ylab = \"x2\", col = \"darkgreen\") # Orig\nabline(ols(x2 ~ x1), lty = 2, lwd = 2, col = \"darkgreen\")\npoints(x = x1MCAR[1:500], y = x2[1:500], pch = \"o\", col = \"red\") # MCAR\nabline(ols(x2 ~ x1MCAR), lty = 1, lwd = 2, col = \"red\")\ntitle(\"a=0, b=0\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n# x1MARx\nplot(x = x1[1:500], y = x2[1:500], pch = 4, xlim = c(-4, 4), ylim = c(-4, 4), cex = 1.2, xlab = \"x1\", ylab = \"x2\", col = \"darkgreen\") # Orig\nabline(ols(x2 ~ x1), lty = 2, col = \"darkgreen\")\npoints(x = x1MARx[1:500], y = x2[1:500], pch = \"o\", col = \"red\") # MCAR\nabline(ols(x2 ~ x1MARx), lty = 1, lwd = 2, col = \"red\")\ntitle(\"a=0.6, b=0\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n# x1MARy\nplot(x = x1[1:500], y = x2[1:500], pch = 4, xlim = c(-4, 4), ylim = c(-4, 4), cex = 1.2, xlab = \"x1\", ylab = \"x2\", col = \"darkgreen\") # Orig\nabline(ols(x2 ~ x1), lty = 2, col = \"darkgreen\")\npoints(x = x1MARy[1:500], y = x2[1:500], pch = \"o\", col = \"red\") # MCAR\nabline(ols(x2 ~ x1MARy), lty = 1, lwd = 2, col = \"red\")\ntitle(\"a=-0.4, b=-0.1\", cex.main = 2) # -.413, -.149\n\n\n\n\n\n\n\n\n\nCode show/hide\n# x1MNAR\nplot(x = x1[1:500], y = x2[1:500], pch = 4, xlim = c(-4, 4), ylim = c(-4, 4), cex = 1.2, xlab = \"x1\", ylab = \"x2\", col = \"darkgreen\") # Orig\nabline(ols(x2 ~ x1), lty = 2, col = \"darkgreen\")\npoints(x = x1MNAR[1:500], y = x2[1:500], pch = \"o\", col = \"red\") # MCAR\nabline(ols(x2 ~ x1MNAR), lty = 1, lwd = 2, col = \"red\")\ntitle(\"a=0, b=0\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n## y ##\n# y MCAR\nplot(x = x1[1:500], y = x2[1:500], pch = 4, xlim = c(-4, 4), ylim = c(-4, 4), cex = 1.2, xlab = \"x1\", ylab = \"x2\", col = \"darkgreen\") # Orig\nabline(ols(x2 ~ x1), lty = 2, col = \"darkgreen\")\npoints(x = x1[!is.na(yMCAR)][1:250], y = x2[!is.na(yMCAR)][1:250], pch = \"o\", col = \"red\") # yMCAR\nabline(ols(x2[!is.na(yMCAR)][1:10000] ~ x1[!is.na(yMCAR)][1:10000]), lty = 1, lwd = 2, col = \"red\")\ntitle(\"a=0, b=0\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n# y MAR\nplot(x = x1[1:500], y = x2[1:500], pch = 4, xlim = c(-4, 4), ylim = c(-4, 4), cex = 1.2, xlab = \"x1\", ylab = \"x2\", col = \"darkgreen\") # Orig\nabline(ols(x2 ~ x1), lty = 2, col = \"darkgreen\")\npoints(x = x1[!is.na(yMARx2)][1:250], y = x2[!is.na(yMARx2)][1:250], pch = \"o\", col = \"red\") # yMCAR\nabline(ols(x2[!is.na(yMARx2)][1:10000] ~ x1[!is.na(yMARx2)][1:10000]), lty = 1, lwd = 2, col = \"red\")\ntitle(\"a=0.6, b=0\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n# y MNAR\nplot(x = x1[1:500], y = x2[1:500], pch = 4, xlim = c(-4, 4), ylim = c(-4, 4), cex = 1.2, xlab = \"x1\", ylab = \"x2\", col = \"darkgreen\") # Orig\nabline(ols(x2 ~ x1), lty = 2, col = \"darkgreen\")\npoints(x = x1[!is.na(yMNAR)][1:250], y = x2[!is.na(yMNAR)][1:250], pch = \"o\", col = \"red\") # yMCAR\nabline(ols(x2[!is.na(yMNAR)][1:10000] ~ x1[!is.na(yMNAR)][1:10000]), lty = 1, lwd = 2, col = \"red\")\ntitle(\"a=0.5, b=0.2\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n### End Fig 7.1 ###\n\n## Table 7.1 ##\nmean(x1MARy, na.rm = T)\n\n\n[1] -0.3579431\n\n\nCode show/hide\nmean(x2[!is.na(x1MARy)], na.rm = T)\n\n\n[1] -0.3703634\n\n\nCode show/hide\nmean(x1MNAR, na.rm = T)\n\n\n[1] 0.6202571\n\n\nCode show/hide\n# y MNAR\nmean(yMNAR, na.rm = T)\n\n\n[1] 1.266413\n\n\nCode show/hide\nmean(x1[!is.na(yMNAR)], na.rm = T)\n\n\n[1] 0.4039278\n\n\nCode show/hide\nmean(x2[!is.na(yMNAR)], na.rm = T)\n\n\n[1] 0.4216802",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "chapters/07-missing.html#figure-7.2",
    "href": "chapters/07-missing.html#figure-7.2",
    "title": "7  Missing values",
    "section": "Figure 7.2",
    "text": "Figure 7.2\n\n\nCode show/hide\n## Fig 7.2 ##\n\n############################################################\n## Make plot function for graphs with y~x1; 4 with y~x2 ##\n## First for missing x1, then missing y\n############################################################\n\nrefx1 &lt;- ols(y1 ~ x1)\nrefx2 &lt;- ols(y1 ~ x2)\nnsel &lt;- 500\n\nCCplot &lt;- function(x1, y1, sel, reffit, lab, xlab = \"x\", ylab = \"y\", nsel1 = 400, nsel2 = 200, nsel3 = 800) {\n  par(pty = \"s\")\n  plot(x = x1[1:nsel], y = y1[1:nsel], pch = 3, xlim = c(-4, 4), ylim = c(-6, 6), cex = 1.2, col = \"darkgreen\", xlab = xlab, ylab = ylab) # Orig\n  abline(reffit, lty = 2, lwd = 2, col = \"darkgreen\")\n  points(x = x1[!is.na(sel)][1:nsel2], y = y1[!is.na(sel)][1:nsel2], pch = \"o\", cex = 1, col = \"red\") # Missings\n  y1_bis &lt;- y1[!is.na(sel)][1:nsel3]\n  x1_bis &lt;- x1[!is.na(sel)][1:nsel3]\n  abline(ols(y1_bis ~ x1_bis), lty = 1, lwd = 2, col = \"red\")\n} # end function\n\n# MCAR\nCCplot(x1 = x1, y1 = y1, sel = x1MCAR, reffit = refx1, lab = \"x1 MCAR\", xlab = \"x1\")\ntitle(\"CC: a=0, b1=1\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\nCCplot(x1 = x2, y1 = y1, sel = x1MCAR, reffit = refx2, lab = \"\", xlab = \"x2\", nsel1 = 400, nsel2 = 200, nsel3 = 1000)\ntitle(\"CC: a=0, b2=1\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n# MAR on x\nCCplot(x1 = x1, y1 = y1, sel = x1MARx, reffit = refx1, lab = \"x1 MAR on x2\", xlab = \"x1\", nsel3 = 10000)\ntitle(\"CC: a=0.6, b1=1\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\nCCplot(x1 = x2, y1 = y1, sel = x1MARx, reffit = refx2, lab = \"\", xlab = \"x2\", nsel1 = 400, nsel2 = 200, nsel3 = 10000)\ntitle(\"CC: a=0, b2=1\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n# MAR on y\nCCplot(x1 = x1, y1 = y1, sel = x1MARy, reffit = refx1, lab = \"x1 MAR on y\", xlab = \"x1\", nsel3 = 10000)\ntitle(\"CC: a=-0.6, b1=0.7\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\nCCplot(x1 = x2, y1 = y1, sel = x1MARy, reffit = refx2, lab = \"\", xlab = \"x2\", nsel1 = 400, nsel2 = 200, nsel3 = 10000)\ntitle(\"CC: a=-0.6, b2=0.7\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n# x1MNAR\nCCplot(x1 = x1, y1 = y1, sel = x1MNAR, reffit = refx1, lab = \"x1 MNAR\", xlab = \"x1\", nsel3 = 3000)\ntitle(\"CC: a=0, b1=1\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\nCCplot(x1 = x2, y1 = y1, sel = x1MNAR, reffit = refx2, lab = \"\", xlab = \"x2\", nsel1 = 400, nsel2 = 200, nsel3 = 10000)\ntitle(\"CC: a=0.6, b2=1\", cex.main = 2)\n\n\n\n\n\n\n\n\n\nCode show/hide\n## end X ##\n## End Fig 7.2 ##\n\n\n##### Extra Fig #####\n##### Missing y patterns ####\n# y MCAR\nCCplot(x1 = x1, y1 = y1, sel = yMCAR, reffit = refx1, lab = \"y MCAR\", xlab = \"x1\", nsel3 = 1000)\nlegend(\"topleft\", legend = c(\"Complete data\", \"Missings, CC:\\na=0, b1=1\"), lty = 2:1, lwd = c(2, 2))\n\n\n\n\n\n\n\n\n\nCode show/hide\nCCplot(x1 = x2, y1 = y1, sel = yMCAR, reffit = refx2, lab = \"\", xlab = \"x2\", nsel1 = 400, nsel2 = 200, nsel3 = 1000)\nlegend(\"topleft\", legend = c(\"Complete data\", \"Missings, CC:\\na=0, b2=1\"), lty = 2:1, lwd = c(2, 2))\n\n\n\n\n\n\n\n\n\nCode show/hide\n# yMARx\nCCplot(x1 = x1, y1 = y1, sel = yMARx2, reffit = refx1, lab = \"y MAR on x2\", xlab = \"x1\", nsel3 = 1000)\nlegend(\"topleft\", legend = c(\"Complete data\", \"Missings, CC:\\na=0.6, b1=1\"), lty = 2:1, lwd = c(2, 2))\n\n\n\n\n\n\n\n\n\nCode show/hide\nCCplot(x1 = x2, y1 = y1, sel = yMARx2, reffit = refx2, lab = \"\", xlab = \"x2\", nsel1 = 400, nsel2 = 200, nsel3 = 1000)\nlegend(\"topleft\", legend = c(\"Complete data\", \"Missings, CC:\\na=0, b2=1\"), lty = 2:1, lwd = c(2, 2))\n\n\n\n\n\n\n\n\n\nCode show/hide\n# yMNAR\nCCplot(x1 = x1, y1 = y1, sel = yMNAR, reffit = refx1, lab = \"y MNAR\", xlab = \"x1\", nsel3 = 1000)\nlegend(\"topleft\", legend = c(\"Complete data\", \"Missings, CC:\\na=1.0, b1=0.6\"), lty = 2:1, lwd = c(2, 2))\n\n\n\n\n\n\n\n\n\nCode show/hide\nCCplot(x1 = x2, y1 = y1, sel = yMNAR, reffit = refx2, lab = \"\", xlab = \"x2\", nsel1 = 400, nsel2 = 200, nsel3 = 1000)\nlegend(\"topleft\", legend = c(\"Complete data\", \"Missings, CC:\\na=1.0, b2=0.6\"), lty = 2:1, lwd = c(2, 2))\n\n\n\n\n\n\n\n\n\nCode show/hide\nplot(x = x1[1:1000], y = y1[1:1000], pch = \".\", xlim = c(-4, 4), ylim = c(-6, 6), ps = .1) # Orig\nabline(ols(y1 ~ x1 + x2), lty = 2)\npoints(x = x1, y = yMNAR, pch = \"o\", ps = .1) # MCAR\nabline(ols(yMNAR ~ x1), lty = 1, lwd = 2)\ntitle(\"y MNAR on y\")\n\n\n\n\n\n\n\n\n\nCode show/hide\n## end y ##",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "chapters/07-missing.html#table-7.4",
    "href": "chapters/07-missing.html#table-7.4",
    "title": "7  Missing values",
    "section": "Table 7.4",
    "text": "Table 7.4\n\n\nCode show/hide\n## Table 7.4; requires large N ##\n# Describe the 7 missing value patterns, impact on x1 and x2\nmissing.descx &lt;- function(x1, x2, digits = 3) {\n  x2 &lt;- x2[!is.na(x1)]\n  print(c(mean(x1, na.rm = T), sd(x1, na.rm = T), mean(x2, na.rm = T), sd(x2, na.rm = T)), digits = digits)\n} # end function\nmissing.descx(x1 = x1MCAR, x2 = x2)\n\n\n[1] -0.00326  1.00187  0.00287  1.00883\n\n\nCode show/hide\nmissing.descx(x1 = x1MARx, x2 = x2)\n\n\n[1] -0.0237  0.9855  0.6324  0.7857\n\n\nCode show/hide\nmissing.descx(x1 = x1MARy, x2 = x2)\n\n\n[1] -0.358  0.914 -0.370  0.956\n\n\nCode show/hide\nmissing.descx(x1 = x1MNAR, x2 = x2)\n\n\n[1] 0.62026 0.79132 0.00461 1.00280\n\n\nCode show/hide\n## y\nmissing.descy &lt;- function(x1, x2, y, digits = 3) {\n  x1 &lt;- x1[!is.na(y)]\n  x2 &lt;- x2[!is.na(y)]\n  print(c(mean(x1, na.rm = T), sd(x1, na.rm = T), mean(x2, na.rm = T), sd(x2, na.rm = T)), digits = digits)\n}\nmissing.descy(x1 = x1, x2 = x2, y = yMCAR)\n\n\n[1] -0.00347  0.99201 -0.00431  1.01040\n\n\nCode show/hide\nmissing.descy(x1 = x1, x2 = x2, y = yMARx2)\n\n\n[1] -0.0257  0.9883  0.6366  0.7855\n\n\nCode show/hide\nmissing.descy(x1 = x1, x2 = x2, y = yMNAR)\n\n\n[1] 0.404 0.915 0.422 0.914\n\n\nCode show/hide\nprint(rbind(\n  missing.descx(x1 = x1MCAR, x2 = x2),\n  missing.descx(x1 = x1MARx, x2 = x2),\n  missing.descx(x1 = x1MARy, x2 = x2),\n  missing.descx(x1 = x1MNAR, x2 = x2),\n  missing.descy(x1 = x1, x2 = x2, y = yMCAR),\n  missing.descy(x1 = x1, x2 = x2, y = yMARx2),\n  missing.descy(x1 = x1, x2 = x2, y = yMNAR)\n), digits = 2)\n\n\n[1] -0.00326  1.00187  0.00287  1.00883\n[1] -0.0237  0.9855  0.6324  0.7857\n[1] -0.358  0.914 -0.370  0.956\n[1] 0.62026 0.79132 0.00461 1.00280\n[1] -0.00347  0.99201 -0.00431  1.01040\n[1] -0.0257  0.9883  0.6366  0.7855\n[1] 0.404 0.915 0.422 0.914\n        [,1] [,2]    [,3] [,4]\n[1,] -0.0033 1.00  0.0029 1.01\n[2,] -0.0237 0.99  0.6324 0.79\n[3,] -0.3579 0.91 -0.3704 0.96\n[4,]  0.6203 0.79  0.0046 1.00\n[5,] -0.0035 0.99 -0.0043 1.01\n[6,] -0.0257 0.99  0.6366 0.79\n[7,]  0.4039 0.92  0.4217 0.91\n\n\nCode show/hide\n## End descriptives Table 7.4 ##",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "chapters/07-missing.html#table-7.5",
    "href": "chapters/07-missing.html#table-7.5",
    "title": "7  Missing values",
    "section": "Table 7.5",
    "text": "Table 7.5\n\n\nCode show/hide\n##############################################\n### Table 7.5 Compare fits in the various selections\n## only x1\nfx1.CC &lt;- ols(y1 ~ x1)\nfx1MCAR.CC &lt;- ols(y1 ~ x1MCAR)\nfx1MARx.CC &lt;- ols(y1 ~ x1MARx)\nfx1MARy.CC &lt;- ols(y1 ~ x1MARy)\nfx1MNAR.CC &lt;- ols(y1 ~ x1MNAR)\n\nfy.CC &lt;- ols(yMCAR ~ x1)\nfyMARx.CC &lt;- ols(yMARx2 ~ x1)\nfyMNAR.CC &lt;- ols(yMNAR ~ x1)\n\n# Regression coefficients\nprint(rbind(\n  coef(fx1.CC),\n  coef(fx1MCAR.CC), coef(fx1MARx.CC), coef(fx1MARy.CC), coef(fx1MNAR.CC),\n  coef(fy.CC), coef(fyMARx.CC), coef(fyMNAR.CC)\n), digits = 2)\n\n\n## only x2\n\nfx1.CC &lt;- ols(y1 ~ x2)\nfx1MCAR.CC &lt;- ols(y1[!is.na(x1MCAR)] ~ x2[!is.na(x1MCAR)])\nfx1MARx.CC &lt;- ols(y1[!is.na(x1MARx)] ~ x2[!is.na(x1MARx)])\nfx1MARy.CC &lt;- ols(y1[!is.na(x1MARy)] ~ x2[!is.na(x1MARy)])\nfx1MNAR.CC &lt;- ols(y1[!is.na(x1MNAR)] ~ x2[!is.na(x1MNAR)])\n\nfy.CC &lt;- ols(yMCAR ~ x2)\nfyMARx.CC &lt;- ols(yMARx2 ~ x2)\nfyMNAR.CC &lt;- ols(yMNAR ~ x2)\n\n# Regression coefficients\nprint(rbind(\n  coef(fx1.CC),\n  coef(fx1MCAR.CC), coef(fx1MARx.CC), coef(fx1MARy.CC), coef(fx1MNAR.CC),\n  coef(fy.CC), coef(fyMARx.CC), coef(fyMNAR.CC)\n), digits = 2)\n\n\n# x1 + x2\nfx1.CC &lt;- ols(y1 ~ x1 + x2)\nfx1MCAR.CC &lt;- ols(y1 ~ x1MCAR + x2)\nfx1MARx.CC &lt;- ols(y1 ~ x1MARx + x2)\nfx1MARy.CC &lt;- ols(y1 ~ x1MARy + x2)\nfx1MNAR.CC &lt;- ols(y1 ~ x1MNAR + x2)\n\nfy.CC &lt;- ols(yMCAR ~ x1 + x2)\nfyMARx.CC &lt;- ols(yMARx2 ~ x1 + x2)\nfyMNAR.CC &lt;- ols(yMNAR ~ x1 + x2)",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "chapters/07-missing.html#table-7.6",
    "href": "chapters/07-missing.html#table-7.6",
    "title": "7  Missing values",
    "section": "Table 7.6",
    "text": "Table 7.6\n\n\nCode show/hide\n## Table 7.6 ##\n# Regression coefficients and R2\nprint(rbind(\n  c(coef(fx1.CC), fx1.CC$stats[\"R2\"]),\n  c(coef(fx1MCAR.CC), fx1MCAR.CC$stats[\"R2\"]),\n  c(coef(fx1MARx.CC), fx1MARx.CC$stats[\"R2\"]),\n  c(coef(fx1MARy.CC), fx1MARy.CC$stats[\"R2\"]),\n  c(coef(fx1MNAR.CC), fx1MNAR.CC$stats[\"R2\"]),\n  c(coef(fy.CC), fy.CC$stats[\"R2\"]),\n  c(coef(fyMARx.CC), fyMARx.CC$stats[\"R2\"]),\n  c(coef(fyMNAR.CC), fyMNAR.CC$stats[\"R2\"])\n),\ndigits = 2\n)\n\n\n# Imputation; make data sets with different types of missings\nd &lt;- as.data.frame(cbind(y1, x1, x2, x1MCAR, x1MARx, x1MARy, x1MNAR))\nd &lt;- d[1:500000, ]\ndMCAR &lt;- d[, c(1, 3, 4)] # data set with x1 missings according to MCAR\ndMARx &lt;- d[, c(1, 3, 5)] # x1 MAR on x2\ndMARy &lt;- d[, c(1, 3, 6)] # x1 MAR on y\ndMNAR &lt;- d[, c(1, 3, 7)] # x1 MNAR at x1\n\ndy &lt;- as.data.frame(cbind(y1, x1, x2, yMCAR, yMARx2, yMNAR))\ndy &lt;- dy[1:500000, ]\ndyMCAR &lt;- d[, c(2, 3, 4)] # data set with y missing MCAR\ndyMARx &lt;- d[, c(2, 3, 5)] # x1 MAR on x2\ndyMNAR &lt;- d[, c(2, 3, 6)] # x1 MAR on x2\n\n## Imputation using the aregImpute function from rms\nm &lt;- 1 # to be changed\ng &lt;- aregImpute(~ y1 + x1MCAR + x2, n.impute = m, data = d, pr = F, type = \"pmm\")\n## fit models per imputed set and combine results using Rubin's rules\n## Use fit.mult.impute() function from rms\nfx1MCAR.MI.u &lt;- fit.mult.impute(y1 ~ x1MCAR, ols, xtrans = g, data = d, pr = F)\nfx2MCAR.MI.u &lt;- fit.mult.impute(y1 ~ x2, ols, xtrans = g, data = d, pr = F)\nfx1MCAR.MI &lt;- fit.mult.impute(y1 ~ x1MCAR + x2, ols, xtrans = g, data = d, pr = F)\n\ng &lt;- aregImpute(~ y1 + x1MARx + x2, n.impute = m, data = d, pr = F, type = \"pmm\")\nfx1MARx.MI.u &lt;- fit.mult.impute(y1 ~ x1MARx, ols, xtrans = g, data = d, pr = F)\nfx2MARx.MI.u &lt;- fit.mult.impute(y1 ~ x2, ols, xtrans = g, data = d, pr = F)\nfx1MARx.MI &lt;- fit.mult.impute(y1 ~ x1MARx + x2, ols, xtrans = g, data = d, pr = F) ## areg\n\ng &lt;- aregImpute(~ y1 + x1MARy + x2, n.impute = m, data = d, pr = F, type = \"pmm\")\nfx1MARy.MI.u &lt;- fit.mult.impute(y1 ~ x1MARy, ols, xtrans = g, data = d, pr = F)\nfx2MARy.MI.u &lt;- fit.mult.impute(y1 ~ x2, ols, xtrans = g, data = d, pr = F)\nfx1MARy.MI &lt;- fit.mult.impute(y1 ~ x1MARy + x2, ols, xtrans = g, data = d, pr = F) ## areg\n\ng &lt;- aregImpute(~ y1 + x1MNAR + x2, n.impute = m, data = d, pr = F, type = \"pmm\")\nfx1MNAR.MI.u &lt;- fit.mult.impute(y1 ~ x1MNAR, ols, xtrans = g, data = d, pr = F)\nfx2MNAR.MI.u &lt;- fit.mult.impute(y1 ~ x2, ols, xtrans = g, data = d, pr = F)\nfx1MNAR.MI &lt;- fit.mult.impute(y1 ~ x1MNAR + x2, ols, xtrans = g, data = d, pr = F) ## areg\n\n# Regression coefficients after MI with aregImpute\nprint(rbind(\n  coef(fx1MCAR.MI.u), coef(fx2MCAR.MI.u), coef(fx1MCAR.MI),\n  coef(fx1MARx.MI.u), coef(fx2MARx.MI.u), coef(fx1MARx.MI),\n  coef(fx1MARy.MI.u), coef(fx2MARy.MI.u), coef(fx1MARy.MI),\n  coef(fx1MNAR.MI.u), coef(fx2MNAR.MI.u), coef(fx1MNAR.MI)\n),\ndigits = 3\n)\n\n# Table 7.6 #\n# Regression coefficients and performqnce of x1+x2 model after MI with aregImpute\nprint(rbind(\n  c(coef(fx1MCAR.MI), fx1MCAR.MI$stats[\"R2\"]),\n  c(coef(fx1MARx.MI), fx1MARx.MI$stats[\"R2\"]),\n  c(coef(fx1MARy.MI), fx1MARy.MI$stats[\"R2\"]),\n  c(coef(fx1MNAR.MI), fx1MNAR.MI$stats[\"R2\"])\n),\ndigits = 3\n)\n\n## End x, start y ##\ng &lt;- aregImpute(~ yMCAR + x1 + x2, n.impute = m, data = dy, pr = F, type = \"pmm\")\nfy.x1.MCAR.MI.u &lt;- fit.mult.impute(yMCAR ~ x1, ols, xtrans = g, data = dy, pr = F)\nfy.x2.MCAR.MI.u &lt;- fit.mult.impute(yMCAR ~ x2, ols, xtrans = g, data = dy, pr = F)\nfyMCAR.MI &lt;- fit.mult.impute(yMCAR ~ x1 + x2, ols, xtrans = g, data = dy, pr = F)\n\ng &lt;- aregImpute(~ yMARx2 + x1 + x2, n.impute = m, data = dy, pr = F, type = \"pmm\")\nfy.x1.MAR.MI.u &lt;- fit.mult.impute(yMARx2 ~ x1, ols, xtrans = g, data = dy, pr = F)\nfy.x2.MAR.MI.u &lt;- fit.mult.impute(yMARx2 ~ x2, ols, xtrans = g, data = dy, pr = F)\nfyMAR.MI &lt;- fit.mult.impute(yMARx2 ~ x1 + x2, ols, xtrans = g, data = dy, pr = F) ## areg\n\ng &lt;- aregImpute(~ yMNAR + x1 + x2, n.impute = m, data = dy, pr = F, type = \"pmm\")\nfy.x1.MNAR.MI.u &lt;- fit.mult.impute(yMNAR ~ x1, ols, xtrans = g, data = dy, pr = F)\nfy.x2.MNAR.MI.u &lt;- fit.mult.impute(yMNAR ~ x2, ols, xtrans = g, data = dy, pr = F)\nfyMNAR.MI &lt;- fit.mult.impute(yMNAR ~ x1 + x2, ols, xtrans = g, data = dy, pr = F) ## areg\n\n# Regression coefficients after MI with aregImpute\nprint(rbind(\n  coef(fy.x1.MCAR.MI.u), coef(fy.x2.MCAR.MI.u), coef(fx1MCAR.MI),\n  coef(fy.x1.MAR.MI.u), coef(fy.x2.MAR.MI.u), coef(fyMCAR.MI),\n  coef(fy.x1.MNAR.MI.u), coef(fy.x2.MNAR.MI.u), coef(fyMNAR.MI)\n),\ndigits = 3\n)\n\n# Regression coefficients and performance after MI with aregImpute\nprint(rbind(\n  c(coef(fyMCAR.MI), fyMCAR.MI$stats[\"R2\"]),\n  c(coef(fyMAR.MI), fyMAR.MI$stats[\"R2\"]),\n  c(coef(fyMNAR.MI), fyMNAR.MI$stats[\"R2\"])\n),\ndigits = 3\n)\n\n\n#### End Table 7.6 illustration missing values and imputation ####\n\n\n#### Repeat for binary outcomes ####\nlibrary(mice)\nmd.pattern(dMNAR)\nna.pattern(dMNAR)\nnaclus(dMNAR)\n\nset.seed(1) # For identical results at repetition\nn &lt;- 100000 # arbitrary sample size\nx2 &lt;- rnorm(n = n, mean = 0, sd = 1) # x2 standard normal\nx1 &lt;- rnorm(n = n, mean = 0, sd = 1) # Uncorrelated x1\ny1.bin &lt;- rbinom(n, 1, plogis(1 * x1 + 1 * x2)) # generate y\nx1MCAR &lt;- ifelse(runif(n) &lt; .5, x1, NA) # MCAR mechanism for 50% of x1\nx1MARx &lt;- ifelse(rnorm(n = n, sd = .8) &lt; x2, x1, NA) # MAR on x2, R2 50%, 50% missing (since mean x2==0)\nx1MARy &lt;- ifelse(rbinom(n, 1, .8 * y1.bin) == 1 | rbinom(n, 1, .2 * (1 - y1.bin)) == 1, NA, x1) # MAR on y, R2 50%, 50% missing (since mean y1==0)\nx1MNAR &lt;- ifelse(rnorm(n = n, sd = .8) &lt; x1, x1, NA) # MNAR on x1, R2 50%, 50% missing (since mean x1==0)\n\nmean(is.na(x1MARy)[y1.bin == 0])\nmean(is.na(x1MARy)[y1.bin == 1])\n\nlrm(y1.bin ~ x1 + x2)\nlrm(y1.bin ~ x1MCAR + x2)\nlrm(y1.bin ~ x1MARx + x2)\nlrm(y1.bin ~ x1MARy + x2)\nlrm(y1.bin ~ x1MNAR + x2)\n\n# Compare fits in the various selections\nfx1.CC &lt;- lrm(y1.bin ~ x1 + x2)\nfx1MARx.CC &lt;- lrm(y1.bin ~ x1MARx + x2)\nfx1MARy.CC &lt;- lrm(y1.bin ~ x1MARy + x2)\nfx1MNAR.CC &lt;- lrm(y1.bin ~ x1MNAR + x2)\n\n# Regression coefficients\nprint(rbind(\n  coef(fx1.CC), coef(fx1MARx.CC),\n  coef(fx1MARy.CC), coef(fx1MNAR.CC)\n), digits = 3)\n\nprint(rbind(\n  sqrt(diag(fx1.CC$var)), sqrt(diag(fx1MARx.CC$var)),\n  sqrt(diag(fx1MARy.CC$var)), sqrt(diag(fx1MNAR.CC$var))\n), digits = 3)\n\n# Imputation; make data sets with different types of missings\nd &lt;- as.data.frame(cbind(y1.bin, x1, x2, x1MCAR, x1MARx, x1MARy, x1MNAR))\ndMCAR &lt;- d[, c(1, 3, 4)] # data set with x1 missings according to MCAR\ndMARx &lt;- d[, c(1, 3, 5)] # x1 MAR on x2\ndMARy &lt;- d[, c(1, 3, 6)] # x1 MAR on y\ndMNAR &lt;- d[, c(1, 3, 7)] # x1 MNAR at x1\n\n## MI using the aregImpute function from rms\n## help(areImpute) for more info\ng &lt;- aregImpute(~ y1.bin + x1MCAR + x2, n.impute = 5, data = d, pr = F, type = \"pmm\")\n## fit models per imputed set and combine results using Rubin's rules\n## Use fit.mult.impute() function from rms\nfx1MCAR.MI &lt;- fit.mult.impute(y1.bin ~ x1MCAR + x2, lrm, xtrans = g, data = d, pr = F)\n\ng &lt;- aregImpute(~ y1.bin + x1MARx + x2, n.impute = 5, data = d, pr = F, type = \"pmm\")\nfx1MARx.MI &lt;- fit.mult.impute(y1.bin ~ x1MARx + x2, lrm, xtrans = g, data = d, pr = F) ## areg\n\ng &lt;- aregImpute(~ y1.bin + x1MARy + x2, n.impute = 5, data = d, pr = F, type = \"pmm\")\nfx1MARy.MI &lt;- fit.mult.impute(y1.bin ~ x1MARy + x2, lrm, xtrans = g, data = d, pr = F) ## areg\n\ng &lt;- aregImpute(~ y1.bin + x1MNAR + x2, n.impute = 5, data = d, pr = F, type = \"pmm\")\nfx1MNAR.MI &lt;- fit.mult.impute(y1.bin ~ x1MNAR + x2, lrm, xtrans = g, data = d, pr = F) ## areg\n\n# Regression coefficients after MI with aregImpute\nprint(rbind(\n  coef(fx1MCAR.MI), coef(fx1MARx.MI),\n  coef(fx1MARy.MI), coef(fx1MNAR.MI)\n), digits = 3)\n\nprint(rbind(\n  sqrt(diag(fx1MCAR.MI$var)), sqrt(diag(fx1MARx.MI$var)),\n  sqrt(diag(fx1MARy.MI$var)), sqrt(diag(fx1MNAR.MI$var))\n), digits = 3)",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "chapters/08-case-study-missing.html",
    "href": "chapters/08-case-study-missing.html",
    "title": "8  Case Study on Dealing with Missing Values",
    "section": "",
    "text": "Chapter 8 additional material upcoming.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study on Dealing with Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/09-coding-categ-contin.html",
    "href": "chapters/09-coding-categ-contin.html",
    "title": "9  Coding of Categorical and Continuous Predictors",
    "section": "",
    "text": "Figues 9.1 to 9.6\nGUSTO-I is a data set with patients suffering from an acute myocardial infarction, where we want to predict 30-day mortality. TBI is a data set with patients suffering from a moderate or severe traumatic brain injury.\nCode show/hide\n# Import gusto\ngusto &lt;- read.csv(\"data/gusto_age_STE.csv\")[, -1]\n\n# Import sample4; n=785\ngustos &lt;- read.csv(\"data/Gustos4Age.csv\")\n\n# Import TBI data; n=2159\nTBI &lt;- read.csv(\"data/TBI2vars.csv\")",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Coding of Categorical and Continuous Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/09-coding-categ-contin.html#figues-9.1-to-9.6",
    "href": "chapters/09-coding-categ-contin.html#figues-9.1-to-9.6",
    "title": "9  Coding of Categorical and Continuous Predictors",
    "section": "",
    "text": "Fig 9.1: Age linear; add square; rcs in GUSTO-I\n\n\nCode show/hide\nagegusto.linear &lt;- lrm(DAY30 ~ AGE, data = gusto, x = T, y = T)\nagegusto.square &lt;- lrm(DAY30 ~ pol(AGE, 2), data = gusto, x = T, y = T)\nagegusto.rcs &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gusto, x = T, y = T)\n## dichotomize\nagegusto.cat65 &lt;- lrm(DAY30 ~ ifelse(AGE &lt; 65, 0, 1), data = gusto, x = T, y = T)\n## 3 categories\nagegusto.3cat &lt;- lrm(DAY30 ~ ifelse(AGE &lt; 60, 0, ifelse(AGE &lt; 70, 1, 2)), data = gusto, x = T, y = T)\n\n# Predict for age 20:95\nnewdata.age &lt;- data.frame(\"AGE\" = seq(20, 95, by = 0.1))\npred.agegusto.linear &lt;- predict(agegusto.linear, newdata.age)\npred.agegusto.square &lt;- predict(agegusto.square, newdata.age)\npred.agegusto.rcs &lt;- predict(agegusto.rcs, newdata.age)\npred.agegusto.cat65 &lt;- predict(agegusto.cat65, newdata.age)\npred.agegusto.3cat &lt;- predict(agegusto.3cat, newdata.age)\n\n# Make plot\ndd &lt;- datadist(gusto)\noptions(datadist = \"dd\") # for rms\npar(mfrow = c(1, 1))\nplot(\n  x = newdata.age[, 1], y = pred.agegusto.linear, xlim = c(20, 92), ylim = c(-7, 0), las = 1, xaxt = \"n\",\n  xlab = \"Age in years\", ylab = \"logit of 30-day mortality\", cex.lab = 1.2, type = \"l\", lwd = 2, col = mycolors[2]\n)\naxis(1, at = c(40, 50, 60, 65, 70, 80, 90))\nlines(x = newdata.age[, 1], y = pred.agegusto.cat65, lty = 2, lwd = 2, col = mycolors[3])\nlines(x = newdata.age[, 1], y = pred.agegusto.3cat, lty = 3, lwd = 2, col = mycolors[4])\nscat1d(x = gusto$AGE, side = 1, frac = .05, col = \"darkblue\")\nlegend(\"topleft\",\n  legend = c(\"linear\", \"&lt;65 vs &gt;=65\", \"&lt;60, 60-69, 70+\"),\n  lty = c(1, 2, 3), lwd = 2, cex = 1, bty = \"n\", col = mycolors[2:4]\n)\n\n\n\n\n\n\n\n\n\n\n\nFig 9.2: Impact of number of ST elevations (STE)\n\n\nCode show/hide\n# winsorize at STE 10\ngusto$STE &lt;- ifelse(gusto$STE &gt; 10, 10, gusto$STE)\ngusto$STE.f &lt;- as.factor(gusto$STE)\nSTE.linear &lt;- lrm(DAY30 ~ STE, data = gusto, x = T, y = T)\nSTE.square &lt;- lrm(DAY30 ~ pol(STE, 2), data = gusto, x = T, y = T)\nSTE.rcs &lt;- lrm(DAY30 ~ rcs(STE, 5), data = gusto, x = T, y = T)\nSTE.factor &lt;- lrm(DAY30 ~ STE.f, data = gusto, x = T, y = T)\n\n# dichotomize\nSTE.cat4 &lt;- lrm(DAY30 ~ ifelse(STE &lt; 5, 0, 1), data = gusto, x = T, y = T)\n\n# predict for STE 0:10\nnewdata.STE &lt;- data.frame(\"STE\" = seq(0, 10, by = 1))\npred.STE.linear &lt;- predict(STE.linear, newdata.STE)\npred.STE.square &lt;- predict(STE.square, newdata.STE)\npred.STE.rcs &lt;- predict(STE.rcs, newdata.STE)\npred.STE.cat4 &lt;- predict(STE.cat4, newdata.STE)\n\npar(mfrow = c(1, 1))\nplot(\n  x = newdata.STE[, 1], y = pred.STE.linear, las = 1,\n  xlab = \"Number of leads with ST elevation\", ylab = \"logit of 30-day mortality\", cex.lab = 1.2, type = \"l\", lwd = 2, col = mycolors[2]\n)\nlines(x = newdata.STE[, 1], y = pred.STE.square, lty = 2, lwd = 2, col = mycolors[3])\nlines(x = newdata.STE[, 1], y = pred.STE.rcs, lty = 3, lwd = 3, col = mycolors[4])\nlines(x = newdata.STE[1:5, 1], y = pred.STE.cat4[1:5], lty = 4, lwd = 2, col = mycolors[5])\nlines(x = newdata.STE[6:11, 1], y = pred.STE.cat4[6:11], lty = 4, lwd = 2, col = mycolors[5])\n\n# Original data points, with size proportional to sqrt(events)\nSTEmort &lt;- log(by(gusto$DAY, gusto$STE, mean) / (1 - by(gusto$DAY, gusto$STE, mean)))[1:11]\nSTEw &lt;- sqrt(by(gusto$DAY, gusto$STE, sum)[1:11]) / 10\npoints(x = newdata.STE[, 1], y = STEmort, pch = 1, lwd = 2, cex = STEw, col = \"black\")\nscat1d(x = gusto$STE, side = 1, frac = .05, col = \"darkblue\")\nlegend(\"topleft\",\n  legend = c(\"data\", \"linear\", \".. +square\", \"rcs\", \"&lt;=4, vs &gt;4\"),\n  lty = c(NA, 1, 2, 3, 4), pch = c(1, NA, NA, NA, NA), lwd = 2, cex = 1, bty = \"n\", col = mycolors[1:5]\n)\n\n\n\n\n\n\n\n\n\n\n\nFig 9.3: Non-linearities in small sample (n=751); and full GUSTO-I (n=40,830)\n\n\nCode show/hide\n# Examine non-linearities in gustos sample\n# Age\nage.linear &lt;- lrm(DAY30 ~ AGE, data = gustos, x = T, y = T, linear.predictors = F)\nage.rcs1 &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gustos, x = T, y = T, linear.predictors = F)\nage.fp1 &lt;- mfp(DAY30 ~ fp(AGE, df = 4), alpha = 1, data = gustos, family = binomial) # selected: -2 and 3\nage.gam1 &lt;- gam(DAY30 ~ s(AGE), data = gustos, family = binomial)\n\n# examine predictions for age 20:95\nage.mat &lt;- matrix(nrow = 751, ncol = 5)\nnames(age.mat) &lt;- list(NULL, Cs(AGE, linear, fp, rcs, gam))\nAGE &lt;- seq(20, 95, by = 0.1)\nage.mat[, 1] &lt;- AGE\nage.mat[, 2] &lt;- predict(age.linear, newdata = as.data.frame(x = AGE), type = \"lp\")\nage.mat[, 3] &lt;- predict(age.fp1, newdata = as.data.frame(x = AGE))\nage.mat[, 4] &lt;- predict(age.rcs1, newdata = as.data.frame(x = AGE), type = \"lp\")\nage.mat[, 5] &lt;- predict(age.gam1, newdata = as.data.frame(x = AGE))\n\n# Plot for n=785, Fig 9.3, part A #\npar(mfrow = c(1, 2))\nplot(\n  x = age.mat[, 1], y = age.mat[, 2], xlim = c(20, 92), ylim = c(-8, 0.2), las = 1, xaxt = \"n\",\n  xlab = \"Age in years\", ylab = \"logit of 30-day mortality\", cex.lab = 1.2, type = \"l\", lwd = 2, col = mycolors[2]\n)\naxis(1, at = c(30, 40, 50, 60, 70, 80, 90))\nlines(x = age.mat[, 1], y = age.mat[, 3], lty = 2, lwd = 2, col = mycolors[3])\nlines(x = age.mat[, 1], y = age.mat[, 4], lty = 3, lwd = 3, col = mycolors[4])\nlines(x = age.mat[, 1], y = age.mat[, 5], lty = 4, lwd = 3, col = mycolors[5])\nhistSpike(x = gustos$AGE, side = 1, frac = .1, col = \"darkblue\", add = T)\nlegend(\"topleft\",\n  legend = c(\"linear\", \"fp\", \"rcs\", \"gam\"),\n  lty = 1:4, lwd = 2, cex = 1.2, bty = \"n\", col = mycolors[2:5]\n)\ntitle(\"GUSTO-I, n=785\")\n# End plot n=785\n\n####################\n## Now: N=40,830 ##\nage.linear.2 &lt;- lrm(DAY30 ~ AGE, data = gusto, x = T, y = T, linear.predictors = F)\nage.rcs1.2 &lt;- lrm(DAY30 ~ rcs(AGE, 5), data = gusto, x = T, y = T, linear.predictors = F)\nage.fp1.2 &lt;- mfp(DAY30 ~ fp(AGE, df = 4), alpha = 1, data = gusto, family = binomial) # selected: -2 and 3\nage.gam1.2 &lt;- gam(DAY30 ~ s(AGE), data = gusto, family = binomial)\n\n# examine predictions for age 20:95\nage.mat.2 &lt;- matrix(nrow = 751, ncol = 5)\nnames(age.mat.2) &lt;- list(NULL, Cs(AGE, linear, fp, rcs, gam))\nage.mat.2[, 1] &lt;- AGE\nage.mat.2[, 2] &lt;- predict(age.linear.2, newdata = as.data.frame(x = AGE), type = \"lp\")\nage.mat.2[, 3] &lt;- predict(age.fp1.2, newdata = as.data.frame(x = AGE))\nage.mat.2[, 4] &lt;- predict(age.rcs1.2, newdata = as.data.frame(x = AGE), type = \"lp\")\nage.mat.2[, 5] &lt;- predict(age.gam1.2, newdata = as.data.frame(x = AGE))\n\n# Plot for n=40830\nplot(\n  x = age.mat.2[, 1], y = age.mat.2[, 2], xlim = c(20, 92), ylim = c(-8, 0.2), las = 1, xaxt = \"n\",\n  xlab = \"Age in years\", ylab = \"\", cex.lab = 1.2, type = \"l\", lwd = 2, col = mycolors[2]\n)\naxis(1, at = c(30, 40, 50, 60, 70, 80, 90))\nlines(x = age.mat.2[, 1], y = age.mat.2[, 3], lty = 2, lwd = 2, col = mycolors[3])\nlines(x = age.mat.2[, 1], y = age.mat.2[, 4], lty = 3, lwd = 3, col = mycolors[4])\nlines(x = age.mat.2[, 1], y = age.mat.2[, 5], lty = 4, lwd = 3, col = mycolors[5])\nhistSpike(x = gusto$AGE, side = 1, frac = .2, col = \"darkblue\", add = T)\ntitle(\"GUSTO-I, n=40,830\")\n\n\n\n\n\n\n\n\n\n\n\nFig 9.4: Glucose and hb in TBI\nWe evaluate the predictors ‘glucose’ and ‘hemoglobin’ in TBI. Both are first truncated, after inspecting boxplots (Fig 9.4), and then analyzed for their prognostic value, with plots for illustration of the estimated relations (Fig 9.5).\n\n\nCode show/hide\n# glucose\nquantile(TBI$glucose, probs = c(.005, .01, .02, .98, .99, .995), na.rm = T)\n\n\n 0.5%    1%    2%   98%   99% 99.5% \n 1.57  2.26  4.28 18.34 20.92 23.39 \n\n\nCode show/hide\n#  0.5%    1%    2%   98%   99% 99.5%\n#  1.57  2.26  4.28 18.34 20.92 23.39\n# So, winsorize / truncate at 3 and 20\n# use simple function to do the winsorizing:\n# {ifelse(x&lt;lower,upper,  ifelse(x&gt;upper,upper, x))}\nwinsorize &lt;- function(x, lower = quantile(x, probs = 0.01), upper = quantile(x, probs = 0.99)) {\n  ifelse(x &lt; lower, lower,\n    ifelse(x &gt; upper, upper, x)\n  )\n}\nTBI$glucoset &lt;- winsorize(TBI$glucose, 3, 20)\n\n# systolic bp\nquantile(TBI$hb, probs = c(.005, .01, .02, .98, .99, .995), na.rm = T)\n\n\n 0.5%    1%    2%   98%   99% 99.5% \n 4.87  5.40  6.30 16.50 16.80 17.00 \n\n\nCode show/hide\n#  0.5%    1%    2%   98%   99% 99.5%\n#  4.87  5.40  6.30 16.50 16.80 17.00\n# So, winsorize / truncate at 6 and 17\nTBI$hbt &lt;- winsorize(TBI$hb, 6, 17)\n\n# boxplots for illustration\npar(mfrow = c(1, 2))\nboxplot(x = cbind(TBI$glucose, TBI$glucoset), outcol = c(\"red\", mycolors[4]), border = mycolors[c(10, 4)], xaxt = \"n\", ylab = \"glucose (mmol/L)\")\naxis(1, at = c(1, 2), labels = c(\"before \", \"after winsorizing\"))\ntitle(\"Glucose\")\nboxplot(x = cbind(TBI$hb, TBI$hbt), outcol = c(\"red\", mycolors[4]), border = mycolors[c(10, 4)], xaxt = \"n\", ylab = \"hb (mmol/L)\")\naxis(1, at = c(1, 2), labels = c(\"before \", \"after winsorizing\"))\ntitle(\"Hemoglobin\")\n\n\n\n\n\n\n\n\n\n\n\nFig 9.5: Non-linear association of glucose\n\n\n\n\n\n\n\n\n\n\n\nFig 9.6: Systolic blood pressure in TBI\nWe now examine the prognostic value of systolic blood pressure (BP) in TBI patients. We expect low BP (hypotension) to be especially risky.\n\n\nCode show/hide\nquantile(TBI$d.sysbpt, probs = c(.01, .25, .75, .99), na.rm = T)\n\n\n   1%   25%   75%   99% \n 92.9 121.1 142.1 171.8 \n\n\nCode show/hide\nTBIs &lt;- TBI[!is.na(TBI$d.sysbpt), ]\n#   1%   25%   75%   99%\n# 92.9 121.1 142.1 171.8\n\ng1 &lt;- lrm(d.gos &lt; 2 ~ rcs(d.sysbpt, 3), data = TBIs)\ng2 &lt;- lrm(d.gos &lt; 3 ~ rcs(d.sysbpt, 3), data = TBIs)\ng3 &lt;- lrm(d.gos &lt; 4 ~ rcs(d.sysbpt, 3), data = TBIs)\ng4 &lt;- lrm(d.gos &lt; 5 ~ rcs(d.sysbpt, 3), data = TBIs)\n\n# Define categorical variants of systolic BP\nTBIs$d.sysbpt.c &lt;- as.factor(ifelse(TBIs$d.sysbpt &lt; 120, 1, ifelse(TBIs$d.sysbpt &gt; 150, 2, 0)))\n\ng1t &lt;- lrm(d.gos &lt; 2 ~ d.sysbpt.c, data = TBIs)\ng2t &lt;- lrm(d.gos &lt; 3 ~ d.sysbpt.c, data = TBIs)\ng3t &lt;- lrm(d.gos &lt; 4 ~ d.sysbpt.c, data = TBIs)\ng4t &lt;- lrm(d.gos &lt; 5 ~ d.sysbpt.c, data = TBIs)\n\ndd &lt;- datadist(TBIs)\noptions(datadist = \"dd\")\n# Odds ratios\nexp(coef(g1t)) # OR 2.78 for low BP and 1.25 for high BP\n\n\n   Intercept d.sysbpt.c=1 d.sysbpt.c=2 \n       0.226        2.777        1.245 \n\n\nCode show/hide\ndescribe(TBI$d.sysbpt)\n\n\nTBI$d.sysbpt \n       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75 \n    2159        0     1566        1    131.4    17.61    107.0    112.2    121.1    131.3    142.1 \n     .90      .95 \n   151.1    156.2 \n\nlowest : 60     64.19  65.6   71.33  73.71 , highest: 182.92 184.16 184.7  188.29 207.4 \n\n\nCode show/hide\ndescribe(TBIs$d.sysbpt.c)\n\n\nTBIs$d.sysbpt.c \n       n  missing distinct \n    2159        0        3 \n                            \nValue          0     1     2\nFrequency   1435   474   250\nProportion 0.665 0.220 0.116\n\n\nCode show/hide\n# data in matrix for plot\nd.sysbpt &lt;- seq(60, 210, by = 1) # 151 elements\ng.mat &lt;- matrix(nrow = 151, ncol = 5)\nnames(g.mat) &lt;- list(NULL, Cs(d.sysbpt, g1, g2, g3, g4))\ng.mat[, 1] &lt;- d.sysbpt\ng.mat[, 2] &lt;- predict(g1, newdata = as.data.frame(x = d.sysbpt))\ng.mat[, 3] &lt;- predict(g2, newdata = as.data.frame(x = d.sysbpt))\ng.mat[, 4] &lt;- predict(g3, newdata = as.data.frame(x = d.sysbpt))\ng.mat[, 5] &lt;- predict(g4, newdata = as.data.frame(x = d.sysbpt))\n\n# Plot: Fig 9.6, part I\npar(mfrow = c(1, 2))\nplot(\n  x = g.mat[, 1], y = g.mat[, 2], xlim = c(50, 210), ylim = c(-2.7, 2.7), las = 1, xaxt = \"n\",\n  xlab = \"Systolic blood pressure (mmHg)\", ylab = \"logit GOS at 6 months\", cex.lab = 1.2, type = \"l\", lwd = 2, col = mycolors[1]\n)\naxis(1, at = c(100, 150, 200))\nlines(x = g.mat[, 1], y = g.mat[, 3], lty = 2, lwd = 2, col = mycolors[2])\nlines(x = g.mat[, 1], y = g.mat[, 4], lty = 3, lwd = 3, col = mycolors[3])\nlines(x = g.mat[, 1], y = g.mat[, 5], lty = 4, lwd = 3, col = mycolors[4])\nhistSpike(x = TBI$d.sysbpt, side = 3, frac = .2, col = \"darkblue\", add = T)\nlegend(\"bottomleft\", legend = c(\"&lt;good recovery\", \"unfavorable\", \"mort+veg\", \"mortality\"), lty = 4:1, lwd = 2, cex = 0.8, bty = \"n\", col = mycolors[4:1])\ntitle(\"continuous, rcs 3 knots\")\n\n## Plot 9.6, part II\nd.sysbpt.c &lt;- c(0, 1, 2)\nd.sysbpt &lt;- seq(60, 210, by = .1) # 1501 elements\ng.mat2 &lt;- matrix(nrow = 1501, ncol = 5)\ng1t.c &lt;- predict(g1t, newdata = as.data.frame(x = d.sysbpt.c))\ng2t.c &lt;- predict(g2t, newdata = as.data.frame(x = d.sysbpt.c))\ng3t.c &lt;- predict(g3t, newdata = as.data.frame(x = d.sysbpt.c))\ng4t.c &lt;- predict(g4t, newdata = as.data.frame(x = d.sysbpt.c))\n\nnames(g.mat2) &lt;- list(NULL, Cs(d.sysbpt, g1, g2, g3, g4))\ng.mat2[, 1] &lt;- d.sysbpt\ng.mat2[, 2] &lt;- ifelse(d.sysbpt &lt; 120, g1t.c[2], ifelse(d.sysbpt &gt; 150, g1t.c[3], g1t.c[1]))\ng.mat2[, 3] &lt;- ifelse(d.sysbpt &lt; 120, g2t.c[2], ifelse(d.sysbpt &gt; 150, g2t.c[3], g2t.c[1]))\ng.mat2[, 4] &lt;- ifelse(d.sysbpt &lt; 120, g3t.c[2], ifelse(d.sysbpt &gt; 150, g3t.c[3], g3t.c[1]))\ng.mat2[, 5] &lt;- ifelse(d.sysbpt &lt; 120, g4t.c[2], ifelse(d.sysbpt &gt; 150, g4t.c[3], g4t.c[1]))\n\n# Plot\nplot(\n  x = g.mat2[, 1], y = g.mat2[, 2], xlim = c(50, 210), ylim = c(-2.7, 2.7), las = 1, xaxt = \"n\",\n  xlab = \"Systolic blood pressure (mmHg)\", ylab = \"\", cex.lab = 1.2, type = \"l\", lwd = 2, col = mycolors[1]\n)\naxis(1, at = c(100, 150, 200))\nlines(x = g.mat2[, 1], y = g.mat2[, 3], lty = 2, lwd = 2, col = mycolors[2])\nlines(x = g.mat2[, 1], y = g.mat2[, 4], lty = 3, lwd = 3, col = mycolors[3])\nlines(x = g.mat2[, 1], y = g.mat2[, 5], lty = 4, lwd = 3, col = mycolors[4])\nhistSpike(x = TBI$d.sysbpt, side = 3, frac = .2, col = \"darkblue\", add = T)\ntitle(\"3 categories\")",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Coding of Categorical and Continuous Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/10-restrict-cand-pred.html",
    "href": "chapters/10-restrict-cand-pred.html",
    "title": "10  Restrictions on Candidate Predictors",
    "section": "",
    "text": "Case-study:",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Restrictions on Candidate Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/10-restrict-cand-pred.html#case-study",
    "href": "chapters/10-restrict-cand-pred.html#case-study",
    "title": "10  Restrictions on Candidate Predictors",
    "section": "",
    "text": "Mortality after surgery for esophageal cancer\nLet’s consider the example of predicting 30 day mortality after surgery for esophageal cancer, We analyzed data from the SEER-Medicare database. Among 2041 patients who were over 65 years old and diagnosed between 1991 and 1996, 221 had died by 30 days JCO 2006.\nFor a robust evaluation of the prognostic relevance of comorbidity, we create a simple sum score. It is based on the sum of imputed values for 5 comorbidities. The maximum is 3 comorbidities in this case.\n\nComorbidity variables\n\n\nVariable\nMeaning\n\n\n\n\nCOMORBI\nComorbidity score based on the count of 5 comorbidities\n\n\nCPD\nChronic Pulmonary Disease\n\n\nCardio\nCardiovascular disease\n\n\nDiabetes\nDiabetes\n\n\nLiver\nLiver disease\n\n\nRenal\nRenal disease\n\n\n\nWe describe the data below. Note that some missing values for comorbidities were imputed with values between 0 and 1. A regression imputation model was used, with the expected value used as a single imputed value.\n\n\nCode show/hide\n# Import SEER data set, n=2041\nSurgery &lt;- read.csv(\"data/EsoSurgery.csv\")\noptions(prType='html')\nhtml(describe(Surgery), scroll=TRUE)\n\n\n\n\n\n\n\nSurgery Descriptives\nSurgery  8  Variables   2041  Observations\n\nD30\n\n \n nmissingdistinctInfoSumMeanGmd\n 2041020.292210.10830.1932\n \n\n\nAGE\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 20410801173.56.72365.8366.4268.5872.4277.2581.9185.17\n \n\nlowest : 65.0021 65.076  65.0815 65.0842 65.1608 ,  highest: 94.6639 94.6667 95.0856 97.9986 101.744\n\nCOMORBI\n        n  missing distinct     Info     Mean      Gmd      .05      .10      .25 \n     2041        0      393    0.737   0.2983   0.4589   0.0000   0.0000   0.0000 \n      .50      .75      .90      .95 \n   0.0000   0.3756   1.0000   1.1968  \n\nlowest : 0        0.179426 0.182296 0.189824 0.193299 ,  highest: 2.14073  2.15795  2.15974  2.21896  3       \n\nCPD\n        n  missing distinct     Info     Mean      Gmd      .05      .10      .25 \n     2041        0      376    0.591  0.09873   0.1718  0.00000  0.00000  0.00000 \n      .50      .75      .90      .95 \n  0.00000  0.07714  0.16931  1.00000  \n\n Value       0.00  0.06  0.07  0.08  0.09  0.10  0.11  0.12  0.13  0.14  0.15  0.16\n Frequency   1514     5    17    32    29    35    28    56    40    30    29    25\n Proportion 0.742 0.002 0.008 0.016 0.014 0.017 0.014 0.027 0.020 0.015 0.014 0.012\n                                                           \n Value       0.17  0.18  0.19  0.20  0.21  0.22  0.23  1.00\n Frequency     18    11     7     5     2     3     2   153\n Proportion 0.009 0.005 0.003 0.002 0.001 0.001 0.001 0.075 \n\n\nFor the frequency table, variable is rounded to the nearest 0.01\n\nCardio\n        n  missing distinct     Info     Mean      Gmd      .05      .10      .25 \n     2041        0      374    0.593  0.09677   0.1702   0.0000   0.0000   0.0000 \n      .50      .75      .90      .95 \n   0.0000   0.0536   0.1692   1.0000  \n\n\n \n lowest :0        0.04250480.04332580.04475250.0448201\n highest:0.296846 0.299705 0.325783 0.36526  1        \n \n\n\nDiabetes\n        n  missing distinct     Info     Mean      Gmd      .05      .10      .25 \n     2041        0      378    0.594  0.09295   0.1641   0.0000   0.0000   0.0000 \n      .50      .75      .90      .95 \n   0.0000   0.0566   0.1304   1.0000  \n\n Value       0.00  0.04  0.05  0.06  0.07  0.08  0.09  0.10  0.11  0.12  0.13  0.14\n Frequency   1511     5    33    59    54    47    35    33    24    32    19    12\n Proportion 0.740 0.002 0.016 0.029 0.026 0.023 0.017 0.016 0.012 0.016 0.009 0.006\n                                                     \n Value       0.15  0.16  0.17  0.18  0.19  0.20  1.00\n Frequency      7     7     3     2     3     1   154\n Proportion 0.003 0.003 0.001 0.001 0.001 0.000 0.075 \n\n\nFor the frequency table, variable is rounded to the nearest 0.01\n\nLiver\n        n  missing distinct     Info     Mean      Gmd      .05      .10      .25 \n     2041        0      391    0.474 0.002736 0.005334 0.000000 0.000000 0.000000 \n      .50      .75      .90      .95 \n 0.000000 0.000000 0.001706 0.005047  \n\n\n \n lowest :0          0.0002774640.0002842780.0002918950.000293725\n highest:0.024005   0.0259095  0.0290324  0.0364262  1          \n \n\n\nRenal\n        n  missing distinct     Info     Mean      Gmd      .05      .10      .25 \n     2041        0      391     0.48 0.007155  0.01381 0.000000 0.000000 0.000000 \n      .50      .75      .90      .95 \n 0.000000 0.000000 0.007221 0.012013  \n\n\n \n lowest :0          0.0002424740.0004064050.0005500730.00060003 \n highest:0.0391617  0.0409958  0.0413482  0.0746603  1          \n \n\n\n\n\n\nCode show/hide\nfit1 &lt;- lrm(D30~COMORBI, data=Surgery)\nprint(fit1)\n\n\nLogistic Regression Model\n\nlrm(formula = D30 ~ COMORBI, data = Surgery)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Likelihood\nRatio Test\nDiscrimination\nIndexes\nRank Discrim.\nIndexes\n\n\n\n\nObs 2041\nLR χ2 13.99\nR2 0.014\nC 0.549\n\n\n0 1820\nd.f. 1\nR21,2041 0.006\nDxy 0.098\n\n\n1 221\nPr(&gt;χ2) 0.0002\nR21,591.2 0.022\nγ 0.162\n\n\nmax |∂log L/∂β| 3×10-9\n\nBrier 0.096\nτa 0.019\n\n\n\n\n\n\n\n\n\n\n\nβ\nS.E.\nWald Z\nPr(&gt;|Z|)\n\n\n\n\nIntercept\n -2.2643\n 0.0850\n-26.65\n&lt;0.0001\n\n\nCOMORBI\n  0.4443\n 0.1129\n3.93\n&lt;0.0001\n\n\n\n\n\nThe prognostic value of the score is modest; on its own (univariate logistic regression of D30~COMORBI), we find a c statistic of 0.55.\n\n\nTesting the equal weights assumption in a simple sumscore\nSimple sums of predictors make the assumption of equal weights for each predictor. This assumption can be assessed in at least two ways\n\nAn overall test: is a more refined coding preferable over a simple sum?\nIn the example of comorbidity, we consider the sum of 5 comorbidity conditions as a simple sumscore (Table 10.2). A model considering the 5 comorbidity conditions separately has 5 df and a Likelihood Ratio statistic of 18, in contrast to 14 for the simple sumscore. The difference of 3.6 with 4 df has a p-value of 0.46, far from convincing against the idea of using the simple sumscore.\nComponent-wise testing: is one of the comborbidities really deviant in a prognostic value?\nWe adding the conditions one by one in a regression model that already contains the sumscore. The coefficient of the condition added in a model indicates the deviation from the common effect based on the other conditions.\nWe note that the deviations from the common effect are relatively small, except for liver disease and renal disease. Renal disease even seemed to have a protective effect. Both effects were based on small numbers. The standard errors of the estimates were large, and the effects were statistically nonsignificant.\n\n\n\nCode show/hide\n# Make function that that a score plus its components\n# outcome and data specified as well\ntest.equal.weights &lt;- function(data=Surgery, y=\"D30\", sumscore=\"COMORBI\", \n                  components=Cs(CPD, Cardio, Diabetes, Liver, Renal)) {\n# results in matrix\nmatrix.coefs &lt;- matrix(nrow=(2+length(components)), ncol=7)\n# labels.components &lt;- dput(as.character(components)) # to get it nice for row.labels\ndimnames(matrix.coefs) &lt;- list(c(\"sumscore\", \"ALL\", dput(as.character(components))),\n                            Cs(Coef.Sumscore, SE.Sumscore, Coef.Component, SE.Component, \n                               LR, df, p-value))\n# Make models:\n# 1. sumscore\nfit1 &lt;- lrm(data[,y] ~ data[,sumscore])\nmatrix.coefs[1,]  &lt;- c(fit1$coef[2], sqrt(fit1$var[2,2]), NA, NA, fit1$stats[3:5])\n\n# 2. full model for overall comparison\nfit.full &lt;- lrm(data[,y] ~ as.matrix(data[,components]), x=T)\n# compare model fits\np.anova.comparison &lt;- pchisq(fit.full$stats[3] - fit1$stats[3], \n                             df= fit.full$stats[4] - fit1$stats[4], lower.tail = F )\nmatrix.coefs[2,]  &lt;- c(NA, NA, NA, NA, fit.full$stats[3:4], p.anova.comparison)\n\n# 3. fit incremenal differences to sumscore\nfor (i in 1:length(components)) {\nfiti &lt;- update(fit1, .~.+ fit.full$x[,i])\n# compare model fits\np.anova.comparison &lt;- pchisq(fiti$stats[3] - fit1$stats[3], \n                             df= fiti$stats[4] - fit1$stats[4], lower.tail = F )\nmatrix.coefs[2+i,]  &lt;- c(fiti$coef[2], sqrt(fiti$var[2,2]), fiti$coef[3], sqrt(fiti$var[3,3]),\n                          fiti$stats[3:4], p.anova.comparison) } # end loop\nreturn(matrix.coefs)\n} # end function\n\nkable(test.equal.weights(data=Surgery, y=\"D30\", sumscore=\"COMORBI\", \n                  components=Cs(CPD, Cardio, Diabetes, Liver, Renal)), \n                  caption=\"**Table 10.2**: Testing deviations for each condition in a sum score. Data from esophageal cancer patients who underwent surgery (2041 patients from SEER-Medicare data, 221 died by 30 days). The overall test for deviations from a simple sum score had a p-value of 0.46 (overall LR test, 4 *df*)\")\n\n\nc(\"CPD\", \"Cardio\", \"Diabetes\", \"Liver\", \"Renal\")\n\n\n\nTable 10.2: Testing deviations for each condition in a sum score. Data from esophageal cancer patients who underwent surgery (2041 patients from SEER-Medicare data, 221 died by 30 days). The overall test for deviations from a simple sum score had a p-value of 0.46 (overall LR test, 4 df)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.Sumscore\nSE.Sumscore\nCoef.Component\nSE.Component\nLR\ndf\np - value\n\n\n\n\nsumscore\n0.44\n0.11\nNA\nNA\n14\n1\n0.00\n\n\nALL\nNA\nNA\nNA\nNA\n18\n5\n0.46\n\n\nCPD\n0.51\n0.14\n-0.22\n0.31\n14\n2\n0.48\n\n\nCardio\n0.49\n0.16\n-0.13\n0.32\n14\n2\n0.69\n\n\nDiabetes\n0.35\n0.15\n0.32\n0.29\n15\n2\n0.27\n\n\nLiver\n0.42\n0.12\n1.31\n1.03\n16\n2\n0.22\n\n\nRenal\n0.48\n0.12\n-1.09\n1.11\n15\n2\n0.26\n\n\n\n\n\n\n\nDiscussion\nIn the SEER data case study, we stick to our assumption of a similar effect for all comorbities. The apparently most deviant effects of liver disease and renal disease were unreliable. We hence may assume similar effects for Liver and Renal as for the other comorbidities.\nAn extension of the component-wise testing might be to apply a LASSO regression model where the sumscore effect is taken as an offset, with shrinkage of deviations from this offset. In the case study, the overall test was far from statistically significant, and hence we expect shrinkage of most or all of these deviations to zero. This idea is similar to updating of a prediction model, with shrinkage of deviating coefficients to values of a prior model: y~predictor, offset=linear.predictor Stat Med 2004.\nFurther discussions on robust modeling were motivated by a case study on prediction of mutations based on family history JAMA 2006. A simple weighting of second degree relatives as half the effect of first degree relatives worked well in this case. And the effect for age of diagnosis in a relative could be assumed identical for the index patient (proband) and the first and second degree relatives. This simplification saved degrees of freedom in the modeling process at the expense of potentially missing specific patterns in the data Stat Med 2007.\nA robust approach is especially attractive in relatively small data set. Indeed, a major study attempted to model family history for different cancers (colon, endometrial, other) separately for first and second degree relatives, with dichotomization of age as below or above 50 years in 870 patients with only 38 mutations identified NEJM 2006. Simulations confirmed that attempting such modeling was a bad idea JCE 2018, both because of severe overfitting (38 events) and dichotomania. In contrast, the robust modeling strategy was applied in various versions of prediction models for mutation status Gastroenterology 2011; JCO 2017, with satisfactory performance in a large-scale international validation study JNCI 2015.\n\n\nLiterature\nSEER data case study:\n\nSteyerberg EW, Neville BA, Koppert LB, Lemmens VE, Tilanus HW, Coebergh JW, Weeks JC, Earle CC. Surgical mortality in patients with esophageal cancer: development and validation of a simple risk score. J Clin Oncol. 2006 Sep 10;24(26):4277-84. doi: 10.1200/JCO.2005.05.0658. PMID: 16963730\n\nMethods papers:\n\nSteyerberg EW, Borsboom GJ, van Houwelingen HC, Eijkemans MJ, Habbema JD. Validation and updating of predictive logistic regression models: a study on sample size and shrinkage. Stat Med. 2004 Aug 30;23(16):2567-86. doi: 10.1002/sim.1844. PMID: 15287085\nSteyerberg EW, Uno H, Ioannidis JPA, van Calster B; Collaborators. Poor performance of clinical prediction models: the harm of commonly applied methods. J Clin Epidemiol. 2018 Jun;98:133-143. doi: 10.1016/j.jclinepi.2017.11.013. Epub 2017 Nov 24. PMID: 29174118\nSteyerberg EW, Balmaña J, Stockwell DH, Syngal S. Data reduction for prediction: a case study on robust coding of age and family history for the risk of having a genetic mutation. Stat Med. 2007 Dec 30;26(30):5545-56. doi: 10.1002/sim.3119. PMID: 17948867\n\nMutation status prediction:\n\nBalmaña J, Stockwell DH, Steyerberg EW, …, Burbidge LA, Syngal S. Prediction of MLH1 and MSH2 mutations in Lynch syndrome. JAMA. 2006 Sep 27;296(12):1469-78. doi: 10.1001/jama.296.12.1469. PMID: 17003395\nBarnetson RA, Tenesa A, Farrington SM, Nicholl ID, Cetnarskyj R, Porteous ME, Campbell H, Dunlop MG. Identification and survival of carriers of mutations in DNA mismatch-repair genes in colon cancer. N Engl J Med. 2006 Jun 29;354(26):2751-63. doi: 10.1056/NEJMoa053493\nKastrinos F, Uno H, Ukaegbu C, …, Steyerberg EW, Syngal S. Development and Validation of the PREMM5 Model for Comprehensive Risk Assessment of Lynch Syndrome. J Clin Oncol. 2017 Jul 1;35(19):2165-2172. doi: 10.1200/JCO.2016.69.6120. Epub 2017 May 10. PMID: 28489507\nKastrinos F, Steyerberg EW, Mercado R, …, Wenstrup RJ, Syngal S. The PREMM(1,2,6) model predicts risk of MLH1, MSH2, and MSH6 germline mutations based on cancer history. Gastroenterology. 2011 Jan;140(1):73-81. doi: 10.1053/j.gastro.2010.08.021. Epub 2010 Aug 19. PMID: 20727894\nKastrinos F, Ojha RP, Leenen C, …, Syngal S, Steyerberg EW; Lynch Syndrome prediction model validation study group. Comparison of Prediction Models for Lynch Syndrome Among Individuals With Colorectal Cancer. J Natl Cancer Inst. 2015 Nov 18;108(2):djv308. doi: 10.1093/jnci/djv308. Print 2016 Feb.PMID: 26582061",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Restrictions on Candidate Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/11-main-effects-select.html",
    "href": "chapters/11-main-effects-select.html",
    "title": "11  Selection of Main Effects",
    "section": "",
    "text": "Chapter 11 materials upcoming.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Selection of Main Effects</span>"
    ]
  },
  {
    "objectID": "chapters/12-additivity-linearity.html",
    "href": "chapters/12-additivity-linearity.html",
    "title": "12  Assumptions in Regression Models - Additivity and Linearity",
    "section": "",
    "text": "GUSTO-I interaction analysis",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assumptions in Regression Models - Additivity and Linearity</span>"
    ]
  },
  {
    "objectID": "chapters/12-additivity-linearity.html#gusto-i-interaction-analysis",
    "href": "chapters/12-additivity-linearity.html#gusto-i-interaction-analysis",
    "title": "12  Assumptions in Regression Models - Additivity and Linearity",
    "section": "",
    "text": "Examine interactions\n\n\nCode show/hide\n# Import gusto, gustoB, and sample4 data sets\ngusto &lt;- read.csv(\"data/gusto1.csv\") # GUSTO sample with 40830 patients\ngustoB &lt;- read.csv(\"data/gustoB.csv\") # GUSTO part B sample with 20318 patients\ngustos &lt;- read.csv(\"data/sample4.csv\") # GUSTO sample4 with 785 patients\n\nsource(\"R/auc.nonpara.mw.R\")\nsource(\"R/ci.auc.R\")\nsource(\"R/val.prob.ci.2.R\")\n\n# levels(gustos$HRT) &lt;- c(\"No tachycardia\", \"Tachycardia\")\ndd &lt;- datadist(gustos)\noptions(datadist = \"dd\")\n\n\nEvaluate interactions with age in a full model, which includes 8 predictors in total. The data set is small (sample4, n=785, 52 events)\n\n\nCode show/hide\n### Full model and age interactions\nfull &lt;- lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F)\nfullint &lt;- lrm(DAY30 ~ AGE * (KILLIP + HIG + DIA + HYP + HRT + TTR + SEX), data = gustos, x = T, y = T, linear.predictors = F)\nanova(fullint)\n\n\n                Wald Statistics          Response: DAY30 \n\n Factor                                      Chi-Square d.f. P     \n AGE  (Factor+Higher Order Factors)          29.93       8   0.0002\n  All Interactions                           11.01       7   0.1380\n KILLIP  (Factor+Higher Order Factors)        4.96       2   0.0839\n  All Interactions                            1.01       1   0.3160\n HIG  (Factor+Higher Order Factors)           6.32       2   0.0424\n  All Interactions                            2.20       1   0.1380\n DIA  (Factor+Higher Order Factors)           3.70       2   0.1571\n  All Interactions                            0.64       1   0.4220\n HYP  (Factor+Higher Order Factors)           5.62       2   0.0603\n  All Interactions                            1.46       1   0.2267\n HRT  (Factor+Higher Order Factors)          13.50       2   0.0012\n  All Interactions                            4.49       1   0.0341\n TTR  (Factor+Higher Order Factors)           8.31       2   0.0157\n  All Interactions                            2.80       1   0.0942\n SEX  (Factor+Higher Order Factors)           0.79       2   0.6736\n  All Interactions                            0.42       1   0.5151\n AGE * KILLIP  (Factor+Higher Order Factors)  1.01       1   0.3160\n AGE * HIG  (Factor+Higher Order Factors)     2.20       1   0.1380\n AGE * DIA  (Factor+Higher Order Factors)     0.64       1   0.4220\n AGE * HYP  (Factor+Higher Order Factors)     1.46       1   0.2267\n AGE * HRT  (Factor+Higher Order Factors)     4.49       1   0.0341\n AGE * TTR  (Factor+Higher Order Factors)     2.80       1   0.0942\n AGE * SEX  (Factor+Higher Order Factors)     0.42       1   0.5151\n TOTAL INTERACTION                           11.01       7   0.1380\n TOTAL                                       64.59      15   &lt;.0001\n\n\nCode show/hide\n### Select only interaction AGE * HRT\nfullints &lt;- lrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = T)\nanova(fullints)\n\n\n                Wald Statistics          Response: DAY30 \n\n Factor                                   Chi-Square d.f. P     \n AGE  (Factor+Higher Order Factors)       26.67      2    &lt;.0001\n  All Interactions                         2.70      1    0.1004\n KILLIP                                    3.52      1    0.0605\n HIG                                       5.38      1    0.0204\n DIA                                       3.14      1    0.0763\n HYP                                       3.49      1    0.0618\n HRT  (Factor+Higher Order Factors)       11.67      2    0.0029\n  All Interactions                         2.70      1    0.1004\n TTR                                       5.66      1    0.0174\n SEX                                       0.19      1    0.6602\n AGE * HRT  (Factor+Higher Order Factors)  2.70      1    0.1004\n TOTAL                                    65.30      9    &lt;.0001\n\n\n\n\nFig 12.1\nMake 2 plots with linear interaction, in the small n=785 sample, and in the full n=40830 sample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 12.2\nMake 4 plots with main effects, linear interaction, and 2 variants of interaction only above age 55. The variable is (Age-55)[+].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the last graph, the green dotted line follows the angle from below age 55 years (only 1 Age effect is estimated for the HRT==0 and HRT==1 and age&lt;55 patients). In the pre-final graph, there are 2 separate angles from age 55 for HRT==0 and HRT==1 (barely noticable for the green dotted line).\n\n\nSmart coding illustration\nSmart coding of age effect: separate for no HRT (HRT==0) and for HRT (HRT==1)\n\n\nCode show/hide\n# Smart coding of age effect: separate for no HRT (HRT==0) and for HRT (HRT==1)\ngustos$AGE0 &lt;- gustos$AGE * (1 - gustos$HRT)\ngustos$AGE1 &lt;- gustos$AGE * gustos$HRT\n# Standard\nlrm(DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = F)\n\n\nLogistic Regression Model\n\nlrm(formula = DAY30 ~ AGE + KILLIP + HIG + DIA + HYP + HRT + \n    TTR + SEX + AGE * HRT, data = gustos, x = T, y = T, linear.predictors = F)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           785    LR chi2      86.28      R2       0.270    C       0.831    \n 0            733    d.f.             9      R2(9,785)0.094    Dxy     0.662    \n 1             52    Pr(&gt; chi2) &lt;0.0001    R2(9,145.7)0.412    gamma   0.662    \nmax |deriv| 1e-09                            Brier    0.051    tau-a   0.082    \n\n          Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept -9.7431 1.6574 -5.88  &lt;0.0001 \nAGE        0.0759 0.0240  3.16  0.0016  \nKILLIP     0.4595 0.2448  1.88  0.0605  \nHIG        0.7900 0.3407  2.32  0.0204  \nDIA        0.7804 0.4403  1.77  0.0763  \nHYP        1.0403 0.5570  1.87  0.0618  \nHRT       -3.6376 2.8352 -1.28  0.1995  \nTTR        0.8201 0.3447  2.38  0.0174  \nSEX       -0.1534 0.3490 -0.44  0.6602  \nAGE * HRT  0.0655 0.0399  1.64  0.1004  \n\n\nCode show/hide\n# Smart coding\nlrm(DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F)\n\n\nLogistic Regression Model\n\nlrm(formula = DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + \n    HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           785    LR chi2      86.28      R2       0.270    C       0.831    \n 0            733    d.f.             9      R2(9,785)0.094    Dxy     0.662    \n 1             52    Pr(&gt; chi2) &lt;0.0001    R2(9,145.7)0.412    gamma   0.662    \nmax |deriv| 1e-09                            Brier    0.051    tau-a   0.082    \n\n          Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept -9.7431 1.6574 -5.88  &lt;0.0001 \nAGE0       0.0759 0.0240  3.16  0.0016  \nAGE1       0.1414 0.0332  4.26  &lt;0.0001 \nHRT       -3.6376 2.8352 -1.28  0.1995  \nKILLIP     0.4595 0.2448  1.88  0.0605  \nHIG        0.7900 0.3407  2.32  0.0204  \nDIA        0.7804 0.4403  1.77  0.0763  \nHYP        1.0403 0.5570  1.87  0.0618  \nTTR        0.8201 0.3447  2.38  0.0174  \nSEX       -0.1534 0.3490 -0.44  0.6602  \n\n\nCode show/hide\n# Identical fit, easier interpretation\n\n# Age 55 as reference for HRT effect\ngustos$AGE0 &lt;- (gustos$AGE - 55) * (1 - gustos$HRT)\ngustos$AGE1 &lt;- (gustos$AGE - 55) * gustos$HRT\nlrm(DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F)\n\n\nLogistic Regression Model\n\nlrm(formula = DAY30 ~ AGE0 + AGE1 + HRT + KILLIP + HIG + DIA + \n    HYP + TTR + SEX, data = gustos, x = T, y = T, linear.predictors = F)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           785    LR chi2      86.28      R2       0.270    C       0.831    \n 0            733    d.f.             9      R2(9,785)0.094    Dxy     0.662    \n 1             52    Pr(&gt; chi2) &lt;0.0001    R2(9,145.7)0.412    gamma   0.662    \nmax |deriv| 4e-11                            Brier    0.051    tau-a   0.082    \n\n          Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept -5.5703 0.5746 -9.69  &lt;0.0001 \nAGE0       0.0759 0.0240  3.16  0.0016  \nAGE1       0.1414 0.0332  4.26  &lt;0.0001 \nHRT       -0.0333 0.7043 -0.05  0.9623  \nKILLIP     0.4595 0.2448  1.88  0.0605  \nHIG        0.7900 0.3407  2.32  0.0204  \nDIA        0.7804 0.4403  1.77  0.0763  \nHYP        1.0403 0.5570  1.87  0.0618  \nTTR        0.8201 0.3447  2.38  0.0174  \nSEX       -0.1534 0.3490 -0.44  0.6602  \n\n\nCode show/hide\n# Even nicer interpretation, HRT effect for age=55\n\n\nThe fit of each of the models is identical (always LR chi2=86.28); each model allows for linear interaction between AGE and HRT. The interpretation of the model is easier if the age effects are estimated for HRT==1 and for HRT==0. Scaling is easier by subtracting 55 from AGE (AGE-55); this implies the HRT effect relates to age 55.\n\n\nTable 12.2 Better predictions?\nAssess the performance if the models created in n=785 in an independent validation part, GustoB, n=20318.\nPlots created with a modification of Frank Harrell’s val.prob() function:\n\n\nCode show/hide\n# Validate in independent part, named gustoB\n# main effects\nlrm.val.full &lt;- predict(full, newdata = gustoB, type = \"lp\")\n# simple interaction\nlrm.val.int1 &lt;- predict(fullints, newdata = gustoB, type = \"lp\")\n\n# Plot\nval.prob.ci.2(\n  y = gustoB[, \"DAY30\"], logit = lrm.val.full, riskdist = \"predicted\", logistic.cal = F,\n  smooth = \"rcs\", nr.knots = 3, g = 8, xlim = c(0, .5), ylim = c(0, .5),\n  legendloc = c(0.18, 0.15), statloc = c(0, .4), roundstats = 3,\n  xlab = \"Predicted probability from n=785\", ylab = \"Observed proportion in n=20318\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions Discrimination: worse with interactions than without. Calibration: We note some overfitting, as expected by a fit in a small sample.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assumptions in Regression Models - Additivity and Linearity</span>"
    ]
  },
  {
    "objectID": "chapters/12-additivity-linearity.html#mfp-and-other-non-linear-analyses-in-n544-data",
    "href": "chapters/12-additivity-linearity.html#mfp-and-other-non-linear-analyses-in-n544-data",
    "title": "12  Assumptions in Regression Models - Additivity and Linearity",
    "section": "MFP and other non-linear analyses in n544 data",
    "text": "MFP and other non-linear analyses in n544 data\nUpcoming.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assumptions in Regression Models - Additivity and Linearity</span>"
    ]
  },
  {
    "objectID": "chapters/13-modern-estimation.html",
    "href": "chapters/13-modern-estimation.html",
    "title": "13  Modern Estimation Methods",
    "section": "",
    "text": "Chapter 13 additional material upcoming.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modern Estimation Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-estimation-external.html",
    "href": "chapters/14-estimation-external.html",
    "title": "14  Estimation with External Information",
    "section": "",
    "text": "Learning from external information",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimation with External Information</span>"
    ]
  },
  {
    "objectID": "chapters/14-estimation-external.html#learning-from-external-information",
    "href": "chapters/14-estimation-external.html#learning-from-external-information",
    "title": "14  Estimation with External Information",
    "section": "",
    "text": "A local model with external information: Table 14.1\nWe can use external information to optimize the development of a prediction model with local or global applicability.\nTable 14.1 provides the basic idea of model development with a focus on a locally applicable model.\n\n\n\nTable 14.1: local vs global models\n\n\n\n\n\n\n\nModeling aspect\nLocal model\nGlobal model\n\n\n\n\nModel specification\nMixture of IPD and literature\nFocus on consensus in literature\n\n\nModel coefficients\nIPD with literature as background\nMeta-analysis of literature\n\n\nBaseline risk\nIPD\nLiterature",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimation with External Information</span>"
    ]
  },
  {
    "objectID": "chapters/14-estimation-external.html#the-data-tbi-n11022-and-aaa-n238-data-sets",
    "href": "chapters/14-estimation-external.html#the-data-tbi-n11022-and-aaa-n238-data-sets",
    "title": "14  Estimation with External Information",
    "section": "The data: TBI (n=11022) and AAA (n=238) data sets",
    "text": "The data: TBI (n=11022) and AAA (n=238) data sets\nThe impact data set includes patients with moderate / severe TBI for 15 studies (Steyerberg 2019). We can learn from the combined information in an Individual Patient Data Meta-Analysis (IPD MA).\nThe AAA data set includes a small set of patients undergoing elective surgery for Abdominal Aortic Aneurysm (Steyerberg 1995). We can learn from univariate information in the literature where many studies are available (Debray 2012). Specifically, we borrow information from the univariate coefficients in other studies and perform an adaptation of the multivariable coefficients in the AAA data set. The resulting model is stabilized such that better local and global performance is expected.\n\n\nCode show/hide\n# impact data come with metamisc package Debray; read here\ndata(\"impact\", package = \"metamisc\")\nimpact$name         &lt;- as.factor(impact$name)\nimpact$ct           &lt;- as.factor(impact$ct)\nimpact$age10        &lt;- impact$age/10 - 3.5 # reference 35-year-old patient, close to mean age\nimpact$motor_score  &lt;- as.factor(impact$motor_score)\nimpact$pupil        &lt;- as.factor(impact$pupil)\nlevels(impact$ct) &lt;- c(\"I/II\", \"III/IV\", \"V/VI\") # correct an error in Debray labels\n\nimpact$motor.lin    &lt;- as.numeric(impact$motor_score) # 1/2, 3, 4, 5/6 linear\nimpact$pupil.lin    &lt;- as.numeric(impact$pupil) # 1, 2, 3 linear\nimpact$study &lt;- as.factor(ifelse(impact$name==\"TINT\", 1,\n                ifelse(impact$name==\"TIUS\", 2,\n                  ifelse(impact$name==\"SLIN\", 3,\n                    ifelse(impact$name==\"SAP\", 4,\n                      ifelse(impact$name==\"PEG\", 5,\n                        ifelse(impact$name==\"HIT I\", 6,\n                          ifelse(impact$name==\"UK4\", 7,\n                            ifelse(impact$name==\"TCDB\", 8,\n                              ifelse(impact$name==\"SKB\", 9,\n                                ifelse(impact$name==\"EBIC\", 10,\n                                  ifelse(impact$name==\"HIT II\", 11,\n                                    ifelse(impact$name==\"NABIS\", 12,\n                                      ifelse(impact$name==\"CSTAT\", 13,\n                                      ifelse(impact$name==\"PHARMOS\", 14,\n                                      ifelse(impact$name==\"APOE\", 15,NA))))))))))))))))\nnames &lt;- levels(impact[,1])\n\nAAA  &lt;- read.csv(\"data/AAA.csv\", row.names = 1)",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimation with External Information</span>"
    ]
  },
  {
    "objectID": "chapters/14-estimation-external.html#the-impact-study",
    "href": "chapters/14-estimation-external.html#the-impact-study",
    "title": "14  Estimation with External Information",
    "section": "The impact study",
    "text": "The impact study\n\nDescriptives\nOverall results are presented below. We note that missing values were imputed (using mice, a single imputation for simple illustrations).\n\n\nCode show/hide\nhtml(describe(impact), scroll=TRUE) \n\n\n\n\n\n\n\nimpact Descriptives\nimpact  15  Variables   11022  Observations\n\nname\n\n \n nmissingdistinct\n 11022015\n \n\n Value         APOE   CSTAT    EBIC   HIT I  HIT II   NABIS     PEG PHARMOS     SAP\n Frequency      756     517     822     350     819     385    1510     856     919\n Proportion   0.069   0.047   0.075   0.032   0.074   0.035   0.137   0.078   0.083\n                                                           \n Value          SKB    SLIN    TCDB    TINT    TIUS     UK4\n Frequency      126     409     603    1118    1041     791\n Proportion   0.011   0.037   0.055   0.101   0.094   0.072 \n\n\ntype\n\n \n nmissingdistinct\n 1102202\n \n\n Value       OBS  RCT\n Frequency  2972 8050\n Proportion 0.27 0.73 \n\n\nage\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 110220800.99934.9317.5817182231465965\n \n\nlowest : 14 15 16 17 18 ,  highest: 89 90 91 92 93\n\nmotor_score\n\n \n nmissingdistinct\n 1102204\n \n\n Value        1/2     3     4   5/6\n Frequency   2850  2285  2438  3449\n Proportion 0.259 0.207 0.221 0.313 \n\n\npupil\n\n \n nmissingdistinct\n 1102203\n \n\n Value       Both  None   One\n Frequency   7325  2296  1401\n Proportion 0.665 0.208 0.127 \n\n\nct\n\n \n nmissingdistinct\n 1102203\n \n\n Value        I/II III/IV   V/VI\n Frequency    4479   2239   4304\n Proportion  0.406  0.203  0.390 \n\n\nhypox\n\n \n nmissingdistinctInfoSumMeanGmd\n 11022020.50723750.21550.3381\n \n\n\nhypots\n\n \n nmissingdistinctInfoSumMeanGmd\n 11022020.42418750.17010.2824\n \n\n\ntsah\n\n \n nmissingdistinctInfoSumMeanGmd\n 11022020.74450120.45470.4959\n \n\n\nedh\n\n \n nmissingdistinctInfoSumMeanGmd\n 11022020.34614640.13280.2304\n \n\n\nmort\n\n \n nmissingdistinctInfoSumMeanGmd\n 11022020.57828740.26080.3856\n \n\n\nage10\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 110220800.999-0.0069591.758-1.8-1.7-1.3-0.4 1.1 2.4 3.0\n \n\nlowest : -2.1 -2   -1.9 -1.8 -1.7 ,  highest: 5.4  5.5  5.6  5.7  5.8 \n\nmotor.lin\n\n \n nmissingdistinctInfoMeanGmd\n 11022040.9322.5881.311\n \n\n Value          1     2     3     4\n Frequency   2850  2285  2438  3449\n Proportion 0.259 0.207 0.221 0.313 \n\n\nFor the frequency table, variable is rounded to the nearest 0\n\npupil.lin\n\n \n nmissingdistinctInfoMeanGmd\n 11022030.6951.4630.6678\n \n\n Value          1     2     3\n Frequency   7325  2296  1401\n Proportion 0.665 0.208 0.127 \n\n\nFor the frequency table, variable is rounded to the nearest 0\n\nstudy\n\n \n nmissingdistinct\n 11022015\n \n\n Value          1     2     3     4     5     6     7     8     9    10    11    12\n Frequency   1118  1041   409   919  1510   350   791   603   126   822   819   385\n Proportion 0.101 0.094 0.037 0.083 0.137 0.032 0.072 0.055 0.011 0.075 0.074 0.035\n                             \n Value         13    14    15\n Frequency    517   856   756\n Proportion 0.047 0.078 0.069 \n\n\n\n\n\nCode show/hide\nkable(table(impact$name, impact$mort), row.names = T, caption = \"Mortality by study\")\n\n\n\nMortality by study\n\n\n\n0\n1\n\n\n\n\nAPOE\n639\n117\n\n\nCSTAT\n402\n115\n\n\nEBIC\n541\n281\n\n\nHIT I\n251\n99\n\n\nHIT II\n631\n188\n\n\nNABIS\n284\n101\n\n\nPEG\n1148\n362\n\n\nPHARMOS\n711\n145\n\n\nSAP\n707\n212\n\n\nSKB\n92\n34\n\n\nSLIN\n315\n94\n\n\nTCDB\n339\n264\n\n\nTINT\n840\n278\n\n\nTIUS\n816\n225\n\n\nUK4\n432\n359\n\n\n\n\n\n\n\nAnalyses for Table 14.2\nWe analyze various model variants for the impact data, with 3 key predictors:\n\nage10: age per decade, continuous; centered at age 40\nmotor.lin: the Motor score component from the Glasgow Coma Scale, continuous for codes 1/2, 3, 4, 5/6\npupil.lin: pupillary reactivity, continuous for codes 1, 2, 3 relating to both, one, or no reacting pupils\n\nThe models are as follows:\n\nA naive analyses of the merged data, ignoring the clustering nature of the data (rms::lrm)\nPer study analyses for each of the 15 studies (rms::lrm)\nStratified analysis, with study as a factor variable to estimate common predictor effects (rms::lrm)\nOne-stage meta-analysis, with study as a random effect to estimate common predictor effects (lme4::glmer)\nTwo-stage univariate meta-analysis, with pooling of the per study estimates obtained in step 2, to obtain random effect estimates for the predictor effects (metamisc::uvmeta)\n\nThe output includes estimates for the model intercept and the predictor effects. Standard errors (SE) are obtained for each estimate. Moreover, the 1-step and 2-step meta-analyses estimate the heterogeneity parameter tau, which reflects between study differences in the estimates. This heterogeneity is considered in the random effect 95% confidence intervals, and in 95% prediction intervals.\n\n\nCode show/hide\n# store the results in 2 matrices to produce something close to Table 14.2\ncoef.matrix &lt;- matrix(nrow=23, ncol=4)\nse.matrix &lt;- matrix(nrow=23, ncol=4)\ndimnames(coef.matrix) &lt;- dimnames(se.matrix) &lt;- \n  list(c(levels(impact$study), \"naive\",\n      \"stratified\", \"one-stage\", \"tau-1\", \"two-stage\", \"tau-2\", \"Low pred\", \"High pred\")\n                      ,Cs(Intercept,age,motor,pupils))\n\n# naive merged model\nfit0 &lt;- lrm(mort~age10+motor.lin+pupil.lin, data=impact)\ncoef.matrix[16,] &lt;- coef(fit0)  \nse.matrix[16,] &lt;- diag(se(fit0))\n\nfor (i in 1:15) {\n# fit stratified models\nfit.lin &lt;-lrm(mort~age10+motor.lin+pupil.lin, data=impact, subset=impact$study==i)\ncoef.matrix[i,] &lt;- coef(fit.lin)  \nse.matrix[i,] &lt;- diag(se(fit.lin)) }\n\n# global model with stratification; intercept is for Study==1\nfit.stratified &lt;- lrm(mort~age10+motor.lin+pupil.lin +name, data=impact)\ncoef.matrix[17,2:4] &lt;- coef(fit.stratified)[2:4]  \nse.matrix[17,2:4] &lt;- diag(se(fit.stratified))[2:4] \n\n# Estimate heterogeneity in 1 step, global model for covariate effects\nfit.lin.meta &lt;- glmer(mort~ (1 | name) + age10  + motor.lin+pupil.lin, \n                      family =binomial(), data = impact)\ncoef.matrix[18,]  &lt;- fit.lin.meta@beta\nse.matrix[18,]    &lt;- sqrt(diag(vcov(fit.lin.meta)))\ncoef.matrix[19,1] &lt;- fit.lin.meta@theta\n\n# Estimate heterogeneity in 2 step model\nfor (i in 1:4) {\nimpact.meta &lt;- uvmeta( r=coef.matrix[1:15,i], r.se=se.matrix[1:15,i])\ncoef.matrix[20,i] &lt;- impact.meta$est\nse.matrix[20,i]   &lt;- impact.meta$se\ncoef.matrix[21,i] &lt;- sqrt(impact.meta$tau)\ncoef.matrix[22,i] &lt;- impact.meta$pi.lb\ncoef.matrix[23,i] &lt;- impact.meta$pi.ub\n} # end loop over 4 columns of per study estimates\n\n# Made Tabe 14.2; SE estimates separate\nkable(coef.matrix, row.names = T, col.names = NA, caption = \"Coefficients and heterogeneity by study\")\n\n\n\nCoefficients and heterogeneity by study\n\n\n\nIntercept\nage\nmotor\npupils\n\n\n\n\n1\n-0.348\n0.275\n-0.479\n0.380\n\n\n2\n-0.177\n0.288\n-0.577\n0.360\n\n\n3\n-0.916\n0.339\n-0.367\n0.495\n\n\n4\n-1.564\n0.260\n-0.259\n0.616\n\n\n5\n-0.748\n0.266\n-0.604\n0.534\n\n\n6\n-0.892\n0.337\n-0.515\n0.586\n\n\n7\n0.060\n0.412\n-0.607\n0.640\n\n\n8\n0.302\n0.485\n-0.677\n0.629\n\n\n9\n-1.023\n0.427\n-0.239\n0.414\n\n\n10\n-0.432\n0.420\n-0.690\n0.696\n\n\n11\n-0.752\n0.335\n-0.440\n0.277\n\n\n12\n-0.976\n0.220\n-0.386\n0.526\n\n\n13\n-1.382\n0.295\n-0.456\n0.806\n\n\n14\n-1.157\n0.255\n-0.360\n0.298\n\n\n15\n-1.375\n0.553\n-0.846\n1.035\n\n\nnaive\n-0.720\n0.339\n-0.512\n0.555\n\n\nstratified\nNA\n0.343\n-0.517\n0.528\n\n\none-stage\n-0.677\n0.342\n-0.517\n0.530\n\n\ntau-1\n0.356\nNA\nNA\nNA\n\n\ntwo-stage\n-0.735\n0.343\n-0.507\n0.538\n\n\ntau-2\n0.438\n0.070\n0.121\n0.135\n\n\nLow pred\n-1.730\n0.182\n-0.784\n0.227\n\n\nHigh pred\n0.260\n0.503\n-0.231\n0.849\n\n\n\n\n\n\n\nEstimation of coefficients: naive, stratified and IPD-MA\nWe consider the estimates for the multivariable coefficients of 3 predictors: age, motor score, and pupils. Per study differences are modest; a forest plot might visualize the patterns. The model is:\nlrm(mort~age10+motor.lin+pupil.lin, data=impact, subset=impact$study==i)\nA naive summary estimate ignores the pooling:\nlrm(mort~age10+motor.lin+pupil.lin, data=impact)\nIt produces overall estimates that are only slightly different than the estimates from a stratified model (with study as fixed effect). Also, similar estimates come for a one-step random effect analysis (with lrm4::glmer), or a two-step random effect analysis (with metamisc::uvmeta). Overall, the summary effect estimates for the predictors are quite similar.\nThe intercept differences between the studies are more substantial, but the summary estimates of the overall baseline risks are similar, all close to the naive estimate of -0.55. For the stratified analysis, a specific study intercept is taken as the reference, so the overall estimate is not available directly from lrm.\nThe between study variability is quantified with the parameter tau: slot @beta in glmer, and $tau in metamisc::uvmeta. The tau estimates were 0.356 with a one-step random effect analysis (with lrm4::glmer); and 0.438 with a two-step random effect analysis (with metamisc::uvmeta). Note that the latter tau estimate for the intercept depends on the coding of the predictors; it is the heterogeneity for a reference patient with zero values for the covariates, in our case; age 35, poor motor score and both pupils reacting.\nThe prediction interval (from metamisc::uvmeta) was quite wide: [-1.7 - .2], or odds ranging from exp(-1.73)=0.177 to exp(0.26)=1.3, or mortality risks between plogis(-1.73)=15% to plogis(0.26)=56% for a reference patient with all covariate values set to zero (age 35, poor motor score and both pupils reacting). We can also obtain prediction intervals for the predictor effects with the two-stage approach, which shows wider intervals than the standard 95% confidence intervals (either from fixed or random effect meta-analysis).\n\n\nEstimation of standard errors\nThe SE estimates (see below) for the predictor effects are very similar with a naive, stratified, or one-stage approach: 0.015 for age, around 0.02 for motor, and 0.03 for pupils. The pooled SEs are substantially smaller than the SE estimates per study. Even for the larger studies, the SEs are substantially wider: around 0.05 for age, around 0.07 for motor, and 0.09 for pupils. These SEs are fixed effect estimates, assuming a single, uniform association of each predictor with the outcome, 6-month mortality.\nThe random effect estimates allow for between study heterogeneity, and are wider: 0.024 for age, around 0.04 for motor, and 0.05 for pupils. For the model intercept, the naive approach underestimates the uncertainty by ignoring the clustering of the data. The one-stage and two-stage meta-analysis approach largely agree (SE = 0.12 - 0.14).\n\n\nCode show/hide\nkable(se.matrix, row.names = T, col.names = NA, caption = \"SE estimates\")\n\n\n\nSE estimates\n\n\n\nIntercept\nage\nmotor\npupils\n\n\n\n\n1\n0.271\n0.050\n0.071\n0.093\n\n\n2\n0.322\n0.062\n0.076\n0.113\n\n\n3\n0.423\n0.089\n0.119\n0.143\n\n\n4\n0.267\n0.052\n0.070\n0.105\n\n\n5\n0.213\n0.046\n0.061\n0.095\n\n\n6\n0.394\n0.084\n0.121\n0.169\n\n\n7\n0.322\n0.046\n0.081\n0.121\n\n\n8\n0.349\n0.065\n0.089\n0.152\n\n\n9\n0.652\n0.158\n0.197\n0.288\n\n\n10\n0.316\n0.046\n0.082\n0.131\n\n\n11\n0.282\n0.056\n0.080\n0.120\n\n\n12\n0.402\n0.095\n0.109\n0.164\n\n\n13\n0.355\n0.084\n0.107\n0.134\n\n\n14\n0.302\n0.066\n0.091\n0.110\n\n\n15\n0.488\n0.068\n0.134\n0.185\n\n\nnaive\n0.079\n0.015\n0.021\n0.032\n\n\nstratified\nNA\n0.015\n0.022\n0.032\n\n\none-stage\n0.123\n0.015\n0.022\n0.032\n\n\ntau-1\nNA\nNA\nNA\nNA\n\n\ntwo-stage\n0.142\n0.024\n0.041\n0.050\n\n\ntau-2\nNA\nNA\nNA\nNA\n\n\nLow pred\nNA\nNA\nNA\nNA\n\n\nHigh pred\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nEstimation for a specific study, assuming a global model holds\nWe can estimate the intercept for study 14 with fixed estimates from the stratified model:\nMortality | Study 14 ~ intercept14 + offset(global linear predictor).\n\n\nCode show/hide\n# Stratified model, fixed effects\nfit.stratified &lt;- lrm(mort~age10+motor.lin+pupil.lin +name, data=impact)\nstudy14 &lt;- impact[impact$study==14, ] # selected reference set\nfit.14 &lt;- lrm.fit(y=study14$mort, \n                  offset=as.matrix(study14[,c(\"age10\",  \"motor.lin\", \"pupil.lin\")]) %*%\n                    fit.stratified$coefficients[2:4] )\n\ncat(\"Fixed effect estimates for study 14, Tirilazad US (TIUS)\\n\")\n\n\nFixed effect estimates for study 14, Tirilazad US (TIUS)\n\n\nCode show/hide\nprint(c(coef(fit.14), fit.stratified$coefficients[2:4]) )\n\n\nIntercept     age10 motor.lin pupil.lin \n   -1.179     0.343    -0.517     0.528",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimation with External Information</span>"
    ]
  },
  {
    "objectID": "chapters/14-estimation-external.html#the-aaa-study",
    "href": "chapters/14-estimation-external.html#the-aaa-study",
    "title": "14  Estimation with External Information",
    "section": "The AAA study",
    "text": "The AAA study\n\nDescriptives\nThe AAA data set is rather small, with n=238 patients undergoing elective surgery for an Abdominal Aortic Aneurysm, and only 18 events (STATUS==1). The data set consists of patients operated on at the University Hospital Leiden (the Netherlands) between 1977 and 1988 (Steyerberg 1995).\n\n\nCode show/hide\nhtml(describe(AAA), scroll=TRUE) \n\n\n\n\n\n\n\nAAA Descriptives\nAAA  8  Variables   238  Observations\n\nSEX\n\n \n nmissingdistinctInfoSumMeanGmd\n 238020.241210.088240.1616\n \n\n\nAGE10\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 2380550.9996.6350.84325.4005.6006.1006.7007.1007.5007.815\n \n\nlowest : 4.3 4.5 4.9 4.9 5   ,  highest: 7.9 7.9 8   8.2 8.4\n\nMI\n\n \n nmissingdistinctInfoSumMeanGmd\n 238020.553580.24370.3702\n \n\n\nCHF\n\n \n nmissingdistinctInfoSumMeanGmd\n 238020.669800.33610.4482\n \n\n\nISCHEMIA\n\n \n nmissingdistinctInfoSumMeanGmd\n 238020.681830.34870.4562\n \n\n\nLUNG\n\n \n nmissingdistinctInfoSumMeanGmd\n 238020.46450.18910.3079\n \n\n\nRENAL\n\n \n nmissingdistinctInfoSumMeanGmd\n 238020.177150.063030.1186\n \n\n\nSTATUS\n\n \n nmissingdistinctInfoSumMeanGmd\n 238020.21180.075630.1404\n \n\n\n\n\n\n\n\nLiterature, univariate estimates: Table 14.4\nWe consider the development of a prediction model for in-hospital mortality after elective surgery for AAA. We consider predictors that are identified as relevant in the literature and that are available in the local data. We performed a literature review to obtain univariate estimates of regression coefficients from publications in PubMed between 1980 and 1994. The selection was limited to English-language studies, which had to contain frequency data on the association of a potential predictor and surgical mortality, either in tables or mentioned in the text.\nThe pooled log odds ratio estimates (=logistic regression coefficients) from a fixed effect or random effect procedure. The estimates agreed up to 2 decimals usually. We compare the univariate coefficients from the literature (fixed or random pooled estimates) to the estimates from the local data.\n\nthe effect for SEX was small, a slightly stronger in the literature (0.36 vs 0.28)\nthe age effect (per decade) was strong. A literature coefficient of 0.79 (slightly smaller than the local 0.98 estimate) implies a exp(0.79)=2.2 times higher odds of mortality per 10 years older. So, as often in surgical procedures, age was a strong predictor (Finlayson 2001)\nthe univariate estimates for cardiac comorbidity (a history of MI, presence of CHF, presence of ISCHEMIA) were all a bit smaller in the literature than the local data\nLUNG or RENAL comorbidity showed quite similar univariate associations in the literature and the IPD, so using or ignoring the literature would result in similar coefficient estimates\n\nThe standard errors (SE) were smaller for the literature estimates, as expected. The pooled estimates of the predictors SEX and AGE10 were based on large numbers and many studies, especially for age. This leads to a relatively small fixed effect SE. For other predictors, the number of studies was small leading to a modestly smaller SE, e.g. for cardiac, lung and renal comorbidity. The random effect SE was larger for SEX and AGE10, including the between study heterogeneity in effect estimates. We used the random effect estimates of the coefficients with the random effect SE for the adaptation procedures.\n\n\nCode show/hide\n# The following estimates come from a univariable logistic regression analysis\n# Fixed effects, better random effect estimates\nlit.coefficients.f  &lt;- c(log(1.44),log(2.20),log(2.80),log(4.89),log(4.58),\n                        log(3.75),log(2.43))\nlit.se.f            &lt;- c(0.08,0.06,0.27,0.33,0.31,0.25,0.23)\n# Random effects\nlit.coefficients    &lt;- c(0.3606,0.788,1.034,1.590,1.514,1.302,0.8502)\nlit.se          &lt;- c(0.176,0.112,0.317,0.4109,0.378,0.2595,0.2367)\n\n# Local data multivariable estimates\nfull &lt;- lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA,x=T,y=T)\n\n# Local data univariate estimates\nlocal.coef &lt;- local.se &lt;- rep(NA, 7)\nfor (i in 1:7) {\nfit.ind.x       &lt;- lrm.fit(y=full$y,x=full$x[,i],maxit=25)\nlocal.coef[i] &lt;- fit.ind.x$coef[2]\nlocal.se[i]   &lt;- sqrt(fit.ind.x$var[2,2]) }\n\n# Table 14.4\ntab14 &lt;- cbind(local.coef, local.se, \n            lit.coefficients.f, lit.se.f, \n            lit.coefficients, lit.se)\nrownames(tab14) &lt;- names(full$coefficients[-1])\ncolnames(tab14) &lt;- c(\"local.coef\", \"local.se\", \n                     \"fixed.coef\", \"fixed.se\", \n                     \"random.coef\", \"random.se\")\nkable(tab14, digits=2, caption = \"Univariate coefficients\")\n\n\n\nUnivariate coefficients\n\n\n\n\n\n\n\n\n\n\n\n\nlocal.coef\nlocal.se\nfixed.coef\nfixed.se\nrandom.coef\nrandom.se\n\n\n\n\nSEX\n0.28\n0.79\n0.36\n0.08\n0.36\n0.18\n\n\nAGE10\n0.98\n0.38\n0.79\n0.06\n0.79\n0.11\n\n\nMI\n1.50\n0.50\n1.03\n0.27\n1.03\n0.32\n\n\nCHF\n1.78\n0.55\n1.59\n0.33\n1.59\n0.41\n\n\nISCHEMIA\n1.72\n0.55\n1.52\n0.31\n1.51\n0.38\n\n\nRENAL\n1.24\n0.70\n1.32\n0.25\n1.30\n0.26\n\n\nLUNG\n0.84\n0.53\n0.89\n0.23\n0.85\n0.24\n\n\n\n\n\n\n\nA simple prediction model: Table 14.5\nWe may fit a simple model for in-hospital mortality based on predictors that are known to be relevant from the literature, and that are available in the local data. The standard logistic model with maximum likelihood is obtained from: lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA)\nWe may apply a shrinkage factor, estimated by bootstrapping using the rms::validate function. And finally we explore penalized logistic regression, using rms::pentrace. We find that\n\nthe estimated coefficients are shrunken towards zero with both the bootstrap shrinkage approach and the penalized maximum likelihood approach.\nthe estimated shrinkage factor is 0.66, the reduction in c statistic is from 0.83 to 0.76, the reduction in R^2 is from 24 to 10%; this is all in agreement with the small effective sample size (18 events).\n\n\n\nCode show/hide\nfull &lt;- lrm(STATUS~SEX+AGE10+MI+CHF+ISCHEMIA+RENAL+LUNG,data=AAA,x=T,y=T)\n\n# Estimate shrinkage factor\nset.seed(1)\nfull.validate &lt;- validate(full, B=500, maxit=10)\n\n\n\nDivergence or singularity in 3 samples\n\n\nCode show/hide\n# Apply shrinkage factor on model coefficients\nfull.shrunk.coefficients    &lt;- full.validate[\"Slope\",\"index.corrected\"] * full$coefficients\n# Adjust intercept with lp as offset variable\nlp.AAA  &lt;- full$x %*% full.shrunk.coefficients[2:(ncol(full$x)+1)]\nfit.offset  &lt;- lrm.fit(y=full$y, offset=lp.AAA)\nfull.shrunk.coefficients[1] &lt;- fit.offset$coef[1]\n\n# Full model, penalized estimation\npenalty &lt;- pentrace(full,penalty=c(0.5,1,2,3,4,6,8,12,16,24,36,48),maxit=25)\nplot(penalty)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode show/hide\ncat(\"The optimal penalty was\", penalty$penalty, \"for effective df around 3.5 rather than 7 df\\n\")\n\n\nThe optimal penalty was 16 for effective df around 3.5 rather than 7 df\n\n\nCode show/hide\nfull.penalized &lt;- update(full, penalty=penalty$penalty)\n\n# the 3 sets of coefs\ncoef.mat &lt;- cbind(full$coef, full.shrunk.coefficients, full.penalized$coef )\ncolnames(coef.mat) &lt;- c(\"Full, ML\", \"Shrinkage\", \"Penalized\")\n\nkable(coef.mat, digits=2, \n      caption = \"Estimated local multivariable regression coefficients in IPD\")\n\n\n\nEstimated local multivariable regression coefficients in IPD\n\n\n\nFull, ML\nShrinkage\nPenalized\n\n\n\n\nIntercept\n-8.10\n-6.04\n-5.60\n\n\nSEX\n0.30\n0.19\n0.16\n\n\nAGE10\n0.58\n0.38\n0.32\n\n\nMI\n0.74\n0.49\n0.55\n\n\nCHF\n1.04\n0.68\n0.64\n\n\nISCHEMIA\n0.99\n0.65\n0.60\n\n\nRENAL\n1.12\n0.74\n0.71\n\n\nLUNG\n0.61\n0.40\n0.37\n\n\n\n\n\nCode show/hide\n# shrinkage factor: slope of lp\ncstat &lt;- full.validate[1,]\ncstat[c(1:3,5)] &lt;- full.validate[1,c(1:3,5)] * 0.5 + 0.5 # c = D/2 + 0.5\ncstat[4] &lt;- full.validate[1,4] * 0.5 # optimism in c\n\n# show key performance measures, including c statistic, R2, and estimated shrinkage factor\nkable(rbind(cstat, full.validate[1:4,]), digits = 2, \n      caption = \"Internally validated performance\" )\n\n\n\nInternally validated performance\n\n\n\n\n\n\n\n\n\n\n\n\nindex.orig\ntraining\ntest\noptimism\nindex.corrected\nn\n\n\n\n\ncstat\n0.83\n0.86\n0.79\n0.07\n0.76\n497\n\n\nDxy\n0.66\n0.72\n0.57\n0.15\n0.52\n497\n\n\nR2\n0.24\n0.32\n0.17\n0.14\n0.10\n497\n\n\nIntercept\n0.00\n0.00\n-0.69\n0.69\n-0.69\n497\n\n\nSlope\n1.00\n1.00\n0.66\n0.34\n0.66\n497\n\n\n\n\n\n\n\nAdaptation of univariate coefficients: Table 14.6\nWe implement two variants of an “adaptation method” that takes advantage of univariate literature data in the estimation of the multivariable regression coefficients in a local prediction model.\nAdaptation method 1 is simple: beta m|(I+L) = beta u|L + (beta m|I - beta u|I), where beta m|(I+L) is the set of multivariable coefficients (“m”) for the predictors considering both individual patient data (“I”) and literature data (“L”). The univariate coefficients are denoted as “beta u”.\nThe adaptation factor (beta m|I - beta u|I) is the difference between multivariable and univariate coefficient in the IPD data set. The adaptation factor adapts beta u|L, the set of univariate estimates from the literature.\nAdaptation method 2 is a bit more complex: beta m|(I+L) = beta m|I + c * (beta u|L - beta u|I), with c a factor between 0 and 1.\n\nWith c=0, adaptation method 2 is ignoring literature information; we simply use the mulktivariable coefficients from our local data.\nWith c=1, adaptation method 2 is the same as adaptation method 1, rewritten as:\nbeta m|(I+L) = beta m|I + beta u|L - beta u|I.\nThe optimal value if c can be derived as a combination of the variance estimates of the components beta m|, beta u|L, and beta u|I with the correlation between univariate and multivariable coefficient in the individual patient data: r(beta m|I, beta u|I). The latter correlation can be estimated empirically by a bootstrap procedure. We estimate coefficients in many bootstap samples drawn with replacement and calculate the correlation after excluding outliers.\n\n\n\nCode show/hide\nset.seed(1)\n# we need the user written function\nfull.uni.mult.cor   &lt;- bootcor.uni.mult(full, group=full$y, B=500, maxit=10, \n                                      trim=.1,save.indices = T)\n\n\nBootsample: 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 \nNumber of valid bootstraps for correlation and shrinkage:  484\nCorrelation coefficients: 0.36 0.84 0.91 0.9 0.89 0.97 0.91 \nShrinkage: 0.72 \n\n\nCode show/hide\n# outliers are removed by 'outliers' package\n# full.uni.mult.cor[4]  # matrix with results per bootstrap\n\n# now do adaptation method approach\nadapt.factors   &lt;- adapt.coefficients   &lt;- adapt.var &lt;- adapt.coef.Gr   &lt;- adapt.var.Gr &lt;- \n  rep(0,7)\n\n# Adaptation method 1, simple, just take the difference in own data between m and u estimates for coefficients\nfor (i in 1:7) {\nfit.ind.x       &lt;- lrm.fit(y=full$y,x=full$x[,i],maxit=25)\nadapt.coef.Gr[i] &lt;- full$coefficients[i+1] + \n                1 * (lit.coefficients[i] - fit.ind.x$coefficients[2])\nadapt.var.Gr[i] &lt;- full$var[i+1,i+1] - fit.ind.x$var[2,2] + lit.se[i]^2 \n } # end adaptation 1\n\n# Adaptation method 2: optimal adaptation factors\nfor (i in 1:7) {\nfit.ind.x       &lt;- lrm.fit(y=full$y,x=full$x[,i],maxit=25)\nadapt.factors[i]    &lt;- (full.uni.mult.cor$r[i] * \n                       sqrt(full$var[i+1,i+1]) * sqrt(fit.ind.x$var[2,2]) ) / \n                      (lit.se[i]^2 + fit.ind.x$var[2,2])\n\nadapt.coefficients[i]   &lt;- full$coefficients[i+1] + \n                    adapt.factors[i] * (lit.coefficients[i] - fit.ind.x$coefficients[2])\n\nadapt.var[i]        &lt;- full$var[i+1,i+1] * (1-(full.uni.mult.cor$r[i]^2 * fit.ind.x$var[2,2] / \n                (lit.se[i]^2 + fit.ind.x$var[2,2]))) } # end adaptation 2\n\n\n# Store all coefficients and related measures in a nice matrix\ntab14 &lt;- cbind(lit.coefficients, local.coef, full$coef[-1], full.uni.mult.cor$r,\n               adapt.factors, adapt.coef.Gr, adapt.coefficients, \n               full.shrunk.coefficients[-1], full.penalized$coef[-1] )\nrownames(tab14) &lt;- names(full$coefficients[-1])\ncolnames(tab14) &lt;- c(\"random.coef.u\", \"local.coef.u\", \"local.coef.m\", \"r\",\n                     \"adapt.opt\", \"adapted.1.m\", \"adapted.2.m\",\n                     \"shrunk.m\", \"penalized.m\")\nkable(tab14, digits=2, caption = \"Coefficients for AAA modeling\")\n\n\n\nCoefficients for AAA modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrandom.coef.u\nlocal.coef.u\nlocal.coef.m\nr\nadapt.opt\nadapted.1.m\nadapted.2.m\nshrunk.m\npenalized.m\n\n\n\n\nSEX\n0.36\n0.28\n0.30\n0.36\n0.38\n0.38\n0.33\n0.19\n0.16\n\n\nAGE10\n0.79\n0.98\n0.58\n0.84\n0.79\n0.38\n0.42\n0.38\n0.32\n\n\nMI\n1.03\n1.50\n0.74\n0.91\n0.74\n0.27\n0.40\n0.49\n0.55\n\n\nCHF\n1.59\n1.78\n1.04\n0.90\n0.63\n0.85\n0.92\n0.68\n0.64\n\n\nISCHEMIA\n1.51\n1.72\n0.99\n0.89\n0.68\n0.79\n0.86\n0.65\n0.60\n\n\nRENAL\n1.30\n1.24\n1.12\n0.97\n0.94\n1.18\n1.18\n0.74\n0.71\n\n\nLUNG\n0.85\n0.84\n0.61\n0.91\n0.84\n0.62\n0.62\n0.40\n0.37\n\n\n\n\n\n\nAdaptation results: coefficients in Table 14.6\nThe first 3 columns show the estimates that are input for the adaptation approaches:\n\nthe random effect pooled estimates for the univariate associations (random.coef.u)\nthe local, or IPD, univariate associations (local.coef.u); and\nthe local multivariable associations (local.coef.m).\n\nThe empirically estimated correlation r between local univariate and multivariable coefficients was between 0.36 for SEX and around 0.9 for other the predictors. The optimal adaptation factor (adapt.opt) was closely related to the estimate of r. Adaptation method 1 or 2 lead to similar estimates for the multivariable coefficient (beta m|(I+L), denoted as adapted.1.m and adapted.2.m). The adapted estimates are larger than shrunk or penalized estimates that only consider the local IPD.\n\n\nCode show/hide\n# Store all standard error estimates in a nice matrix\ntab14 &lt;- cbind(lit.se, local.se, sqrt(diag(full$var)[-1]),\n               sqrt(adapt.var.Gr), sqrt(adapt.var), \n              sqrt(diag(full.penalized$var)[-1] ))\n# Reduction in variance\nreduct.var &lt;- 100 - round(100*adapt.var / diag(full$var)[-1])\ntab14 &lt;- cbind(tab14, reduct.var)\n\nrownames(tab14) &lt;- names(full$coefficients[-1])\ncolnames(tab14) &lt;- c(\"random.u\", \"local.u\", \"local.m\",\n                     \"adapted.1\", \"adapted.2\",\n                     \"penalized.m\", \"% reduction variance adapt 2\")\nkable(tab14, digits=2, caption = \"SE estimates and reduction in variance by adaptation\")\n\n\n\nSE estimates and reduction in variance by adaptation\n\n\n\n\n\n\n\n\n\n\n\n\n\nrandom.u\nlocal.u\nlocal.m\nadapted.1\nadapted.2\npenalized.m\n% reduction variance adapt 2\n\n\n\n\nSEX\n0.18\n0.79\n0.86\n0.40\n0.81\n0.61\n13\n\n\nAGE10\n0.11\n0.38\n0.39\n0.14\n0.23\n0.24\n65\n\n\nMI\n0.32\n0.50\n0.57\n0.41\n0.36\n0.39\n60\n\n\nCHF\n0.41\n0.55\n0.59\n0.47\n0.41\n0.38\n52\n\n\nISCHEMIA\n0.38\n0.55\n0.62\n0.48\n0.42\n0.38\n54\n\n\nRENAL\n0.26\n0.70\n0.77\n0.41\n0.31\n0.62\n83\n\n\nLUNG\n0.24\n0.53\n0.59\n0.34\n0.33\n0.43\n69\n\n\n\n\n\n\n\nAdaptation results: variance in Table 14.6\nAs discussed above, the standard errors (SEs) from random effect pooling of literature data are notably smaller than those for the local univariate coefficients. The local multivariable coefficients have larger SEs, as always for logistic regression models (Robinson & Jewell, 1991).\nThe adaptation methods suggest substantially smaller SEs. For SEX the reduction is only 13%, reflecting the poor correlation between multivariable and univariable coefficients in the IPD (r=0.36). Over 50% reduction in variance is obtained for the other estimates; assuming a global prediction model holds with respect to the predictor effects.\n\n\nCode show/hide\n# Round the coefficients after shrinkage 0.9\nrounded.coef &lt;- round(0.9*adapt.coefficients,1)\n# we estimate the intercept\nX   &lt;- as.matrix(full$x)\nX[,2] &lt;- X[,2] - 7 # center around age=7 decades (70 years)\n# calculate linear predictor, used as offset in lrm.fit\noffset.AAA  &lt;- X %*% rounded.coef\nfit.offset  &lt;- lrm.fit(y=full$y, offset=offset.AAA)\n# Recalibrate baseline risk to 5% (odds(0.05.0.95)); observed was 18/238, or odds 18/220\nrecal.intercept &lt;- fit.offset$coef[1] + log((0.05/0.95)/(18/220))\n\n###################################################################\n# Alternative, crude naive Bayes approach \n# Recalibrate literature coefficients with one calibration factor \n# calculate linear predictor, used as the only x variable in lrm.fit\nlit.score   &lt;- X %*% lit.coefficients\nfit.lit  &lt;- lrm.fit(y=full$y, x=lit.score)\n\nprint(fit.lit, digits=2) # uni coefs model fit\n\n\nLogistic Regression Model\n\nlrm.fit(x = lit.score, y = full$y)\n\n                       Model Likelihood     Discrimination    Rank Discrim.    \n                             Ratio Test            Indexes          Indexes    \nObs           238    LR chi2      25.14     R2       0.242    C       0.835    \n 0            220    d.f.             1     R2(1,238)0.096    Dxy     0.669    \n 1             18    Pr(&gt; chi2) &lt;0.0001    R2(1,49.9)0.384    gamma   0.670    \nmax |deriv| 1e-08                           Brier    0.061    tau-a   0.094    \n\n          Coef  S.E. Wald Z Pr(&gt;|Z|)\nIntercept -4.08 0.56 -7.35  &lt;0.0001 \nx[1]       0.69 0.16  4.44  &lt;0.0001 \n\n\nCode show/hide\n# 3 formulas for risk calculation\nrecal.intercept.lit &lt;- fit.lit$coef[1] + log((0.05/0.95)/(18/220))\n\ntab14 &lt;- cbind(full$coefficients, \n               full.penalized$coefficients, \n               c(recal.intercept, rounded.coef),\n               c(NA, lit.coefficients),\n               c(recal.intercept.lit, fit.lit$coef[2]*lit.coefficients))\n\nrownames(tab14) &lt;- names(full$coefficients)\ncolnames(tab14) &lt;- c(\"full.coef\", \"penalized.m\", \"adapted\", \"lit coefs\", \"lit score\")\nkable(tab14, digits=1, \n      caption = \"Rounded coefficients with IPD only, adaptation approach, or naive Bayes score\")\n\n\n\nRounded coefficients with IPD only, adaptation approach, or naive Bayes score\n\n\n\nfull.coef\npenalized.m\nadapted\nlit coefs\nlit score\n\n\n\n\nIntercept\n-8.1\n-5.6\n-4.1\nNA\n-4.5\n\n\nSEX\n0.3\n0.2\n0.3\n0.4\n0.3\n\n\nAGE10\n0.6\n0.3\n0.4\n0.8\n0.5\n\n\nMI\n0.7\n0.5\n0.4\n1.0\n0.7\n\n\nCHF\n1.0\n0.6\n0.8\n1.6\n1.1\n\n\nISCHEMIA\n1.0\n0.6\n0.8\n1.5\n1.0\n\n\nRENAL\n1.1\n0.7\n1.1\n1.3\n0.9\n\n\nLUNG\n0.6\n0.4\n0.6\n0.9\n0.6\n\n\n\n\n\n\n\nA score for prediction in AAA patients: Table 14.6\nPrediction of mortality in AAA patients could be based on the simple full model estimates, based on maximum likelihood (full.coef). Shrinkage or penalization leads to estimates closer to zero (penalized.m). The adaptation results were shrunk with an overall factor of 0.9, based on the empirical behavior in the GUSTO data set, as described in Table 14.3, section 14.1.9 of Clinical Prediction Models. The adapted estimates are rather somewhat in between the full.coef and penalized.mestimates, and are kind of compromise with the univariate literature estimates (lit coefs). Smaller adapted estimates than in the penalized model are obtained for MI (history of a myocardial infarction), reflecting the finding of a univariate estimate of 1.5 in the IPD versus 1.0 in the literature.\nAn alternative approach is to calibrate a score based on the univariate literature coefficients. The calibration factor is 0.69, estimated by using the score as a single predictor:\nlit.score  &lt;- X %*% lit.coefficients\nfit.lit     &lt;- lrm.fit(y=full$y, x=lit.score)\nFinally, we can calibrate the scores to an average risk of 5%. This recalibration was implemented by the log(odds ratio) for odds(5%) / odds(case study) to the estimated logistic regression model intercept :\nfit.offset$coef[1] + log((0.05/0.95)/(18/220))\n\n\nCode show/hide\n# fit with adapted coefficients in score to obtain c statistics\nadapt.fit &lt;- lrm.fit(y=full$y, x=offset.AAA)\ntab14 &lt;- matrix(c(full$stats[6], full.penalized$stats[6], adapt.fit$stats[6], fit.lit$stats[6]), nrow=1)\nrownames(tab14) &lt;- \"Apparent c statistic\"\ncolnames(tab14) &lt;- c(\"full.coef\", \"penalized.m\", \"adapted\", \"lit score\")\nkable(tab14, digits=3, caption = \"C stats\")\n\n\n\nC stats\n\n\n\nfull.coef\npenalized.m\nadapted\nlit score\n\n\n\n\nApparent c statistic\n0.832\n0.833\n0.829\n0.835\n\n\n\n\n\nCode show/hide\n# calibration\ntab14 &lt;- matrix(c(adapt.fit$coef[2], 1/adapt.fit$coef[2] ), nrow=1)\nrownames(tab14) &lt;- \"\"\ncolnames(tab14) &lt;- c(\"adapted coefficient\", \"adapted effective shrinkage\")\nkable(tab14, digits=3, caption = \"Calibration insights\")\n\n\n\nCalibration insights\n\n\n\nadapted coefficient\nadapted effective shrinkage\n\n\n\n\n\n1.29\n0.778\n\n\n\n\n\nThe apparent discriminative performance is all similar for these sets of coefficients, with c statistics around 0.83. We noted from the bootstrap procedure above that the internally validated estimate was 0.80 rather than 0.83. We do not know what the validated performance is for the model variants based on literature data (adapted; lit score).\nFor calibration, we noted that the estimated shrinkage factor for the full model was around 0.7. When we fit a model with adapted coefficients, the coefficient is 1.29; so an effective shrinkage of 1/1.29 = 0.78 was built in by considering the literature estimates in an adaptation approach.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimation with External Information</span>"
    ]
  },
  {
    "objectID": "chapters/14-estimation-external.html#conclusions",
    "href": "chapters/14-estimation-external.html#conclusions",
    "title": "14  Estimation with External Information",
    "section": "Conclusions",
    "text": "Conclusions\nFindings from other studies can be used in various ways to develop a locally applicable prediction model.\n\nThe modeling with impact data for TBI patients illustrated an IPD MA approach. A global model can readily be derived with a local baseline risk estimate.\nThe modeling with AAA data for aneurysm surgery illustrated two variants of adaptation approaches:\n\nsimple, subtracting the difference in IPD data between univariate and multivariable coefficient from univariate literature estimates;\na more sophisticated approach, with an optimized adaptation factor from a bootstrap procedure\n\nWe can also recalibrate a simple score based on univariate literature coefficients. This is a variant of naive Bayes modeling.\n\nBy definition, estimates that borrow information of other studies are more stable than per study estimates; the assumption is that the observed predictor associations in other studies are relevant for the local setting. For baseline risk, substantial heterogeneity was observed in the TBI case study. For the AAA study, calibration to an average risk of 5% was implemented.\nLocal validation studies are needed to confirm the applicability of prediction models; followed by updating where needed (Binuya 2022).",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimation with External Information</span>"
    ]
  },
  {
    "objectID": "chapters/15-evaluation-performance.html",
    "href": "chapters/15-evaluation-performance.html",
    "title": "15  Evaluation of Performance",
    "section": "",
    "text": "Chapter 15 additional material upcoming.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Performance</span>"
    ]
  },
  {
    "objectID": "chapters/16-clinical-usefulness.html",
    "href": "chapters/16-clinical-usefulness.html",
    "title": "16  Evaluation of Clinical Usefulness",
    "section": "",
    "text": "Chapter 16 additional material upcoming.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Evaluation of Clinical Usefulness</span>"
    ]
  },
  {
    "objectID": "chapters/17-validation.html",
    "href": "chapters/17-validation.html",
    "title": "17  Validation of Prediction Models",
    "section": "",
    "text": "Chapter 17 additional material upcoming.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Validation of Prediction Models</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html",
    "href": "chapters/18-presentation.html",
    "title": "18  Presentation Formats",
    "section": "",
    "text": "Fit logistic models in n544 data set; 6 predictors\nWe develop a prediction model with 6 predictors for presence of residual tumor in 544 men treated for metastatic nonseminomatous testicular cancer. Predictors were pre-specified based on review of the medical literature.\nCode show/hide\n# Fit a full 6 predictor model in n544\noptions(prType='html')\nhtml(describe(n544), scroll=TRUE)\n\n\n\n\n\n\n\nn544 Descriptives\nn544  11  Variables   544  Observations\n\nTeratoma\n\n \n nmissingdistinctInfoSumMeanGmd\n 544020.7462520.46320.4982\n \n\n\nPre.AFP\n\n \n nmissingdistinctInfoSumMeanGmd\n 544020.6751860.34190.4508\n \n\n\nPre.HCG\n\n \n nmissingdistinctInfoSumMeanGmd\n 544020.7042050.37680.4705\n \n\n\nPre.lnLDH\n        n  missing distinct     Info     Mean      Gmd      .05      .10      .25 \n      544        0      472        1   0.4582   0.7327 -0.38735 -0.26645 -0.03266 \n      .50      .75      .90      .95 \n  0.31299  0.92886  1.33528  1.65328  \n\n\n \n lowest :-1.06582 -0.948039-0.766913-0.731368-0.564972\n highest:2.42812  2.44938  2.48708  2.6237   2.76577  \n \n\n\nsqpost\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 5440930.9965.1332.922 1.414 2.236 3.162 4.472 6.652 9.66910.000\n \n\nlowest : 1.41421 1.73205 2       2.05477 2.23607 ,  highest: 10.9545 12.2474 13.0384 13.784  17.3205\n\nreduc10\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 54402000.9984.4824.307-2.500 0.000 2.000 5.167 7.337 8.78510.000\n \n\nlowest : -13.8095 -10      -8.33333 -8.10811 -6       ,  highest: 9.125    9.16667  9.28571  9.33333  10      \n\nNecrosis\n\n \n nmissingdistinctInfoSumMeanGmd\n 544020.7432450.45040.496\n \n\n\nTumor\n\n \n nmissingdistinctInfoSumMeanGmd\n 544020.7432990.54960.496\n \n\n\nReduction\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 54402000.99844.8243.07-25.00  0.00 20.00 51.67 73.37 87.85100.00\n \n\nlowest : -138.095 -100     -83.3333 -81.0811 -60      ,  highest: 91.25    91.6667  92.8571  93.3333  100     \n\nLDHst\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 544047212.031.6050.67890.76610.96791.36752.53173.80145.2242\n \n\nlowest : 0.344444 0.3875   0.464444 0.48125  0.568376 ,  highest: 11.3375  11.5812  12.0261  13.7867  15.8913 \n\nPost.size\n\n \n nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95\n 5440930.99633.2633.37  2.00  5.00 10.00 20.00 44.25 93.50100.00\n \n\nlowest : 2       3       4       4.22209 5       ,  highest: 120     150     170     190     300    \n\n\n\n\nCode show/hide\n# Start the fitting of 3 models\nset.seed(1)\nfull &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+log(LDHst)+sqrt(Post.size)+Reduction,\n            data=n544,x=T,y=T,linear.predictors=T)\nprint(full)\n\n\nLogistic Regression Model\n\nlrm(formula = Necrosis ~ Teratoma + Pre.AFP + Pre.HCG + log(LDHst) + \n    sqrt(Post.size) + Reduction, data = n544, x = T, y = T, linear.predictors = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Likelihood\nRatio Test\nDiscrimination\nIndexes\nRank Discrim.\nIndexes\n\n\n\n\nObs 544\nLR χ2 211.56\nR2 0.431\nC 0.839\n\n\n0 299\nd.f. 6\nR26,544 0.315\nDxy 0.677\n\n\n1 245\nPr(&gt;χ2) &lt;0.0001\nR26,404 0.399\nγ 0.678\n\n\nmax |∂log L/∂β| 1×10-7\n\nBrier 0.163\nτa 0.336\n\n\n\n\n\n\n\n\n\n\n\nβ\nS.E.\nWald Z\nPr(&gt;|Z|)\n\n\n\n\nIntercept\n -1.0425\n 0.6086\n-1.71\n0.0867\n\n\nTeratoma\n  0.9094\n 0.2140\n4.25\n&lt;0.0001\n\n\nPre.AFP\n  0.9025\n 0.2333\n3.87\n0.0001\n\n\nPre.HCG\n  0.7827\n 0.2305\n3.40\n0.0007\n\n\nLDHst\n  0.9854\n 0.2089\n4.72\n&lt;0.0001\n\n\nPost.size\n -0.2915\n 0.0815\n-3.58\n0.0003\n\n\nReduction\n  0.0158\n 0.0052\n3.04\n0.0024",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#fit-logistic-models-in-n544-data-set-6-predictors",
    "href": "chapters/18-presentation.html#fit-logistic-models-in-n544-data-set-6-predictors",
    "title": "18  Presentation Formats",
    "section": "",
    "text": "We first consider and full model and 2 shrinkage variants: uniform shrinkage and a penalized model.\nWe then study various formats for presentation of the prediction model for application in medical practice. The formats differ in user-friendliness and accuracy: for some simplifications we sacrifice nearly nothing and for other a lot in terms of c statistic. The formats are:\n\n\n\nnomogram (Fig 18.1)\nscore chart (Table 18.2 and Fig 18.2)\nsimple table (Table 18.3)\niso-probability lines (Fig 18.3)\ntree on a meta-model (Fig 18.4)\n\n\n\nInternal validation by bootstrapping\n\n\nCode show/hide\n## Validate model with bootstrapping\nset.seed(1)\nval.full  &lt;- validate(full, B=100)\nval.full[1,c(1:3,5) ] &lt;- val.full[1,c(1:3,5) ]/2 + 0.5 # transform Dxy to C\nval.full[1,4 ] &lt;- val.full[1,4]/2  # optimism in c\nrownames(val.full)[1] &lt;- \"C\"\nkable(as.data.frame(val.full[1:4,]), digits=3)  %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\nindex.orig\ntraining\ntest\noptimism\nindex.corrected\nn\n\n\n\n\nC\n0.839\n0.839\n0.835\n0.004\n0.835\n100\n\n\nR2\n0.431\n0.434\n0.423\n0.011\n0.420\n100\n\n\nIntercept\n0.000\n0.000\n0.000\n0.000\n0.000\n100\n\n\nSlope\n1.000\n1.000\n0.978\n0.022\n0.978\n100\n\n\n\n\n\nSo, limited optimism in c index (Dxy/2+0.5): 0.004.\nAnd limited need for shrinkage, since the calibration slope is close to 1: 0.98.\nFor illustration, we consider 2 shrinkage approaches:\n1. shrink coefficients with a uniform factor (from bootstrap validation (or by a heuristic formula as described by Copas 1983 and Van Houwelingen 1990 ). 2. penalize coefficients with a penalty factor as estimated by the optimal AIC in the pentrace function.\nEmpirical comparisons between shrinkage apporaches are here: Steyerberg 2000. A recent study compared the stability of shrinkage estimators: Van Calster 2020.\n\n\nShrunk model and penalization\n\n\nCode show/hide\n# Apply linear shrinkage\nfull.shrunk &lt;- full\nfull.shrunk$coef &lt;- val.full[4,5] * full.shrunk$coef  # use result from bootstrapping\n# val.full[4,5] is shrinkage factor; heuristic estimate (LR - df) / LR = (211-6)/211=0.97\n\n# Estimate new intercept, with shrunk lp as offset variable, i.e. coef fixed at unity\nfull.shrunk$coef[1] &lt;- lrm.fit(y=full$y, offset= full$x %*% full.shrunk$coef[2:7])$coef[1]\n\n# Make a penalized model with pentrace function\np   &lt;- pentrace(full, c(0,1,2,3,4,5,6,7,8,10,12,14,20))\nplot(p, which='aic.c', xlim=c(0,15), ylim=c(196,201))\nlines(x=c(4,4), y=c(196, 1000))\n\n\n\n\n\n\n\n\n\nThe optimal penalty factor is 4. Let’s update the full model fit with this penalty term.\n\n\nFit penalized model and compare fits\n\n\nCode show/hide\nfull.pen    &lt;- update(full, penalty=p$penalty) \n\n## compare coefs of 3 model variants\nfit.coefs &lt;- cbind(full=full$coefficients, shrunk=full.shrunk$coef, penalized=full.pen$coefficients)\nkable(as.data.frame(fit.coefs, digits=3))  %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\nfull\nshrunk\npenalized\n\n\n\n\nIntercept\n-1.042\n-1.021\n-1.090\n\n\nTeratoma\n0.909\n0.890\n0.873\n\n\nPre.AFP\n0.903\n0.883\n0.860\n\n\nPre.HCG\n0.783\n0.766\n0.729\n\n\nLDHst\n0.985\n0.964\n0.884\n\n\nPost.size\n-0.292\n-0.285\n-0.261\n\n\nReduction\n0.016\n0.015\n0.016",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#fig-18.1",
    "href": "chapters/18-presentation.html#fig-18.1",
    "title": "18  Presentation Formats",
    "section": "Fig 18.1",
    "text": "Fig 18.1\n\nMake nomogram from penalized model\n\n\nCode show/hide\npar(mfrow=c(1,1), mar=c(1,1,4,.5))\n# Need some stats for the data to plot nomogram\ndd  &lt;- datadist(n544)\noptions(datadist=\"dd\")\nnom &lt;- nomogram(full.pen,  fun=c(function(x)(1-plogis(x)), plogis), lp=T,lp.at=c(-2,-1,0,1,2,3),\n  LDHst=c(.5,1,1.5,2,3), Post.size=c(50,20,10,5,2), Reduction=c(0,35,70,100), \n         fun.at=c(seq(.1,.9,by=.1),0.95), \n         funlabel=c(\"p(residual tumor)\", \"p(necrosis)\"),  vnames=\"lab\", maxscale=10)\nplot(nom)\ntitle(\"Testicular cancer prediction model: risk of residual tumor\")\n\n\n\n\n\n\n\n\n\nSo, we have a nice presentation that allows for assessing the relative importance of predictors (by length of the lines), and allows for precise estimates of risks of residual tumor (and its complement, necrosis, benign tissue).\n\nInstruction to physicians using the model in their care:\nDetermine the patient’s value for each predictor, and draw a straight line upward to the points axis to determine how many points toward benign histology the patient receives. Sum the points received for each predictor and locate this sum on the total points axis. Draw a straight line down to find the patient’s predicted probability of residual tumor or necrosis (benign histology).\n\n\nInstruction to patient: “Mr. X, if we had 100 men exactly like you, we would expect that the chemotherapy was fully successful in approximately &lt;predicted probability from nomogram * 100&gt;, as reflected in fully benign disease at surgical resection of your abdominal lymph nodes.” see Kattan et al.\nNote that the number 100 may be debated, since the effective sample size for some covariate patterns may be far less.\n\n\n\nScore chart creation with categorized and continuous predictors\nWe first search for some nice rounding of logistic regression coefficients. A classic approach is multiplying by 10. We find that multiplying by 10/8, or 1.25 works well.\nWe then continue with searching for a similarly nice scoring for continuous predictors. This is a trial and error process, supported by graphical illustrations.\n\n\nConsider making rounded scores from penalized coefs\n\n\nCode show/hide\nscores &lt;- matrix(nrow=10, ncol=length(full.pen$coefficients), \n                 dimnames = list(NULL, c(\"Multiplier\", names(full.pen$coefficients[-1]))))\nfor (i in 1:10) {\n  scores[i,] &lt;- c(10/i, round(full.pen$coefficients[-1] * 10 / i)) }\n# consider a range of multipliers\nkable(as.data.frame(scores, digits=3))  %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\nMultiplier\nTeratoma\nPre.AFP\nPre.HCG\nLDHst\nPost.size\nReduction\n\n\n\n\n10.00\n9\n9\n7\n9\n-3\n0\n\n\n5.00\n4\n4\n4\n4\n-1\n0\n\n\n3.33\n3\n3\n2\n3\n-1\n0\n\n\n2.50\n2\n2\n2\n2\n-1\n0\n\n\n2.00\n2\n2\n1\n2\n-1\n0\n\n\n1.67\n1\n1\n1\n1\n0\n0\n\n\n1.43\n1\n1\n1\n1\n0\n0\n\n\n1.25\n1\n1\n1\n1\n0\n0\n\n\n1.11\n1\n1\n1\n1\n0\n0\n\n\n1.00\n1\n1\n1\n1\n0\n0\n\n\n\n\n\nCode show/hide\n# rounded scores\nkable(as.data.frame(rbind(mult10=round(10*full.pen$coef[-1]),\n                          mult1.25 =round(10/8*full.pen$coef[-1]))), digits=3)  %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\nTeratoma\nPre.AFP\nPre.HCG\nLDHst\nPost.size\nReduction\n\n\n\n\nmult10\n9\n9\n7\n9\n-3\n0\n\n\nmult1.25\n1\n1\n1\n1\n0\n0\n\n\n\n\n\nSo, we like the multiplier of 10/8, or 1.25. Let’s search for the mapping of coefficients of continuous predictors on nice scores as well.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#scores-for-continuous-predictors",
    "href": "chapters/18-presentation.html#scores-for-continuous-predictors",
    "title": "18  Presentation Formats",
    "section": "Scores for continuous predictors",
    "text": "Scores for continuous predictors\n\nLDH\nStart with LDH, where we use the log scale for standardized LDH values. We could e.g. find: 1 point per ln, so 1 point per 2.7 times normal. We use a score of 10/8 for LDH in a plot. We add reference lines for point allocation.\n\n\nCode show/hide\npar(mfrow=c(1,1), mar=c(4,4,2,2))\n# Use score of 10/8 for LDH, plot score by LDHst\nLDHplot &lt;- as.data.frame(cbind(n544$LDHst, round(10/8*full.pen$coef[5]*n544$Pre.lnLDH,1)))\nplot(x=LDHplot[,1], y=LDHplot[,2], xlab=\"LDHst (LDH / upper limit of normal LDH)\", ylab=\"Rounded score\", \n    axes=F, xlim=c(.5,6.9), ylim=c(-.6,2.2), cex.lab=1.2, main = \"LDH\")\nscat1d(x=n544$LDHst, side=1, col=\"red\", lwd=2)\naxis(side=1, at=c(.5,1,1.5,2,2.5,3,4,5,6,7))\naxis(side=2, at=c(-.5, 0,.5,1,1.5,2))\n# horizontal reference lines\nabline(a=0, b=0, lwd=2)\nabline(a=1, b=0)\nabline(a=2, b=0)\n# vertical reference lines\nlines(x=c(1,1), y=c(-.5, .5), lwd=2)\nlines(x=c(2.5,2.5), y=c(-.5, 1.5))\nlines(x=c(2.5^2,2.5^2), y=c(-.5, 2.5))\n\n\n\n\n\n\n\n\n\nWe find that a score of 1 corresponds to 2.5 times larger LDH values.\nContinue with post.size.\n\n\nStudy scores for postchemotherapy size\n\n\nCode show/hide\n# calculate score by sequence 2 to 100mm; \n# 2 is minimum for analysis; set 10 to zero in sequence (sqrt(seq(2,50,by=1))-sqrt(10))\n\n# We use 10mm as a reference, set to zero\nSizeplot &lt;- as.data.frame(cbind(n544$Post.size, \n            round(10/8*full.pen$coef[6]*(sqrt(n544$Post.size) - sqrt(10)),1)), main = \"Postchemotherapy size\")\nplot(x=Sizeplot[,1], y=Sizeplot[,2], xlab=\"Postchemo size (mm)\", ylab=\"Rounded score\", \n    axes=F, xlim=c(0,100), ylim=c(-2.1,1), cex.lab=1.2, main = \"Postsize\")\nscat1d(x=n544$Post.size, side=1, col=\"red\", lwd=2)\naxis(side=1, at=c(2,5,10,20,30,40,50,70, 100))\naxis(side=2, at=c(-2,-1,-.5,0,0.5))\n\n# horizontal reference lines\nabline(a=0, b=0, lwd=2)\nabline(a=.5, b=0)\nabline(a=-.5, b=0)\nabline(a=-1, b=0)\nabline(a=-2, b=0)\n\n# vertical reference lines\nlines(x=c(2,2), y=c(-2.1, .75))\nlines(x=c(10,10), y=c(-2.1, 0.25), lwd=2)\nlines(x=c(20,20), y=c(-2.1, -.1))\nlines(x=c(40,40), y=c(-2.1, -0.6))\nlines(x=c(100,100), y=c(-2.1, -1.5))\n\n\n\n\n\n\n\n\n\nSo, approximate scores for clinically meaningful sizes: 2 mm score +.5; 10mm, 0; 20mm, -.5; 40mm -1; 100 -2\nContinue with Reduction. This should be simple, since Reduction was modelled as a linear variable, without transformations.\n\n\nStudy scores for reduction in size\n\n\nCode show/hide\nRedplot &lt;- as.data.frame(cbind(n544$Reduction, \n            round(10/8*full.pen$coef[7]*n544$Reduction,1)))\nplot(x=Redplot[,1], y=Redplot[,2], xlab=\"Reduction in size (%)\", ylab=\"Rounded score\", \n    axes=F, xlim=c(0,100), ylim=c(-.1,2.1), cex.lab=1.2, col=\"red\", main = \"Reduction in size\")\nscat1d(x=n544$Reduction, side=1,col=\"red\", lwd=2)\naxis(side=1, at=c(0,25,50,75,100))\naxis(side=2, at=c(0,1,2))\n\n# horizontal reference lines\nabline(a=0, b=0, lwd=2)\nabline(a=1, b=0)\nabline(a=2, b=0)\n\n# vertical reference lines\nlines(x=c(0,0), y=c(-.1, 0.1), lwd=2)\nlines(x=c(50,50), y=c(-.1, 1.25))\nlines(x=c(100,100), y=c(-.1, 2.5))\n\n\n\n\n\n\n\n\n\nSo, scores for reduction in size: 50%, score 1; 100%, score 2\nWe store the scores in new variables for later use in a logistic regression model, to check the impact of rounding on model performance.\n\n\nCode show/hide\n# A score of 1 corresponds to 2.5 times larger LDH\n# rescale such that exp(l) does get more than 1 point; 1 point at 2.5 times normal LDH\nn544$LDHr  &lt;- round(n544$Pre.lnLDH*exp(1)/2.5,1)\n\n# postsize: coef -.3 per sqrt(mm). Hence -1 with 3.3*sqrt(mm)\n# So, approximate scores for clinically meaningful sizes:\n# 2 mm score +.5; 10mm, 0; 20mm, -.5; 40mm -1; 100 -2\n\n# change the intercept by defining score = 0 for 10 mm, which is clinically of most interest\nn544$SQPOSTr  &lt;- round((n544$sqpost - sqrt(10)) / (sqrt(10) - sqrt(40)),1)\n\n# Rescale reduction such that effect 1 point\n# 50% reduction 1 point; i.e. 0% = 0; 50%=1; 100%=2\nn544$REDUC5  &lt;- round(n544$reduc10 / 5,1)",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#table-18.2",
    "href": "chapters/18-presentation.html#table-18.2",
    "title": "18  Presentation Formats",
    "section": "Table 18.2",
    "text": "Table 18.2\n\nMake a score chart\nScore chart to be made by hand, using the points as derived above.\nCould also have kept life simpler by multiplying coefs by 10 and rounding.\n\n\nCode show/hide\nscore.fit &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+LDHr+SQPOSTr+REDUC5,data=n544, x=T,y=T)\n\n# scores &lt;- as.data.frame(round(10/8*full.pen$coef[-1]))\nkable(as.data.frame(10/8*score.fit$coef[-1]), digits=3) %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\n10/8 * score.fit$coef[-1]\n\n\n\n\nTeratoma\n1.134\n\n\nPre.AFP\n1.137\n\n\nPre.HCG\n0.964\n\n\nLDHr\n1.110\n\n\nSQPOSTr\n1.114\n\n\nREDUC5\n1.008\n\n\n\n\n\nCode show/hide\n#\n# Continuous scores\nLDHvalues &lt;- c(0.6, 1, 1.6, 2.5, 4, 6)\nkable(as.data.frame(rbind(LDH=LDHvalues, LDHscores=log(LDHvalues)*exp(1)/2.5)), digits = 2, col.names =NULL) %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLDH\n0.60\n1\n1.60\n2.5\n4.00\n6.00\n\n\nLDHscores\n-0.56\n0\n0.51\n1.0\n1.51\n1.95\n\n\n\n\n\nCode show/hide\nPostvalues &lt;- c(2,10,20,40,70)\nkable(as.data.frame(rbind(Postsize=Postvalues, \n                          Postscores=(sqrt(Postvalues) - sqrt(10)) / (sqrt(10) - sqrt(40)))), \n                    digits = 2, col.names =NULL) %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPostsize\n2.00\n10\n20.00\n40\n70.00\n\n\nPostscores\n0.55\n0\n-0.41\n-1\n-1.65\n\n\n\n\n\nCode show/hide\nRedvalues &lt;- c(0,5,10)\nkable(as.data.frame(rbind(Reduction=Redvalues, \n                          Redscores=Redvalues / 5)), \n                    digits = 2, col.names =NULL) %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nReduction\n0\n5\n10\n\n\nRedscores\n0\n1\n2\n\n\n\n\n\nSo, we see that the continuous values of some predictors map to scores of approximately 0.5 or 1. Intermediate scores can be obtained by interpolation.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#fig-18.2",
    "href": "chapters/18-presentation.html#fig-18.2",
    "title": "18  Presentation Formats",
    "section": "Fig 18.2",
    "text": "Fig 18.2\n\nTranslate score in probability estimates with 95% CI\n\n\nCode show/hide\n##'# Function to relate scores to predictions in graph ###\nmake.score.matrix &lt;-function(fit, scores, shrinkage=1, limits.scores=NULL) {   \n  # lpscore for score chart + graph\n  fit         &lt;- update(fit, x=T,y=T, se.fit=T)   # SE of predictions from original model\n  rounded.lp    &lt;- fit$x %*% scores                 # Linear predictor\n  multiplier  &lt;- lrm.fit(y=fit$y, x=rounded.lp)$coef[2]    ## For score formula\n  shrunk.beta   &lt;- shrinkage * multiplier           # Shrinkage built in\n  ## Estimate intercept for scores, using the scores as offset with shrinkage\n  fit.lp      &lt;- lrm.fit(y=score.fit$y, offset=shrunk.beta*rounded.lp) \n  ## lp formula; this version is not fully rounded yet\n  lp2           &lt;- fit.lp$coef[1] + shrunk.beta*rounded.lp\n  cat(\"\\nlinear predictor:\", fit.lp$coef[1], \"+\", shrunk.beta, \"* rounded.lp\\n\",\n        \" original range of scores\", min(rounded.lp), max(rounded.lp), \"\\n\")\n  if (!is.null(limits.scores))  {\n    rounded.lp[rounded.lp &lt; min(limits.scores)] &lt;- min(limits.scores)\n    rounded.lp[rounded.lp &gt; max(limits.scores)] &lt;- max(limits.scores) }\n  cat(\"restricted range of scores\", min(rounded.lp), max(rounded.lp), \"\\n\") \n  \n  ## Data for graph\n  graph.lp      &lt;- cbind(rounded.lp, fit$se.fit)        # lp and se.fit in matrix\n  lp.events     &lt;- tapply(fit$y,list(round(graph.lp[,1],0)),sum)    # events per lp score\n  lp.nonevents &lt;- tapply(1-fit$y,list(round(graph.lp[,1],0)),sum)   # non-events per lp score\n  se.lp           &lt;- tapply(graph.lp[,2],list(round(graph.lp[,1],0)),mean)  # mean se per lp score\n  lp.lp           &lt;- tapply(graph.lp[,1],list(round(graph.lp[,1],0)),mean)  # mean lp per lp score\n  score         &lt;- lp.lp                        # range of score, rounded to 0 decimals\n  lp3               &lt;- fit.lp$coef[1] + shrunk.beta*round(score,0) # lp on logistic scale, rounded\n\n  ## data frame with predicted prob + 95% CI\n  p.lp          &lt;- as.data.frame(cbind(score=round(score,0),\n                    p=plogis(lp3),plow=plogis(lp3-1.96*se.lp),phigh=plogis(lp3+1.96*se.lp),\n                    lp.events,lp.nonevents))\n  p.lp$total &lt;- p.lp$lp.events + p.lp$lp.nonevents\n  p.lp\n}\n\n## Make Fig 18.2 ##\n\n## Continuous predictors in score\nscore.fit &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+LDHr+SQPOSTr+REDUC5,data=n544, x=T,y=T)\nscores  &lt;- rep(1,6)  # All scores a weight of 1\n\nlp.n544 &lt;- make.score.matrix(fit=score.fit, scores=scores, shrinkage=0.95, limits.scores=c(-2,5))\n\n\n\nlinear predictor: -1.94 + 0.815 * rounded.lp\n  original range of scores -4.3 6.7 \nrestricted range of scores -2 5 \n\n\nCode show/hide\nkable(as.data.frame(lp.n544), \n                    digits = 2) %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\nscore\np\nplow\nphigh\nlp.events\nlp.nonevents\ntotal\n\n\n\n\n-2\n-2\n0.03\n0.01\n0.07\n0\n22\n22\n\n\n-1\n-1\n0.06\n0.03\n0.11\n2\n46\n48\n\n\n0\n0\n0.13\n0.07\n0.21\n6\n53\n59\n\n\n1\n1\n0.24\n0.16\n0.36\n19\n58\n77\n\n\n2\n2\n0.42\n0.31\n0.54\n48\n59\n107\n\n\n3\n3\n0.62\n0.49\n0.74\n62\n37\n99\n\n\n4\n4\n0.79\n0.68\n0.87\n66\n19\n85\n\n\n5\n5\n0.89\n0.82\n0.94\n42\n5\n47\n\n\n\n\n\nCode show/hide\npar(mfrow=c(1,1),mar=c(4.5,4.5,3,1))\nplot(x=lp.n544[,1], y=100*lp.n544[,2], lty = 1, las=1, type = \"b\", cex.lab=1.2,\n        ylab = \"Predicted probability of necrosis (%)\", ylim=c(-10,100), \n        xlab=\"Sum score\", cex=sqrt(lp.n544$total)/5, xaxt=\"n\", pch=16, col=\"red\")\naxis(side=1,at=lp.n544[,1], cex=1)\n\n# add 95% CI\nfor (i in 1:nrow(lp.n544)) lines(x=c(lp.n544[i,1], lp.n544[i,1]), y=c(100*lp.n544[i,3], 100*lp.n544[i,4]), type=\"l\", col=\"red\")\n# add N= ...\nmtext(\"N=\", side = 2, outer = FALSE, line=1.5, at = -10, adj = 0, las=1, cex=1, col=\"red\")\ntext(x=lp.n544[,1],y=-10,labels=lp.n544$total, cex=1, col=\"red\")",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#end-table-18.2-score-chart-and-fig-18.2-graphic-translation-from-score-to-probability",
    "href": "chapters/18-presentation.html#end-table-18.2-score-chart-and-fig-18.2-graphic-translation-from-score-to-probability",
    "title": "18  Presentation Formats",
    "section": "End Table 18.2 (score chart) and Fig 18.2 (graphic translation from score to probability)",
    "text": "End Table 18.2 (score chart) and Fig 18.2 (graphic translation from score to probability)\n\nCategorized coding\nWe can categorize the continuous predictors; a bad idea generally speaking\n\n\nCode show/hide\n# LDH\nn544$PRELDH &lt;- ifelse(n544$Pre.lnLDH&lt;log(1),0,1)\n# PostSize simple coding: 0,1,2\nn544$POST2 &lt;- ifelse(n544$sqpost&lt;sqrt(20),2,\n                ifelse(n544$sqpost&lt;sqrt(50),1,0))\n# Reduction in categories: increase=-1, 0-49%=0, &gt;=50%=1\nn544$REDUCr  &lt;- ifelse(n544$Reduction&lt;0,-1, ifelse(n544$Reduction&gt;=50,1,0))\n## Everything coded in categories\nscore.fit2 &lt;- lrm(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+PRELDH+POST2+REDUCr,data=n544, x=T,y=T)\nkable(as.data.frame(x=10/8*score.fit2$coef[-1]), digits=2) %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\n10/8 * score.fit2$coef[-1]\n\n\n\n\nTeratoma\n1.15\n\n\nPre.AFP\n1.08\n\n\nPre.HCG\n0.83\n\n\nPRELDH\n1.13\n\n\nPOST2\n1.08\n\n\nREDUCr\n0.98\n\n\n\n\n\nSo, each category approx 1 point.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#table-18.3",
    "href": "chapters/18-presentation.html#table-18.3",
    "title": "18  Presentation Formats",
    "section": "Table 18.3",
    "text": "Table 18.3\n\nAlternative: make a score from 0 - 5\n\nUltra simple scores\n“.. categories of the predictors were simplified for practical application. We conclude that a simple statistical model, based on a limited number of patient characteristics, provides better guidelines for patient selection than those currently used in clinical practice. Br J Cancer 1996”\n\n\n\nCode show/hide\nscore5  &lt;- n544$Teratoma+n544$Pre.AFP+n544$Pre.HCG+n544$PRELDH+n544$REDUCr\nscore5  &lt;- ifelse(n544$REDUCr&lt;0,0,score5)\nn544$score5 &lt;- score5\n\n# Simple coding: 5 categories for postsize (no difference 20-30 and 30-50 mm)\nPOST5 &lt;- ifelse(n544$sqpost&lt;=sqrt(10),0,\n                ifelse(n544$sqpost&lt;=sqrt(20),1,\n                       ifelse(n544$sqpost&lt;=sqrt(30),2,\n                              ifelse(n544$sqpost&lt;=sqrt(50),3,4))))\nn544$POST5 &lt;- as.factor(POST5)  # add the post size categories as a factor\nfull.simple2  &lt;- lrm(Necrosis~POST5+score5, data=n544, x=T,y=T,se.fit=T)\npred.simple2  &lt;- aggregate(plogis(predict(full.simple2)), by=list(POST5, score5), FUN=mean)\n\npred.simple2  &lt;- as.data.frame(with(pred.simple2,tapply(x,list(Group.1,Group.2),mean)))\n\nkable(as.data.frame(pred.simple2), digits = 2)  %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nNA\n0.32\n0.52\n0.71\n0.85\n0.93\n\n\n1\n0.14\n0.27\n0.45\n0.66\n0.81\n0.91\n\n\n2\n0.06\n0.12\n0.24\n0.42\n0.63\n0.79\n\n\n3\n0.08\n0.16\n0.30\n0.50\n0.69\nNA\n\n\n4\n0.04\n0.09\n0.19\n0.35\n0.55\nNA\n\n\n\n\n\nCode show/hide\nn544$POST5 &lt;- as.numeric(n544$POST5)\n\n# Make simple risk scores per category\nn544$simple.cat  &lt;- ifelse(n544$POST5==3 & score5==4,70,\n                    ifelse(n544$POST5==3 & score5==5,80,\n                      ifelse(n544$POST5==2 & score5==3,60,\n                      ifelse(n544$POST5==2 & score5==4,80,                    \n                      ifelse(n544$POST5==2 & score5==5,90,                    \n                        ifelse(n544$POST5&lt;2 & score5==2,60,\n                        ifelse(n544$POST5&lt;2 & score5==3,70,\n                        ifelse(n544$POST5&lt;2 & score5==4,80,\n                        ifelse(n544$POST5&lt;2 & score5==5,90, 50)))))))))\n\nsimple.tab &lt;- as.data.frame(with(n544,tapply(simple.cat,list(POST5=POST5,score=score5),mean)))\nsimple.tab[is.na(simple.tab)] &lt;- 50\ncolnames(simple.tab) &lt;- Cs(score.0, score.1, score.2, score.3, score.4, score.5)\nrownames(simple.tab) &lt;- Cs(size10, size20, size30, size50, size.gt.50)\n\nsimple.tab %&gt;% \nmutate(score.0 = cell_spec(score.0, \"html\", color = ifelse(score.0 &gt; 50, \"orange\", \"red\")),\n       score.1 = cell_spec(score.1, \"html\", color = ifelse(score.1 &gt; 50, \"orange\", \"red\")),\n       score.2 = cell_spec(score.2, \"html\", color = ifelse(score.2 &gt; 50, \"orange\", \"red\")),\n       score.3 = cell_spec(score.3, \"html\", color = ifelse(score.3 == 70, \"green\",\n                                                           ifelse(score.3 == 60, \"orange\",\"red\"))),\n       score.4 = cell_spec(score.4, \"html\", color = ifelse(score.4 == 80, \"white\",\n                                                           ifelse(score.4 == 70, \"green\",\n                                                           ifelse(score.4 == 60, \"orange\",\"red\"))),\n                           background=ifelse(score.4 == 80, \"green\",\"white\" )),\n       score.5 = cell_spec(score.5, \"html\", color = ifelse(score.5 &gt;= 80, \"white\",\n                                                           ifelse(score.5 == 70, \"green\",\n                                                           ifelse(score.5 == 60, \"orange\",\"red\"))),\n                           background=ifelse(score.5 == 90, \"darkgreen\",\n                                             ifelse(score.5 == 80, \"green\",\"white\" )))\n) %&gt;%\nkable(format = \"html\", escape = F, row.names = T) %&gt;% kable_styling(full_width=F, position = \"left\")\n\n\n\n\n\n\nscore.0\nscore.1\nscore.2\nscore.3\nscore.4\nscore.5\n\n\n\n\nsize10\n50\n50\n60\n70\n80\n90\n\n\nsize20\n50\n50\n50\n60\n80\n90\n\n\nsize30\n50\n50\n50\n50\n70\n80\n\n\nsize50\n50\n50\n50\n50\n50\n50\n\n\nsize.gt.50\n50\n50\n50\n50\n50\n50\n\n\n\n\n\nCode show/hide\n# kable(simple.tab, digits = 2) %&gt;% kable_styling(full_width=F, position = \"left\") \n\n\nIn this way we created a very simple table, with probability of necrosis according to post-chemotherapy mass size (rows) and a simple, categorized score of the other 5 predictors.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#fig-18.3",
    "href": "chapters/18-presentation.html#fig-18.3",
    "title": "18  Presentation Formats",
    "section": "Fig 18.3",
    "text": "Fig 18.3\n\nIso-probability lines\nFor radiologists, it is natural to consider the pre-chemotherapy mass size and post-chemotherapy mass size. According to our analysis, the key information is the pre-chemotherapy size and the reduction in size. These pieces of information need to be considered jointly with the 4 other predictors (Teratoma, 3 tumor markers). We attempted to convey this message in iso-probability lines (Fig 18.3) Radiology\n\n\nCode show/hide\n## Radiology paper: could graph pre - post with lines for score 1 - 4\nscore4  &lt;- n544$Teratoma+n544$Pre.AFP+n544$Pre.HCG+n544$Pre.lnLDH\nfull.simple3 &lt;- lrm(Necrosis ~ score4 + sqpost + reduc10, data=n544, x=T, y=T)\n\nkable(as.data.frame(full.simple3$coef), digits = 2) %&gt;% kable_styling(full_width=F, position = \"left\") \n\n\n\n\n\n\nfull.simple3$coef\n\n\n\n\nIntercept\n-1.20\n\n\nscore4\n0.89\n\n\nsqpost\n-0.27\n\n\nreduc10\n0.17\n\n\n\n\n\nCode show/hide\n# Make a presize variable, based on reduction and postsize\n# reduc = (pre-post) / pre; reduc = 1 - post/pre; reduc-1 = -post/pre; pre = -post/(reduc-1)\nn544$presize &lt;- - n544$sqpost^2 / ifelse((n544$reduc10/10 - 1) != 0,(n544$reduc10/10 - 1),-.04)  \n# Now calculate iso probability lines: e.g. prob -70%, \n# Calculate PRESIZE from SQPOST, for given probabilities\n# presize = f(sqpost, plogis(full))\n# pre = -post/(reduc10-10) # for reduc10&lt;10\n# range sqpost from 2 to 50; score 4 0 - 4\n\n## Fig 18.3 ##\n# matrix with sqpost and scores\nx &lt;- as.matrix(cbind(rep(sqrt(1:50),5),c(rep(0,50),rep(1,50),rep(2,50),rep(3,50),rep(4,50))))\n# linear predictor from simple model 'full.simple3'\nlp.simple3  &lt;- x %*% full.simple3$coef[3:2] + full.simple3$coef[1]\n# now calculate reduc10 with condition p=70% etc\n# Solve equation qlogis(.7) = lp.simple3 + full.simple3$coef[4] * n544$REDUC10\nreduc10.70 &lt;- (qlogis(.7) - lp.simple3) /full.simple3$coef[4]\n# Calculate presizes: pre = -post/(reduc10/10-1)\npresize.70  &lt;- - rep(1:50,5) / (reduc10.70/10-1)\n# in 1 formula for efficiency\npresize.90  &lt;- - rep(1:50,5) / (((qlogis(.9) - lp.simple3) /(full.simple3$coef[4])/10) -1)\npresize.80  &lt;- - rep(1:50,5) / (((qlogis(.8) - lp.simple3) /(full.simple3$coef[4])/10) -1)\npresize.60  &lt;- - rep(1:50,5) / (((qlogis(.6) - lp.simple3) /(full.simple3$coef[4])/10) -1)\npresize.50  &lt;- - rep(1:50,5) / (((qlogis(.5) - lp.simple3) /(full.simple3$coef[4])/10) -1)\n\n# make Postsize from sqpost\nx[,1] &lt;- x[,1]^2\n# combine results\nx &lt;- as.matrix(cbind(x,lp.simple3,\n        reduc10.70, presize.70, presize.90,presize.80,presize.60,presize.50 ))\ncolnames(x) &lt;- Cs(postsize, score4,lp,reduc1070,\n          presize70,presize90,presize80,presize60,presize50)\nx &lt;- as.data.frame(x)\nx[x&lt;0]  &lt;- NA\n\n# Make some plots; isoprobability lines for different scores, x-axis=postsize\npar(mfrow=c(2,2), mar=c(4,4,3,.5))\nfor (i in 1:4) {\n  xp = x[x$score4==i & !is.na(x$presize50) & x$presize50&lt;100 & x$postsize&lt;50, ]\n  xp = xp[sort(xp$presize50),]\n  plot(y=xp$postsize,x=xp$presize50, main=paste(\"Score=\",i,sep=\"\"),\n  ylab='Postchemo size (mm)', xlab=\"Prechemo size (mm)\", \n  xlim=c(0,100), ylim=c(0,50),type=\"l\", pch=\"5\", axes=F,las=1, col=mycolors[5], lwd=2)\naxis(side=2,at=c(0,10,20,30,50))\naxis(side=1,at=c(0,20,50,100))\nif (i==1) legend(\"topleft\", lty=c(1,2,1,2,1), col=mycolors[c(5,4,2,3,6)], lwd=2,bty=\"n\",\n                 legend=c(\"50%\", \"60\", \"70%\", \"80%\", \"90%\"))\n\nxp = x[x$score4==i & !is.na(x$presize60), ]\nxp = xp[sort(xp$presize60),]\nlines(y=xp$postsize,x=xp$presize60, pch=\"6\",lty=2, col=mycolors[4], lwd=2)\nxp = x[x$score4==i & !is.na(x$presize70), ]\nxp = xp[sort(xp$presize70),]\nlines(y=xp$postsize,x=xp$presize70, pch=\"7\",lty=1, col=mycolors[2], lwd=2)\n\nxp = x[x$score4==i & !is.na(x$presize80), ]\nxp = xp[sort(xp$presize80),]\nlines(y=xp$postsize,x=xp$presize80, pch=\"8\", lty=2, col=mycolors[3], lwd=2)\n\nxp = x[x$score4==i & !is.na(x$presize90), ]\nxp = xp[sort(xp$presize90),]\nlines(y=xp$postsize,x=xp$presize90, pch=\"9\", lty=1, col=mycolors[6], lwd=2)\n} # end loop over score 1 - 4\n\n\n\n\n\n\n\n\n\nSo, we obtain some nice graphics. With score 0, men are at high risk of tumor. With score 4, even a relatively large post-chemotherapy mass size leads to high probabilities of necrosis, especially if the pre-chemotherapy mass was large.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#fig-18.4",
    "href": "chapters/18-presentation.html#fig-18.4",
    "title": "18  Presentation Formats",
    "section": "Fig 18.4",
    "text": "Fig 18.4\n\nMeta-model with tree presentation\nWe can present a decision rule for patients with testicular cancer, assuming a threshold of 70% for the probability of necrosis. If the probability is higher than 70%, this is a good prognosis group which could be spared surgery.\n\n\nCode show/hide\n## Start with dichotomizing predictions from full as &lt;70% vs &gt;=70%\nn544$predhigh &lt;- ifelse(plogis(full$linear.predictor)&lt;.7,0,1)\n\n# Try to make tree model for this outcome\npar(mfrow=c(1,1), mar=c(2,2,2,1))\ntree.orig &lt;- rpart(Necrosis ~ Teratoma+Pre.AFP+Pre.HCG+LDHst+Post.size+Reduction,\n            data=n544)\nplot(tree.orig)\ntext(tree.orig, use.n=F)\ntitle(\"Meta-model for original predictions\")\n\n\n\n\n\n\n\n\n\nCode show/hide\ntree.meta &lt;- rpart(predhigh ~ Teratoma+Pre.AFP+Pre.HCG+LDHst+Post.size+Reduction,\n            data=n544)\nplot(tree.meta)\ntext(tree.meta, use.n=F)\ntitle(\"Meta-model for probability &gt; 70%\")\n\n\n\n\n\n\n\n\n\nCode show/hide\n## Make smooth tree presentation; classification with reduction, teratoma, AFP\nscore3  &lt;- as.numeric(n544$Reduction&gt;70)+as.numeric(n544$Teratoma==1)+as.numeric(n544$Pre.AFP==1)\nn544$tree.cat  &lt;- ifelse(n544$Reduction&gt;50 & score3&gt;=2,1,0)\ntree.results &lt;- as.data.frame(table(n544$tree.cat, n544$Necrosis))\ncolnames(tree.results) &lt;- Cs(\"Risk group\", Necrosis, N)\nkable(tree.results, digits = 2) %&gt;% kable_styling(full_width=F, position = \"left\") \n\n\n\n\n\nRisk group\nNecrosis\nN\n\n\n\n\n0\n0\n264\n\n\n1\n0\n35\n\n\n0\n1\n134\n\n\n1\n1\n111\n\n\n\n\n\nend tree analysis",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/18-presentation.html#apparent-performance-of-penalized-vs-rescaled-vs-simplified-models",
    "href": "chapters/18-presentation.html#apparent-performance-of-penalized-vs-rescaled-vs-simplified-models",
    "title": "18  Presentation Formats",
    "section": "Apparent performance of penalized vs rescaled vs simplified models",
    "text": "Apparent performance of penalized vs rescaled vs simplified models\nWhat is the c statistic of various formats to predict necrosis at resection after chemotherapy? a) nomogram (Fig 18.1): original or penalized coefficients, negligable difference expected b) score chart (Table 18.2 and Fig 18.2): rounding, minor decline expected c) simple table (Table 18.3): categorization and simplification, much loss expected d) iso-probability lines (Fig 18.3): major predictors (size, reduction) kept continuous, some loss expected e) tree on a meta-model (Fig 18.4): ultra simple, major loss in performance expected\n\n\nCode show/hide\n# 3 variants of full model fit\ncstats &lt;- cbind(\n  rcorr.cens(full$linear.predictor, n544$Necrosis)[1],\n  rcorr.cens(full.shrunk$linear.predictor, n544$Necrosis)[1],\n  rcorr.cens(full.pen$linear.predictor, n544$Necrosis)[1],\n  # rounding for score chart\n  rcorr.cens(score.fit$linear.predictor, n544$Necrosis)[1],\n  # simple table\n  rcorr.cens(n544$simple.cat, n544$Necrosis)[1],\n  # Radiology paper\n  rcorr.cens(full.simple3$linear.predictor, n544$Necrosis)[1],\n  # Tree\n  rcorr.cens(n544$tree.cat, n544$Necrosis)[1])\ncolnames(cstats) &lt;- Cs(full, shrunk, penalized, scores, table, radiology, tree)\nrownames(cstats) &lt;- \"C\"\ndotchart(cstats, xlab=\"C statistic\", xlim=c(0.6,.9))\n\n\n\n\n\n\n\n\n\nCode show/hide\nkable(as.data.frame(cstats), digits = 4) %&gt;% kable_styling(full_width=F, position = \"left\") \n\n\n\n\n\n\nfull\nshrunk\npenalized\nscores\ntable\nradiology\ntree\n\n\n\n\nC\n0.839\n0.839\n0.839\n0.838\n0.725\n0.838\n0.668\n\n\n\n\n\n\nEnd comparisons of c statistics\nAs expected, the apparent performance of penalized vs rescaled models was very similar. Two formats led to a substantial loss in performance:\n\nsimple table (Table 18.3): categorization and simplification, c=0.73 cp to c=0.84\n\ntree on a meta-model (Fig 18.4): ultra simple, c=0.67, for a single threshold to classify men as low risk and spare them resection.\n\nPlease use and improve the code above when desired. The major tools for presentation are:\n1. The nomogram function: allows for scores with categorical and continuous predictors, with transformations.\n2. The regression formula: can be implemented in wwww tools, including the Evidencio toolbox.",
    "crumbs": [
      "Part II: Developing Valid Prediction Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Presentation Formats</span>"
    ]
  },
  {
    "objectID": "chapters/19-external-validity.html",
    "href": "chapters/19-external-validity.html",
    "title": "19  Patterns of External Validity",
    "section": "",
    "text": "Fig 19.1: a missed predictor Z\nActual correlations in n=1000\nr=0: -0.0114  r=.33: 0.312  r=.5: 0.475",
    "crumbs": [
      "Part III: Generalizability of Prediction Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Patterns of External Validity</span>"
    ]
  },
  {
    "objectID": "chapters/19-external-validity.html#fig-19.7-case-control-design-disturbs-calibration",
    "href": "chapters/19-external-validity.html#fig-19.7-case-control-design-disturbs-calibration",
    "title": "19  Patterns of External Validity",
    "section": "Fig 19.7: Case-control design disturbs calibration",
    "text": "Fig 19.7: Case-control design disturbs calibration\nDisturbance is exactly as expected by ratio of selecting cases:controls (log(2)) Select all cases, and half of the controls –&gt; same as shift in intercept\n\n\n\n\n\n\n\n\n\n\nFig 19.8: Overfitting disturbs discrimination and calibration\n\n\n\n\n\n\n\n\n\n\n\nFig 19.9: Different coeficients (model misspecification) disturbs discrimination and calibration\n\n\n\n\n\n\n\n\n\n\n\nScenarios: Table 19.4\nChange of setting may especially impact calibration RCT vs survey may impact discrimination (more homogeneity) and calibration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty in validation simulations",
    "crumbs": [
      "Part III: Generalizability of Prediction Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Patterns of External Validity</span>"
    ]
  },
  {
    "objectID": "chapters/20-updating.html",
    "href": "chapters/20-updating.html",
    "title": "20  Updating for a New Setting",
    "section": "",
    "text": "Chapter 20 additional material upcoming.",
    "crumbs": [
      "Part III: Generalizability of Prediction Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Updating for a New Setting</span>"
    ]
  },
  {
    "objectID": "chapters/21-updating-multiple.html",
    "href": "chapters/21-updating-multiple.html",
    "title": "21  Updating for Multiple Settings",
    "section": "",
    "text": "Chapter 21 additional material upcoming.",
    "crumbs": [
      "Part III: Generalizability of Prediction Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Updating for Multiple Settings</span>"
    ]
  },
  {
    "objectID": "chapters/22-case-study-gusto.html",
    "href": "chapters/22-case-study-gusto.html",
    "title": "22  Case Study on a Prediction of 30-Day Mortality",
    "section": "",
    "text": "Chapter 22 additional material upcoming.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Case Study on a Prediction of 30-Day Mortality</span>"
    ]
  },
  {
    "objectID": "chapters/23-case-study-cardiovascular.html",
    "href": "chapters/23-case-study-cardiovascular.html",
    "title": "23  Case Study on Survival Analysis: Prediction of Cardiovascular Event",
    "section": "",
    "text": "Chapter 23 additional material upcoming.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Case Study on Survival Analysis: Prediction of Cardiovascular Event</span>"
    ]
  },
  {
    "objectID": "chapters/24-lessons-datasets.html",
    "href": "chapters/24-lessons-datasets.html",
    "title": "24  Overall Lessons and Data Sets",
    "section": "",
    "text": "Chapter 24 additional material upcoming.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Overall Lessons and Data Sets</span>"
    ]
  }
]